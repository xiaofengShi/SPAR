{"question": "Can you tell me some papers about hybrid architectures in reconstruction-based techniques?", "answer": ["Multivariate Time-series Anomaly Detection via Graph Attention Network"], "answer_arxiv_id": ["2009.02040"], "source_meta": {"published_time": "20231024"}, "qid": "AutoScholarQuery_test_0"}
{"question": "Are there any studies that analysed the use of target networks for Deep Q-learning?", "answer": ["A Theoretical Analysis of Deep Q-Learning"], "answer_arxiv_id": ["1901.00137"], "source_meta": {"published_time": "20230224"}, "qid": "AutoScholarQuery_test_1"}
{"question": "Any resources providing information about attempts to detect or calibrate biases automatically in peer reviews?", "answer": ["Uncovering Latent Biases in Text: Method and Application to Peer Review", "You Are the Best Reviewer of Your Own Papers: An Owner-Assisted Scoring Mechanism", "Your 2 is My 1, Your 3 is My 9: Handling Arbitrary Miscalibrations in Ratings", "Least Square Calibration for Peer Reviews"], "answer_arxiv_id": ["2010.15300", "2110.14802", "1806.05085", "2110.12607"], "source_meta": {"published_time": "20231212"}, "qid": "AutoScholarQuery_test_2"}
{"question": "What papers are the foundation models for the Natural Language Processing (NLP) field based on?", "answer": ["BERT: Pre-training of Deep Bidirectional Transformers for Language\n  Understanding", "Language Models are Few-Shot Learners", "Exploring the Limits of Transfer Learning with a Unified Text-to-Text\n  Transformer", "PaLM: Scaling Language Modeling with Pathways", "LLaMA: Open and Efficient Foundation Language Models"], "answer_arxiv_id": ["1810.04805", "2005.14165", "1910.10683", "2204.02311", "2302.13971"], "source_meta": {"published_time": "20231214"}, "qid": "AutoScholarQuery_test_3"}
{"question": "Could you list the papers that explored identifying optimal interventions through sequential experimentation in causal bandits and causal reinforcement learning?", "answer": ["Causal Bandits: Learning Good Interventions via Causal Inference"], "answer_arxiv_id": ["1606.03203"], "source_meta": {"published_time": "20230531"}, "qid": "AutoScholarQuery_test_4"}
{"question": "Could you provide me some studies that focused on white-box scenarios for cyber-security in machine learning?", "answer": ["Universal Adversarial Triggers for Attacking and Analyzing NLP"], "answer_arxiv_id": ["1908.07125"], "source_meta": {"published_time": "20240122"}, "qid": "AutoScholarQuery_test_5"}
{"question": "Which papers generalize the coordinate definition of the field to cases where the parameters of a viewing ray are used?", "answer": ["Light Field Networks: Neural Scene Representations with\n  Single-Evaluation Rendering", "Scene Representation Transformer: Geometry-Free Novel View Synthesis\n  Through Set-Latent Scene Representations"], "answer_arxiv_id": ["2106.02634", "2111.13152"], "source_meta": {"published_time": "20231129"}, "qid": "AutoScholarQuery_test_6"}
{"question": "Which works develop suitable approximations of the predictive distribution or parts of the integral for uncertainties in deep learning?", "answer": ["A Probabilistic U-Net for Segmentation of Ambiguous Images", "Stochastic Segmentation Networks: Modelling Spatially Correlated Aleatoric Uncertainty", "A Hierarchical Probabilistic U-Net for Modeling Multi-Scale Ambiguities", "PHiSeg: Capturing Uncertainty in Medical Image Segmentation"], "answer_arxiv_id": ["1806.05034", "2006.06015", "1905.13077", "1906.04045"], "source_meta": {"published_time": "20230328"}, "qid": "AutoScholarQuery_test_7"}
{"question": "Which studies have proposed using voxel for spatial geometry and texture modeling in 3D scene representation?", "answer": ["3D ShapeNets: A Deep Representation for Volumetric Shapes", "3D-R2N2: A Unified Approach for Single and Multi-view 3D Object\n  Reconstruction"], "answer_arxiv_id": ["1406.5670", "1604.00449"], "source_meta": {"published_time": "20231128"}, "qid": "AutoScholarQuery_test_8"}
{"question": "Which studies present issues about the stationary distribution of rewards over contexts?", "answer": ["The K-Nearest Neighbour UCB algorithm for multi-armed bandits with covariates", "Nonparametric Stochastic Contextual Bandits", "Smooth Contextual Bandits: Bridging the Parametric and Non-differentiable Regret Regimes", "Randomized Allocation with Nonparametric Estimation for Contextual Multi-Armed Bandits with Delayed Rewards", "Self-Tuning Bandits over Unknown Covariate-Shifts", "Smoothness-Adaptive Contextual Bandits", "Transfer Learning for Contextual Multi-armed Bandits"], "answer_arxiv_id": ["1803.00316v1", "1801.01750", "1909.02553", "1902.00819", "2007.08584", "1910.09714", "2211.12612"], "source_meta": {"published_time": "20230711"}, "qid": "AutoScholarQuery_test_9"}
{"question": "Which work first implemented token-level edit operation prediction in Seq2Edit methods?", "answer": ["Encode, Tag, Realize: High-Precision Text Editing"], "answer_arxiv_id": ["1909.01187"], "source_meta": {"published_time": "20240528"}, "qid": "AutoScholarQuery_test_10"}
{"question": "Could you provide me a study about generating sign pose sequences from gloss sequences by employing VQ-VAE?", "answer": ["G2P-DDM: Generating Sign Pose Sequence from Gloss Sequence with Discrete\n  Diffusion Model"], "answer_arxiv_id": ["2208.09141"], "source_meta": {"published_time": "20240611"}, "qid": "AutoScholarQuery_test_11"}
{"question": "Who proposed source-free universal domain adaptation (SF-UniDA)?", "answer": ["UMAD: Universal Model Adaptation under Domain and Category Shift", "Upcycling Models under Domain and Category Shift"], "answer_arxiv_id": ["2112.08553", "2303.07110"], "source_meta": {"published_time": "20240306"}, "qid": "AutoScholarQuery_test_12"}
{"question": "What works aim to study the policies or features that remain stable across the different training tasks?", "answer": ["Invariant Policy Optimization: Towards Stronger Generalization in Reinforcement Learning", "Instance-based Generalization in Reinforcement Learning", "Domain Adversarial Reinforcement Learning", "Generalization in Reinforcement Learning with Selective Noise Injection and Information Bottleneck", "Decoupling Representation Learning from Reinforcement Learning", "Deep Reinforcement and InfoMax Learning"], "answer_arxiv_id": ["2006.01096", "2011.01089", "2102.07097", "1910.12911", "2009.08319", "2006.07217"], "source_meta": {"published_time": "20230605"}, "qid": "AutoScholarQuery_test_13"}
{"question": "Could you provide me some works about fine-tuning LLMs to better response to visual instructions?", "answer": ["mPLUG-Owl: Modularization Empowers Large Language Models with\n  Multimodality", "Improved Baselines with Visual Instruction Tuning"], "answer_arxiv_id": ["2304.14178", "2310.03744"], "source_meta": {"published_time": "20240219"}, "qid": "AutoScholarQuery_test_14"}
{"question": "Could you mention some works that classify unsupervised segmentation into two categories: clustering based on invariance and clustering using pre-trained models?", "answer": ["PiCIE: Unsupervised Semantic Segmentation using Invariance and\n  Equivariance in Clustering", "Invariant Information Clustering for Unsupervised Image Classification\n  and Segmentation", "Unsupervised Semantic Segmentation with Self-supervised Object-centric\n  Representations", "ACSeg: Adaptive Conceptualization for Unsupervised Semantic Segmentation", "Unsupervised Semantic Segmentation by Distilling Feature Correspondences", "NamedMask: Distilling Segmenters from Complementary Foundation Models"], "answer_arxiv_id": ["2103.17070", "1807.06653", "2207.05027", "2210.05944", "2203.08414", "2209.11228"], "source_meta": {"published_time": "20230823"}, "qid": "AutoScholarQuery_test_15"}
{"question": "Could you provide me examples of the development of more sophisticated feature extractors that enhance Point Cloud processing?", "answer": ["PointConv: Deep Convolutional Networks on 3D Point Clouds", "Point Transformer", "Rethinking Network Design and Local Geometry in Point Cloud: A Simple\n  Residual MLP Framework", "An Image is Worth 16x16 Words: Transformers for Image Recognition at\n  Scale"], "answer_arxiv_id": ["1811.07246", "2012.09164", "2202.07123", "2010.11929"], "source_meta": {"published_time": "20240328"}, "qid": "AutoScholarQuery_test_16"}
{"question": "What are the papers related to face reenactment, specifically aimed at transferring facial expressions and movements?", "answer": ["Structure-Aware Motion Transfer with Deformable Anchor Model", "Thin-Plate Spline Motion Model for Image Animation", "Latent Image Animator: Learning to Animate Images via Latent Space\n  Navigation", "DPE: Disentanglement of Pose and Expression for General Video Portrait\n  Editing"], "answer_arxiv_id": ["2204.05018", "2203.14367", "2203.09043", "2301.06281"], "source_meta": {"published_time": "20231228"}, "qid": "AutoScholarQuery_test_17"}
{"question": "What papers propose the use of spatiotemporal transformer for BEV generation?", "answer": ["BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera\n  Images via Spatiotemporal Transformers"], "answer_arxiv_id": ["2203.17270"], "source_meta": {"published_time": "20240313"}, "qid": "AutoScholarQuery_test_18"}
{"question": "Can you name some works that extend Global Descent for deep learning architectures?", "answer": ["A Convergence Theory for Deep Learning via Over-Parameterization", "Gradient Descent Finds Global Minima of Deep Neural Networks", "An Improved Analysis of Training Over-parameterized Deep Neural Networks"], "answer_arxiv_id": ["1811.03962", "1811.03804", "1906.04688"], "source_meta": {"published_time": "20220613"}, "qid": "AutoScholarQuery_test_19"}
{"question": "Could you provide me large multimodal models (LMMs) references?", "answer": ["Visual Instruction Tuning", "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image\n  Encoders and Large Language Models", "Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic", "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large\n  Language Models", "Qwen Technical Report"], "answer_arxiv_id": ["2304.08485", "2301.12597", "2306.15195", "2304.10592", "2309.16609v1"], "source_meta": {"published_time": "20231206"}, "qid": "AutoScholarQuery_test_20"}
{"question": "Could you provide me studies about achieving local editing by involving semantic masks as intermediate representations?", "answer": ["FENeRF: Face Editing in Neural Radiance Fields", "IDE-3D: Interactive Disentangled Editing for High-Resolution 3D-aware\n  Portrait Synthesis"], "answer_arxiv_id": ["2111.15490", "2205.15517"], "source_meta": {"published_time": "20231203"}, "qid": "AutoScholarQuery_test_21"}
{"question": "Which works propose methods for feature matching by detecting and describing keypoints on images?", "answer": ["SuperPoint: Self-Supervised Interest Point Detection and Description", "D2-Net: A Trainable CNN for Joint Detection and Description of Local Features", "R2D2: Repeatable and Reliable Detector and Descriptor"], "answer_arxiv_id": ["1712.07629", "1905.03561", "1906.06195"], "source_meta": {"published_time": "20230627"}, "qid": "AutoScholarQuery_test_22"}
{"question": "Which work first demonstrated the possibility of reconstructing accurate 3D full-body motion using only six IMUs?", "answer": ["Sparse Inertial Poser: Automatic 3D Human Pose Estimation from Sparse\n  IMUs"], "answer_arxiv_id": ["1703.08014"], "source_meta": {"published_time": "20240306"}, "qid": "AutoScholarQuery_test_23"}
{"question": "Could you provide me a work that extended the minimax method to deep neural networks?", "answer": ["Stochastic AUC Maximization with Deep Neural Networks"], "answer_arxiv_id": ["1908.10831"], "source_meta": {"published_time": "20230420"}, "qid": "AutoScholarQuery_test_24"}
{"question": "Any works that have commented on the challenge of training the PRM due to expensive human-annotated datasets?", "answer": ["Solving math word problems with process- and outcome-based feedback", "Let's Verify Step by Step"], "answer_arxiv_id": ["2211.14275", "2305.20050"], "source_meta": {"published_time": "20231214"}, "qid": "AutoScholarQuery_test_25"}
{"question": "Which papers are known for initially representing 3D scenes with a set of 3D Gaussians?", "answer": ["3D Gaussian Splatting for Real-Time Radiance Field Rendering"], "answer_arxiv_id": ["2308.04079"], "source_meta": {"published_time": "20231226"}, "qid": "AutoScholarQuery_test_26"}
{"question": "Which works proposed architectures for group equivariance in image classification?", "answer": ["Group Equivariant Convolutional Networks", "Steerable CNNs", "Group Equivariant Stand-Alone Self-Attention For Vision"], "answer_arxiv_id": ["1602.07576", "1612.08498", "2010.00977v2"], "source_meta": {"published_time": "20230517"}, "qid": "AutoScholarQuery_test_27"}
{"question": "What papers mention the increased computational complexity and decreased utility due to DPSGD?", "answer": ["Deep Learning with Differential Privacy", "Differentially Private Learning Needs Better Features (or Much More Data)"], "answer_arxiv_id": ["1607.00133", "2011.11660"], "source_meta": {"published_time": "20230524"}, "qid": "AutoScholarQuery_test_28"}
{"question": "In which studies has it been demonstrated that multi-modal models are vulnerable to adversarial attacks?", "answer": ["Fusion is Not Enough: Single Modal Attacks on Fusion Models for 3D\n  Object Detection", "Towards Adversarial Attack on Vision-Language Pre-training Models", "Can audio-visual integration strengthen robustness under multimodal\n  attacks?", "Fooling Vision and Language Models Despite Localization and Attention\n  Mechanism", "Cycle-Consistency for Robust Visual Question Answering", "Explaining and Harnessing Adversarial Examples"], "answer_arxiv_id": ["2304.14614", "2206.09391", "2104.02000", "1709.08693", "1902.05660", "1412.6572"], "source_meta": {"published_time": "20240328"}, "qid": "AutoScholarQuery_test_29"}
{"question": "Could you provide me studies that expound the impossibility of identifying latent factors for i.i.d. nonlinearly-dependent data without labels or assumptions about the data generating process?", "answer": ["Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations", "Variational Autoencoders and Nonlinear ICA: A Unifying Framework"], "answer_arxiv_id": ["1811.12359", "1907.04809"], "source_meta": {"published_time": "20231108"}, "qid": "AutoScholarQuery_test_30"}
{"question": "Could you provide me an example where an open-source model was introduced for input-output unsafety detection for LLMs?", "answer": ["Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations"], "answer_arxiv_id": ["2312.06674"], "source_meta": {"published_time": "20240221"}, "qid": "AutoScholarQuery_test_31"}
{"question": "What papers focused on source data estimation or self-training for pinhole images in the context of SFUDA?", "answer": ["Generalize then Adapt: Source-Free Domain Adaptive Semantic Segmentation", "Source-Free Open Compound Domain Adaptation in Semantic Segmentation", "Source-Free Domain Adaptation for Semantic Segmentation", "Source-Free Domain Adaptation for Image Segmentation"], "answer_arxiv_id": ["2108.11249", "2106.03422", "2103.16372", "2108.03152"], "source_meta": {"published_time": "20240319"}, "qid": "AutoScholarQuery_test_32"}
{"question": "Can you provide some works about predicting the contact map, the distance map and/or the torsion angles between protein residues?", "answer": ["Accurate De Novo Prediction of Protein Contact Map by Ultra-Deep Learning Model"], "answer_arxiv_id": ["1609.00680v6"], "source_meta": {"published_time": "20221012"}, "qid": "AutoScholarQuery_test_33"}
{"question": "What paper explored the application of VLMs, specifically CLIP, for BEV retrieval tasks?", "answer": ["BEV-TSR: Text-Scene Retrieval in BEV Space for Autonomous Driving"], "answer_arxiv_id": ["2401.01065"], "source_meta": {"published_time": "20240313"}, "qid": "AutoScholarQuery_test_34"}
{"question": "Could you list research that demonstrated the advantages of Quantization-Aware Training (QAT), which can enable the model to learn better representations for low-bit weights?", "answer": ["LLM-QAT: Data-Free Quantization Aware Training for Large Language Models", "OmniQuant: Omnidirectionally Calibrated Quantization for Large Language\n  Models", "PB-LLM: Partially Binarized Large Language Models", "BitNet: Scaling 1-bit Transformers for Large Language Models"], "answer_arxiv_id": ["2305.17888v1", "2308.13137", "2310.00034", "2310.11453"], "source_meta": {"published_time": "20240216"}, "qid": "AutoScholarQuery_test_35"}
{"question": "What are the researches that have explored the application of Crypto-based Private Learning in privacy-preserving machine learning?", "answer": ["Privacy-Preserving Machine Learning with Fully Homomorphic Encryption\n  for Deep Neural Network"], "answer_arxiv_id": ["2106.07229"], "source_meta": {"published_time": "20231205"}, "qid": "AutoScholarQuery_test_36"}
{"question": "Any works that focus on augmenting sparse inputs with synthetically generated views?", "answer": ["Ray Priors through Reprojection: Improving Neural Radiance Fields for\n  Novel View Extrapolation", "VM-NeRF: Tackling Sparsity in NeRF with View Morphing", "GeCoNeRF: Few-shot Neural Radiance Fields via Geometric Consistency"], "answer_arxiv_id": ["2205.05922", "2210.04214", "2301.10941"], "source_meta": {"published_time": "20240326"}, "qid": "AutoScholarQuery_test_37"}
{"question": "Which work introduces Point-E, a language-guided DM?", "answer": ["Point-E: A System for Generating 3D Point Clouds from Complex Prompts"], "answer_arxiv_id": ["2212.08751"], "source_meta": {"published_time": "20240331"}, "qid": "AutoScholarQuery_test_38"}
{"question": "Which papers discuss the practical applicability of black-box and transfer-based threat model, and the related security and safety risks?", "answer": ["Practical Black-Box Attacks against Machine Learning", "Boosting Adversarial Attacks with Momentum"], "answer_arxiv_id": ["1602.02697", "1710.06081"], "source_meta": {"published_time": "20220924"}, "qid": "AutoScholarQuery_test_39"}
{"question": "What studies develop hierarchical models in relation to diffusion models?", "answer": ["Photorealistic Text-to-Image Diffusion Models with Deep Language\n  Understanding", "Hierarchical Text-Conditional Image Generation with CLIP Latents", "Cascaded Diffusion Models for High Fidelity Image Generation"], "answer_arxiv_id": ["2205.11487", "2204.06125", "2106.15282"], "source_meta": {"published_time": "20231130"}, "qid": "AutoScholarQuery_test_40"}
{"question": "What are the papers that analyze the limitations of simple random walks on the clique expansion of the hypergraph?", "answer": ["Hyper-SAGNN: a self-attention based graph neural network for hypergraphs", "Neural Predicting Higher-order Patterns in Temporal Networks"], "answer_arxiv_id": ["1911.02613", "2106.06039"], "source_meta": {"published_time": "20230619"}, "qid": "AutoScholarQuery_test_41"}
{"question": "Which study explicitly determines and measures the faithfulness of explanations in LLMs?", "answer": ["Question Decomposition Improves the Faithfulness of Model-Generated\n  Reasoning"], "answer_arxiv_id": ["2307.11768"], "source_meta": {"published_time": "20231113"}, "qid": "AutoScholarQuery_test_42"}
{"question": "Which study argued on the difficulties of implementing a GAN-like procedure using the dual form of UOT?", "answer": ["Robust Optimal Transport with Applications in Generative Modeling and Domain Adaptation"], "answer_arxiv_id": ["2010.05862"], "source_meta": {"published_time": "20230524"}, "qid": "AutoScholarQuery_test_43"}
{"question": "What studies deal with standard feature selection that selects the same subset of features for each data sample?", "answer": ["Feature Selection: A Data Perspective"], "answer_arxiv_id": ["1601.07996"], "source_meta": {"published_time": "20230527"}, "qid": "AutoScholarQuery_test_44"}
{"question": "What works are related to the use of commonsense knowledge in Knowledge Graphs?", "answer": ["ConceptNet 5.5: An Open Multilingual Graph of General Knowledge"], "answer_arxiv_id": ["1612.03975"], "source_meta": {"published_time": "20231024"}, "qid": "AutoScholarQuery_test_45"}
{"question": "What works discuss the lack of robustness in NLP benchmarks?", "answer": ["When Benchmarks are Targets: Revealing the Sensitivity of Large Language\n  Model Leaderboards"], "answer_arxiv_id": ["2402.01781"], "source_meta": {"published_time": "20240613"}, "qid": "AutoScholarQuery_test_46"}
{"question": "Which papers examined pretraining on scientific text corpora?", "answer": ["SciBERT: A Pretrained Language Model for Scientific Text", "Domain-Specific Language Model Pretraining for Biomedical Natural\n  Language Processing", "BioBERT: a pre-trained biomedical language representation model for biomedical text mining", "ClinicalBERT: Modeling Clinical Notes and Predicting Hospital\n  Readmission"], "answer_arxiv_id": ["1903.10676", "2007.15779", "1901.08746v4", "1904.05342"], "source_meta": {"published_time": "20230603"}, "qid": "AutoScholarQuery_test_47"}
{"question": "Which studies apply model-agnostic meta learning (MAML) to deep anomaly detector models?", "answer": ["Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks", "Few-Shot One-Class Classification via Meta-Learning", "Few-Shot Scene-Adaptive Anomaly Detection", "Few-shot Network Anomaly Detection via Cross-network Meta-learning"], "answer_arxiv_id": ["1703.03400", "2007.04146", "2007.07843", "2102.11165"], "source_meta": {"published_time": "20230215"}, "qid": "AutoScholarQuery_test_48"}
{"question": "What works have proposed guidelines for documenting ML datasets?", "answer": ["Datasheets for Datasets"], "answer_arxiv_id": ["1803.09010"], "source_meta": {"published_time": "20240613"}, "qid": "AutoScholarQuery_test_49"}
{"question": "Which papers focused on locally aligning fixed patches with textual words?", "answer": ["ViLT: Vision-and-Language Transformer Without Convolution or Region\n  Supervision", "FILIP: Fine-grained Interactive Language-Image Pre-Training", "Improving Joint Learning of Chest X-Ray and Radiology Report by Word\n  Region Alignment", "Multi-Granularity Cross-modal Alignment for Generalized Medical Visual\n  Representation Learning"], "answer_arxiv_id": ["2102.03334", "2111.07783", "2109.01949", "2210.06044"], "source_meta": {"published_time": "20231213"}, "qid": "AutoScholarQuery_test_50"}
{"question": "What is the fundamental work on fully convolutional networks (FCNs) used for deep learning-based semantic segmentation?", "answer": ["Fully Convolutional Networks for Semantic Segmentation"], "answer_arxiv_id": ["1411.4038"], "source_meta": {"published_time": "20230227"}, "qid": "AutoScholarQuery_test_51"}
{"question": "Which works focused on ray-based rendering for novel view synthesis approach?", "answer": ["Stereo Radiance Fields (SRF): Learning View Synthesis for Sparse Views\n  of Novel Scenes", "IBRNet: Learning Multi-View Image-Based Rendering", "Generalizable Patch-Based Neural Rendering", "Is Attention All That NeRF Needs?", "Explicit Correspondence Matching for Generalizable Neural Radiance\n  Fields"], "answer_arxiv_id": ["2104.06935", "2102.13090", "2207.10662", "2207.13298", "2304.12294"], "source_meta": {"published_time": "20231207"}, "qid": "AutoScholarQuery_test_52"}
{"question": "Which papers contribute to the advancement of model-based reinforcement learning through the study of the world model?", "answer": ["Recurrent World Models Facilitate Policy Evolution", "Learning Latent Dynamics for Planning from Pixels", "Dream to Control: Learning Behaviors by Latent Imagination", "Learning to Fly via Deep Model-Based Reinforcement Learning", "Mastering Atari with Discrete World Models", "Mastering Diverse Domains through World Models", "Model Based Reinforcement Learning for Atari", "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model"], "answer_arxiv_id": ["1809.01999", "1811.04551", "1912.01603", "2003.08876", "2010.02193", "2301.04104", "1903.00374", "1911.08265"], "source_meta": {"published_time": "20231204"}, "qid": "AutoScholarQuery_test_53"}
{"question": "Could you provide me some studies proposing models for learning latent graphs?", "answer": ["Dynamic Graph CNN for Learning on Point Clouds", "Latent-Graph Learning for Disease Prediction", "Differentiable Graph Module (DGM) for Graph Convolutional Networks"], "answer_arxiv_id": ["1801.07829", "2003.13620", "2002.04999"], "source_meta": {"published_time": "20221126"}, "qid": "AutoScholarQuery_test_54"}
{"question": "Which study proposed a method that works only on toy images of up to 333 objects on a black background?", "answer": ["Learning Object-Centric Video Models by Contrasting Sets"], "answer_arxiv_id": ["2011.10287"], "source_meta": {"published_time": "20230524"}, "qid": "AutoScholarQuery_test_55"}
{"question": "Any work about applying re-reading prompt to improve reasoning tasks of LLM?", "answer": ["Re-Reading Improves Reasoning in Large Language Models"], "answer_arxiv_id": ["2309.06275"], "source_meta": {"published_time": "20240228"}, "qid": "AutoScholarQuery_test_56"}
{"question": "What studies introduce the unsupervised disentanglement score called Distortion?", "answer": ["Analyzing the Latent Space of GAN through Local Dimension Estimation"], "answer_arxiv_id": ["2205.13182"], "source_meta": {"published_time": "20221011"}, "qid": "AutoScholarQuery_test_57"}
{"question": "Which research leveraged large language models like GPT-3 to learn a proxy reward function while avoiding the need for many expert demonstrations?", "answer": ["Language Models are Few-Shot Learners"], "answer_arxiv_id": ["2005.14165"], "source_meta": {"published_time": "20230227"}, "qid": "AutoScholarQuery_test_58"}
{"question": "What papers used a predefined set of names for enhancing cross-style transfer?", "answer": ["Rethinking the Role of Demonstrations: What Makes In-Context Learning\n  Work?", "Description-Driven Task-Oriented Dialog Modeling", "Larger language models do in-context learning differently"], "answer_arxiv_id": ["2202.12837", "2201.08904", "2303.03846v2"], "source_meta": {"published_time": "20230524"}, "qid": "AutoScholarQuery_test_59"}
{"question": "Which studies have recently been working on the integration of visual perception and large language models?", "answer": ["Attention Is All You Need", "Language Models are Few-Shot Learners", "GPT-4 Technical Report", "LLaMA: Open and Efficient Foundation Language Models", "Llama 2: Open Foundation and Fine-Tuned Chat Models"], "answer_arxiv_id": ["1706.03762", "2005.14165", "2303.08774", "2302.13971", "2307.09288"], "source_meta": {"published_time": "20231204"}, "qid": "AutoScholarQuery_test_60"}
{"question": "What papers introduced the fast gradient sign method (FGSM) and the basic iterative method (BIM) for adversarial attacks?", "answer": ["Explaining and Harnessing Adversarial Examples", "Adversarial examples in the physical world"], "answer_arxiv_id": ["1412.6572", "1607.02533"], "source_meta": {"published_time": "20220924"}, "qid": "AutoScholarQuery_test_61"}
{"question": "Any works talked about the use of meta-gradients to learn a combination of hyperparameters?", "answer": ["Bootstrapped Meta-Learning"], "answer_arxiv_id": ["2109.04504v2"], "source_meta": {"published_time": "20230602"}, "qid": "AutoScholarQuery_test_62"}
{"question": "Are there any works that improve cost-effectiveness, performance, and data generation quality in the prompting framework of large language models?", "answer": ["ReWOO: Decoupling Reasoning from Observations for Efficient Augmented\n  Language Models", "Reflexion: Language Agents with Verbal Reinforcement Learning", "MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action", "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world\n  APIs", "ToolAlpaca: Generalized Tool Learning for Language Models with 3000\n  Simulated Cases"], "answer_arxiv_id": ["2305.18323", "2303.11366", "2303.11381", "2307.16789", "2306.05301"], "source_meta": {"published_time": "20240223"}, "qid": "AutoScholarQuery_test_63"}
{"question": "In which paper the term FPE was formalised for general function approximators?", "answer": ["Batch Policy Learning under Constraints"], "answer_arxiv_id": ["1903.08738"], "source_meta": {"published_time": "20230224"}, "qid": "AutoScholarQuery_test_64"}
{"question": "Which works focus on modelling the annotator distribution?", "answer": ["PHiSeg: Capturing Uncertainty in Medical Image Segmentation", "A Hierarchical Probabilistic U-Net for Modeling Multi-Scale Ambiguities", "Uncertainty quantification in medical image segmentation with normalizing flows"], "answer_arxiv_id": ["1906.04045", "1905.13077", "2006.02683"], "source_meta": {"published_time": "20230328"}, "qid": "AutoScholarQuery_test_65"}
{"question": "Which studies designed a siamese network framework using AlexNet for feature extraction in visual object tracking?", "answer": ["Fully-Convolutional Siamese Networks for Object Tracking"], "answer_arxiv_id": ["1606.09549"], "source_meta": {"published_time": "20240315"}, "qid": "AutoScholarQuery_test_66"}
{"question": "What graph analysis model is tested in the benchmark?", "answer": ["Explainable Classification of Brain Networks via Contrast Subgraphs"], "answer_arxiv_id": ["2006.05176"], "source_meta": {"published_time": "20221111"}, "qid": "AutoScholarQuery_test_67"}
{"question": "Any research focused on the memorization risks during the fine-tuning stage?", "answer": ["Memorization in NLP Fine-tuning Methods", "Do Language Models Plagiarize?"], "answer_arxiv_id": ["2205.12506", "2203.07618"], "source_meta": {"published_time": "20231010"}, "qid": "AutoScholarQuery_test_68"}
{"question": "Could you provide me some studies about reducing the gradient misestimation by approximating discrete quantization with a differentiable function?", "answer": ["Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit Neural Networks", "DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients"], "answer_arxiv_id": ["1908.05033", "1606.06160"], "source_meta": {"published_time": "20230204"}, "qid": "AutoScholarQuery_test_69"}
{"question": "Could you provide me some works about optimizing batch processing for LLMs?", "answer": ["Batch Prompting: Efficient Inference with Large Language Model APIs", "TurboTransformers: An Efficient GPU Serving System For Transformer Models"], "answer_arxiv_id": ["2301.08721", "2010.05680"], "source_meta": {"published_time": "20230522"}, "qid": "AutoScholarQuery_test_70"}
{"question": "Which study extended the capabilities of LLMs to the field of multi-modality?", "answer": ["Gemini: A Family of Highly Capable Multimodal Models"], "answer_arxiv_id": ["2312.11805v4"], "source_meta": {"published_time": "20240228"}, "qid": "AutoScholarQuery_test_71"}
{"question": "What research has been done on finding optimal interventions using observational data?", "answer": ["Learning to search efficiently for causally near-optimal treatments"], "answer_arxiv_id": ["2007.00973"], "source_meta": {"published_time": "20230531"}, "qid": "AutoScholarQuery_test_72"}
{"question": "What papers are about prototypical adaptation methods?", "answer": ["Bending Reality: Distortion-aware Transformers for Adapting to Panoramic\n  Semantic Segmentation", "Behind Every Domain There is a Shift: Adapting Distortion-aware Vision\n  Transformers for Panoramic Semantic Segmentation"], "answer_arxiv_id": ["2203.01452", "2207.11860"], "source_meta": {"published_time": "20240319"}, "qid": "AutoScholarQuery_test_73"}
{"question": "Could you name the works that applied CLIP for zero-shot AD, scoring the anomalies by comparing the alignment of test images with the correct text of normal samples?", "answer": ["Exposing Outlier Exposure: What Can Be Learned From Few, One, and Zero Outlier Images"], "answer_arxiv_id": ["2205.11474"], "source_meta": {"published_time": "20230215"}, "qid": "AutoScholarQuery_test_74"}
{"question": "What papers illustrate recent neural scene representations methods that try to optimize poses with differentiable rendering in Structure-from-Motion research?", "answer": ["BARF : Bundle-Adjusting Neural Radiance Fields", "Self-Calibrating Neural Radiance Fields"], "answer_arxiv_id": ["2104.06405", "2108.13826"], "source_meta": {"published_time": "20230627"}, "qid": "AutoScholarQuery_test_75"}
{"question": "Could you provide me some works that investigate the interplay between weight loss landscape and adversarial robustness?", "answer": ["Adversarial Weight Perturbation Helps Robust Generalization", "Enhancing Adversarial Training with Second-Order Statistics of Weights"], "answer_arxiv_id": ["2004.05884", "2203.06020"], "source_meta": {"published_time": "20240326"}, "qid": "AutoScholarQuery_test_76"}
{"question": "Which works employed a dynamic weighting transformer for integration in MMEA?", "answer": ["MEAformer: Multi-modal Entity Alignment Transformer for Meta Modality\n  Hybrid"], "answer_arxiv_id": ["2212.14454"], "source_meta": {"published_time": "20240723"}, "qid": "AutoScholarQuery_test_77"}
{"question": "Which works have been conducted on memory methods for object navigation tasks?", "answer": ["SOON: Scenario Oriented Object Navigation with Graph-based Exploration"], "answer_arxiv_id": ["2103.17138"], "source_meta": {"published_time": "20230203"}, "qid": "AutoScholarQuery_test_78"}
{"question": "Which study presents the use of synthetic captions for training BLIP and BLIP2 models?", "answer": ["BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation", "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"], "answer_arxiv_id": ["2201.12086", "2301.12597"], "source_meta": {"published_time": "20230719"}, "qid": "AutoScholarQuery_test_79"}
{"question": "Could you provide me some works about multi-agent debating frameworks?", "answer": ["Improving Factuality and Reasoning in Language Models through Multiagent\n  Debate", "Encouraging Divergent Thinking in Large Language Models through\n  Multi-Agent Debate", "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate"], "answer_arxiv_id": ["2305.14325", "2305.19118", "2308.07201"], "source_meta": {"published_time": "20230922"}, "qid": "AutoScholarQuery_test_80"}
{"question": "Which research provide examples of multimodal-conditional image synthesis systems?", "answer": ["High-Resolution Image Synthesis with Latent Diffusion Models", "eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert\n  Denoisers", "Adding Conditional Control to Text-to-Image Diffusion Models", "GLIGEN: Open-Set Grounded Text-to-Image Generation"], "answer_arxiv_id": ["2112.10752", "2211.01324", "2302.05543", "2301.07093"], "source_meta": {"published_time": "20230406"}, "qid": "AutoScholarQuery_test_81"}
{"question": "Which studies showed successful results using group-level persona variables?", "answer": ["Jury Learning: Integrating Dissenting Voices into Machine Learning\n  Models", "When the Majority is Wrong: Modeling Annotator Disagreement for\n  Subjective Tasks"], "answer_arxiv_id": ["2202.02950", "2305.06626"], "source_meta": {"published_time": "20240216"}, "qid": "AutoScholarQuery_test_82"}
{"question": "Could you provide me with works that discuss the problem of performance degradation when distilling larger LMs, especially when the student is of small scale?", "answer": ["Improved Knowledge Distillation via Teacher Assistant", "On the Efficacy of Knowledge Distillation", "Lifting the Curse of Capacity Gap in Distilling Language Models"], "answer_arxiv_id": ["1902.03393", "1910.01348", "2305.12129"], "source_meta": {"published_time": "20240219"}, "qid": "AutoScholarQuery_test_83"}
{"question": "Could you provide me some works about generative methods for transferable adversarial attacks?", "answer": ["Generative Adversarial Perturbations", "Cross-Domain Transferability of Adversarial Perturbations", "On Generating Transferable Targeted Perturbations"], "answer_arxiv_id": ["1712.02328v3", "1905.11736", "2103.14641"], "source_meta": {"published_time": "20230223"}, "qid": "AutoScholarQuery_test_84"}
{"question": "What is the key work on Trust Region Policy Optimization?", "answer": ["Trust Region Policy Optimization"], "answer_arxiv_id": ["1502.05477"], "source_meta": {"published_time": "20230130"}, "qid": "AutoScholarQuery_test_85"}
{"question": "What works focus on spatial feature transformation for BEV feature generation?", "answer": ["PersFormer: 3D Lane Detection via Perspective Transformer and the\n  OpenLane Benchmark"], "answer_arxiv_id": ["2203.11089"], "source_meta": {"published_time": "20240313"}, "qid": "AutoScholarQuery_test_86"}
{"question": "What work used a modified VQ-GAN for isolated word sign language video generation?", "answer": ["Continuous 3D Multi-Channel Sign Language Production via Progressive\n  Transformers and Mixture Density Networks"], "answer_arxiv_id": ["2103.06982"], "source_meta": {"published_time": "20240611"}, "qid": "AutoScholarQuery_test_87"}
{"question": "What papers propose the use of FP8 for accelerated inference?", "answer": ["FP8 Quantization: The Power of the Exponent"], "answer_arxiv_id": ["2208.09225"], "source_meta": {"published_time": "20230320"}, "qid": "AutoScholarQuery_test_88"}
{"question": "Who analysed the NTK spectrum for shallow ReLU networks under the uniform and nonuniform distributions?", "answer": ["Frequency Bias in Neural Networks for Input of Non-Uniform Density"], "answer_arxiv_id": ["2003.04560"], "source_meta": {"published_time": "20221115"}, "qid": "AutoScholarQuery_test_89"}
{"question": "Which works explored the theoretical analysis of the NTK spectrum via random matrix theory?", "answer": ["Spectra of the Conjugate Kernel and Neural Tangent Kernel for Linear-Width Neural Networks"], "answer_arxiv_id": ["2005.11879"], "source_meta": {"published_time": "20221115"}, "qid": "AutoScholarQuery_test_90"}
{"question": "Any research work about directly predicting CNN classifier accuracy by deriving distribution distance features between training and test images with a linear regression model?", "answer": ["Are Labels Always Necessary for Classifier Accuracy Evaluation?", "What Does Rotation Prediction Tell Us about Classifier Accuracy under Varying Testing Environments?"], "answer_arxiv_id": ["2007.02915", "2106.05961"], "source_meta": {"published_time": "20231023"}, "qid": "AutoScholarQuery_test_91"}
{"question": "What works feature insightful discussions on preconditioning?", "answer": ["When Does Preconditioning Help or Hurt Generalization?", "Preconditioned Score-based Generative Models", "Deep Residual Learning for Image Recognition"], "answer_arxiv_id": ["2006.10732", "2302.06504", "1512.03385"], "source_meta": {"published_time": "20230531"}, "qid": "AutoScholarQuery_test_92"}
{"question": "Which paper introduced Vector Quantized Variational Autoencoders (VQ-VAE)?", "answer": ["Neural Discrete Representation Learning"], "answer_arxiv_id": ["1711.00937"], "source_meta": {"published_time": "20240611"}, "qid": "AutoScholarQuery_test_93"}
{"question": "Which research introduced a graph generation method for query structure prediction in parsing?", "answer": ["Formal Query Building with Query Structure Prediction for Complex Question Answering over Knowledge Base"], "answer_arxiv_id": ["2109.03614"], "source_meta": {"published_time": "20231024"}, "qid": "AutoScholarQuery_test_94"}
{"question": "Could you provide some works about deep AD approaches that employ a self-supervised loss function to train the detector and score anomalies?", "answer": ["Deep Anomaly Detection Using Geometric Transformations", "Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty", "Learning and Evaluating Representations for Deep One-class Classification", "Classification-Based Anomaly Detection for General Data", "Neural Transformation Learning for Deep Anomaly Detection Beyond Images", "Detecting Anomalies within Time Series using Local Neural Transformations", "Deep Anomaly Detection under Labeling Budget Constraints"], "answer_arxiv_id": ["1805.10917", "1906.12340", "2011.02578", "2005.02359", "2103.16440", "2202.03944", "2302.07832v2"], "source_meta": {"published_time": "20230215"}, "qid": "AutoScholarQuery_test_95"}
{"question": "What studies have proposed methods to facilitate better model and AI service documentation?", "answer": ["Model Cards for Model Reporting"], "answer_arxiv_id": ["1810.03993"], "source_meta": {"published_time": "20240613"}, "qid": "AutoScholarQuery_test_96"}
{"question": "Which study offers a lightweight, subject-driven personalization for text-to-image diffusion models?", "answer": ["HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image\n  Models"], "answer_arxiv_id": ["2307.06949"], "source_meta": {"published_time": "20231211"}, "qid": "AutoScholarQuery_test_97"}
{"question": "What works present operators of tensor decomposition composed of fast Fourier / trigonometric transforms?", "answer": ["Faster Johnson-Lindenstrauss Transforms via Kronecker Products"], "answer_arxiv_id": ["1909.04801"], "source_meta": {"published_time": "20230129"}, "qid": "AutoScholarQuery_test_98"}
{"question": "What paper describes the dataset MiniWoB++, where sequences of low-level UI commands describe multi-step tasks?", "answer": ["Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration"], "answer_arxiv_id": ["1802.08802"], "source_meta": {"published_time": "20230719"}, "qid": "AutoScholarQuery_test_99"}
{"question": "Which paper proposed improving the anomaly score for reconstruction-based techniques via constructing a score that integrates reconstruction error and discriminator loss?", "answer": ["TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Data"], "answer_arxiv_id": ["2201.07284"], "source_meta": {"published_time": "20231024"}, "qid": "AutoScholarQuery_test_100"}
{"question": "What work proposes to model speech based on HuBERT codes or semantic tokens?", "answer": ["Generative Spoken Language Modeling from Raw Audio", "Speech Resynthesis from Discrete Disentangled Self-Supervised\n  Representations"], "answer_arxiv_id": ["2102.01192", "2104.00355"], "source_meta": {"published_time": "20240603"}, "qid": "AutoScholarQuery_test_101"}
{"question": "What research works talk about using Inverse Propensity Score (IPS) and Self-Normalized IPS (SNIPS) methods to tackle selection bias on data?", "answer": ["Recommendations as Treatments: Debiasing Learning and Evaluation"], "answer_arxiv_id": ["1602.05352"], "source_meta": {"published_time": "20220510"}, "qid": "AutoScholarQuery_test_102"}
{"question": "What paper introduced the Segment Anything Model (SAM) which is a foundational model for image segmentation?", "answer": ["Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks"], "answer_arxiv_id": ["2401.14159"], "source_meta": {"published_time": "20231226"}, "qid": "AutoScholarQuery_test_103"}
{"question": "Which publications contain the principal components of latent space obtained by performing PCA as global meaningful perturbations?", "answer": ["GANSpace: Discovering Interpretable GAN Controls"], "answer_arxiv_id": ["2004.02546"], "source_meta": {"published_time": "20221011"}, "qid": "AutoScholarQuery_test_104"}
{"question": "What studies introduced mask classification-based methods for instance-level segmentation?", "answer": ["End-to-End Object Detection with Transformers", "MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers"], "answer_arxiv_id": ["2005.12872", "2012.00759"], "source_meta": {"published_time": "20230227"}, "qid": "AutoScholarQuery_test_105"}
{"question": "What studies extensively examined the MMD two-sample test?", "answer": ["Learning Deep Kernels for Exponential Family Densities", "Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy", "Fast Two-Sample Testing with Analytic Representations of Probability Measures", "Interpretable Distribution Features with Maximum Testing Power"], "answer_arxiv_id": ["1811.08357", "1611.04488", "1506.04725", "1605.06796"], "source_meta": {"published_time": "20221004"}, "qid": "AutoScholarQuery_test_106"}
{"question": "What works are related to 3D Gaussian Splatting for 3D reconstruction?", "answer": ["3D Gaussian Splatting for Real-Time Radiance Field Rendering", "Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis"], "answer_arxiv_id": ["2308.04079", "2308.09713"], "source_meta": {"published_time": "20231128"}, "qid": "AutoScholarQuery_test_107"}
{"question": "Can you specify the studies about using prompt and fine-tuning techniques for adapting VLMs to a new downstream task?", "answer": ["Learning to Prompt for Vision-Language Models", "CLIP-Adapter: Better Vision-Language Models with Feature Adapters"], "answer_arxiv_id": ["2109.01134", "2110.04544"], "source_meta": {"published_time": "20240313"}, "qid": "AutoScholarQuery_test_108"}
{"question": "What works propose 2D-to-3D pose lifting task and regress the 3D keypoints based on a convolutional neural network from 2D keypoints?", "answer": ["A simple yet effective baseline for 3d human pose estimation"], "answer_arxiv_id": ["1705.03098"], "source_meta": {"published_time": "20230910"}, "qid": "AutoScholarQuery_test_109"}
{"question": "What researches have created evaluation data for individual tasks in Indic languages?", "answer": ["IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for all 22 Scheduled Indian Languages", "Mukhyansh: A Headline Generation Dataset for Indic Languages", "PMIndiaSum: Multilingual and Cross-lingual Headline Summarization for\n  Languages in India", "Samanantar: The Largest Publicly Available Parallel Corpora Collection for 11 Indic Languages"], "answer_arxiv_id": ["2305.16307v3", "2311.17743", "2305.08828", "2104.05596v4"], "source_meta": {"published_time": "20240425"}, "qid": "AutoScholarQuery_test_110"}
{"question": "What are some classic methods for learning latent graphs?", "answer": ["Learning Discrete Structures for Graph Neural Networks", "Iterative Deep Graph Learning for Graph Neural Networks: Better and Robust Node Embeddings", "Graph Structure Learning for Robust Graph Neural Networks"], "answer_arxiv_id": ["1903.11960", "2006.13009", "2005.10203"], "source_meta": {"published_time": "20221126"}, "qid": "AutoScholarQuery_test_111"}
{"question": "Which paper introduced the method known as CoT prompting?", "answer": ["Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"], "answer_arxiv_id": ["2201.11903"], "source_meta": {"published_time": "20231113"}, "qid": "AutoScholarQuery_test_112"}
{"question": "Which works assumed Gaussian noise in RGB space for pixel-wise uncertainty in the context of NeRF?", "answer": ["NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo\n  Collections", "ActiveNeRF: Learning where to See with Uncertainty Estimation"], "answer_arxiv_id": ["2008.02268", "2209.08546"], "source_meta": {"published_time": "20240326"}, "qid": "AutoScholarQuery_test_113"}
{"question": "What papers explored using diffusion models within the realm of reinforcement learning?", "answer": ["Planning with Diffusion for Flexible Behavior Synthesis"], "answer_arxiv_id": ["2205.09991"], "source_meta": {"published_time": "20230125"}, "qid": "AutoScholarQuery_test_114"}
{"question": "Could you provide me works that touch upon one-class classification approaches for video anomaly detection?", "answer": ["Old is Gold: Redefining the Adversarially Learned One-Class Classifier\n  Training Paradigm", "Adversarially Learned One-Class Classifier for Novelty Detection", "Memorizing Normality to Detect Anomaly: Memory-augmented Deep\n  Autoencoder for Unsupervised Anomaly Detection", "Object-centric Auto-encoders and Dummy Anomalies for Abnormal Event\n  Detection in Video"], "answer_arxiv_id": ["2004.07657", "1802.09088", "1904.02639", "1812.04960"], "source_meta": {"published_time": "20240401"}, "qid": "AutoScholarQuery_test_115"}
{"question": "Which studies have focused on the protein docking technique?", "answer": ["Independent SE(3)-Equivariant Models for End-to-End Rigid Protein Docking"], "answer_arxiv_id": ["2111.07786"], "source_meta": {"published_time": "20230201"}, "qid": "AutoScholarQuery_test_116"}
{"question": "What are some works in vision that stress the importance of data selection in supervised or semi-supervised setting?", "answer": ["Beyond neural scaling laws: beating power law scaling via data pruning", "Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt", "Learning From Less Data: A Unified Data Subset Selection and Active Learning Framework for Computer Vision", "Glister: Generalization based Data Subset Selection for Efficient and Robust Learning", "Grad-Match: Gradient Matching based Data Subset Selection for Efficient Deep Model Training", "RETRIEVE: Coreset Selection for Efficient and Robust Semi-Supervised Learning", "Optimizing Data Usage via Differentiable Rewards", "Deep Learning on a Data Diet: Finding Important Examples Early in Training", "Coresets for Data-efficient Training of Machine Learning Models", "Selection via Proxy: Efficient Data Selection for Deep Learning", "Active Learning for Convolutional Neural Networks: A Core-Set Approach"], "answer_arxiv_id": ["2206.14486v6", "2206.07137", "1901.01151", "2012.10630", "2103.00123", "2106.07760v2", "1911.10088", "2107.07075", "1906.01827", "1906.11829", "1708.00489"], "source_meta": {"published_time": "20230206"}, "qid": "AutoScholarQuery_test_117"}
{"question": "What works adopted large language models (LLMs) for a cost-effective generation of Counterfactually Augmented Data (CAD)?", "answer": ["Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and\n  Improving Models", "Generate Your Counterfactuals: Towards Controlled Counterfactual\n  Generation for Text", "AutoCAD: Automatically Generating Counterfactuals for Mitigating\n  Shortcut Learning", "CORE: A Retrieve-then-Edit Framework for Counterfactual Data Generation", "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search", "DISCO: Distilling Counterfactuals with Large Language Models"], "answer_arxiv_id": ["2101.00288", "2012.04698", "2211.16202", "2210.04873", "2305.03495", "2212.10534"], "source_meta": {"published_time": "20240609"}, "qid": "AutoScholarQuery_test_118"}
{"question": "Which studies demonstrated that vector arithmetic on latent space leads to the semantic arithmetic on the image space?", "answer": ["Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks", "Deep Feature Interpolation for Image Content Changes"], "answer_arxiv_id": ["1511.06434", "1611.05507"], "source_meta": {"published_time": "20221011"}, "qid": "AutoScholarQuery_test_119"}
{"question": "Which research papers cover the challenges of covariate shift and causal confusion in behavior cloning?", "answer": ["A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning", "Causal Confusion in Imitation Learning"], "answer_arxiv_id": ["1011.0686v3", "1905.11979"], "source_meta": {"published_time": "20231012"}, "qid": "AutoScholarQuery_test_120"}
{"question": "What papers focus on the concept of equivariance in Convolutional Neural Networks (CNNs)?", "answer": ["Group Equivariant Convolutional Networks"], "answer_arxiv_id": ["1602.07576"], "source_meta": {"published_time": "20230619"}, "qid": "AutoScholarQuery_test_121"}
{"question": "What papers discuss the definitions and concepts of Pseudo-Global Stability in regards to Differential Privacy?", "answer": ["User-Level Private Learning via Correlated Sampling"], "answer_arxiv_id": ["2110.11208"], "source_meta": {"published_time": "20230523"}, "qid": "AutoScholarQuery_test_122"}
{"question": "Which works focus on predicting model generalization error?", "answer": ["Are Labels Always Necessary for Classifier Accuracy Evaluation?", "Leveraging Unlabeled Data to Predict Out-of-Distribution Performance", "Predicting Out-of-Distribution Error with the Projection Norm", "On the Strong Correlation Between Model Invariance and Generalization", "Predicting with Confidence on Unseen Distributions", "What Does Rotation Prediction Tell Us about Classifier Accuracy under Varying Testing Environments?"], "answer_arxiv_id": ["2007.02915", "2201.04234", "2202.05834", "2207.07065", "2107.03315", "2106.05961"], "source_meta": {"published_time": "20231023"}, "qid": "AutoScholarQuery_test_123"}
{"question": "Which study proposed the Subformer model with shared middle layers and embedding factorization?", "answer": ["Subformer: Exploring Weight Sharing for Parameter Efficiency in\n  Generative Transformers"], "answer_arxiv_id": ["2101.00234"], "source_meta": {"published_time": "20240224"}, "qid": "AutoScholarQuery_test_124"}
{"question": "What studies have proven Generation-Augmented Retrieval effective in question answering and passage retrieval?", "answer": ["Generation-Augmented Retrieval for Open-domain Question Answering", "Precise Zero-Shot Dense Retrieval without Relevance Labels", "Query2doc: Query Expansion with Large Language Models"], "answer_arxiv_id": ["2009.08553", "2212.10496", "2303.07678"], "source_meta": {"published_time": "20240109"}, "qid": "AutoScholarQuery_test_125"}
{"question": "What paper introduces the Mirror Descent Modified Policy Iteration (MD-MPI) framework?", "answer": ["A Theory of Regularized Markov Decision Processes"], "answer_arxiv_id": ["1901.11275"], "source_meta": {"published_time": "20230130"}, "qid": "AutoScholarQuery_test_126"}
{"question": "What papers provide linear convergence guarantees of NPG and PMD in softmax tabular policy settings by adding regularization?", "answer": ["Fast Global Convergence of Natural Policy Gradient Methods with Entropy Regularization", "Policy Mirror Descent for Regularized Reinforcement Learning: A Generalized Framework with Linear Convergence", "Policy Mirror Descent for Reinforcement Learning: Linear Convergence, New Sampling Complexity, and Generalized Problem Classes", "Homotopic Policy Mirror Descent: Policy Convergence, Implicit Regularization, and Improved Sample Complexity"], "answer_arxiv_id": ["2007.06558", "2105.11066", "2102.00135", "2201.09457v9"], "source_meta": {"published_time": "20230130"}, "qid": "AutoScholarQuery_test_127"}
{"question": "Which papers established a basis of ARA using language models based on deep neural networks?", "answer": ["Supervised and Unsupervised Neural Approaches to Text Readability"], "answer_arxiv_id": ["1907.11779"], "source_meta": {"published_time": "20240603"}, "qid": "AutoScholarQuery_test_128"}
{"question": "Which works proposed a simple approach based on the statistics of the dataset for response length prediction?", "answer": ["Fast Structured Decoding for Sequence Models"], "answer_arxiv_id": ["1910.11555"], "source_meta": {"published_time": "20230522"}, "qid": "AutoScholarQuery_test_129"}
{"question": "Can you mention publications that implemented transformers in 2D-to-3D pose lifting?", "answer": ["3D Human Pose Estimation with Spatial and Temporal Transformers", "MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose\n  Estimation in Video", "PoseFormerV2: Exploring Frequency Domain for Efficient and Robust 3D\n  Human Pose Estimation"], "answer_arxiv_id": ["2103.10455", "2203.00859", "2303.17472"], "source_meta": {"published_time": "20240228"}, "qid": "AutoScholarQuery_test_130"}
{"question": "Can you provide an example of a paper that presents self-tuning algorithms?", "answer": ["A Self-Tuning Actor-Critic Algorithm"], "answer_arxiv_id": ["2002.12928"], "source_meta": {"published_time": "20230602"}, "qid": "AutoScholarQuery_test_131"}
{"question": "Which papers talk about using LLMs to interpret themselves or other ML models by providing numerical importances for their inputs?", "answer": ["Are Large Language Models Post Hoc Explainers?"], "answer_arxiv_id": ["2310.05797"], "source_meta": {"published_time": "20231113"}, "qid": "AutoScholarQuery_test_132"}
{"question": "Could you provide me studies about knowledge distillation methods for semantic segmentation focusing on preserving structural semantic relations?", "answer": ["Knowledge Adaptation for Efficient Semantic Segmentation", "Channel-wise Knowledge Distillation for Dense Prediction", "Cross-Image Relational Knowledge Distillation for Semantic Segmentation"], "answer_arxiv_id": ["1903.04688v1", "2011.13256v4", "2204.06986"], "source_meta": {"published_time": "20220529"}, "qid": "AutoScholarQuery_test_133"}
{"question": "What works have focused on evaluating API-use scenarios based on a separate LLM to assess the quality of examples?", "answer": ["ToolAlpaca: Generalized Tool Learning for Language Models with 3000\n  Simulated Cases", "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world\n  APIs"], "answer_arxiv_id": ["2306.05301", "2307.16789"], "source_meta": {"published_time": "20240223"}, "qid": "AutoScholarQuery_test_134"}
{"question": "Any works about user-annotations based image animation?", "answer": ["iPOKE: Poking a Still Image for Controlled Stochastic Video Synthesis", "Stochastic Latent Residual Video Prediction", "DragNUWA: Fine-grained Control in Video Generation by Integrating Text,\n  Image, and Trajectory", "VideoComposer: Compositional Video Synthesis with Motion Controllability", "ControlVideo: Training-free Controllable Text-to-Video Generation", "Motion-Conditioned Diffusion Model for Controllable Video Synthesis"], "answer_arxiv_id": ["2107.02790", "2002.09219", "2308.08089", "2306.02018", "2305.13077", "2304.14404"], "source_meta": {"published_time": "20230914"}, "qid": "AutoScholarQuery_test_135"}
{"question": "What works conducted studies on LegalBERT and CaseLaw-BERT that focused on the legal domain?", "answer": ["LEGAL-BERT: The Muppets straight out of Law School", "When Does Pretraining Help? Assessing Self-Supervised Learning for Law\n  and the CaseHOLD Dataset"], "answer_arxiv_id": ["2010.02559", "2104.08671"], "source_meta": {"published_time": "20230603"}, "qid": "AutoScholarQuery_test_136"}
{"question": "Are there any works that created datasets and systems for tasks related to information time-tracking?", "answer": ["Socially-Informed Timeline Generation for Complex Events", "Examining the State-of-the-Art in News Timeline Summarization"], "answer_arxiv_id": ["1606.05699", "2005.10107"], "source_meta": {"published_time": "20220727"}, "qid": "AutoScholarQuery_test_137"}
{"question": "Could you provide me studies about anomaly detection in federated learning particularly related to network security?", "answer": ["A Secure Federated Learning Framework for 5G Networks"], "answer_arxiv_id": ["2005.05752"], "source_meta": {"published_time": "20240401"}, "qid": "AutoScholarQuery_test_138"}
{"question": "What research focused on 3D instance segmentation with only 3D box annotation requirements?", "answer": ["Box2Mask: Weakly Supervised 3D Semantic Instance Segmentation Using\n  Bounding Boxes"], "answer_arxiv_id": ["2206.01203"], "source_meta": {"published_time": "20230325"}, "qid": "AutoScholarQuery_test_139"}
{"question": "Are there any papers that investigate the need for an initially annotated fraction of data to bootstrap an active learning method?", "answer": ["Making Your First Choice: To Address Cold Start Problem in Vision Active Learning", "You Never Get a Second Chance To Make a Good First Impression: Seeding Active Learning for 3D Semantic Segmentation", "Cold-start Active Learning through Self-supervised Language Modeling", "Addressing the Item Cold-start Problem by Attribute-driven Active Learning"], "answer_arxiv_id": ["2210.02442", "2304.11762v2", "2010.09535", "1805.09023"], "source_meta": {"published_time": "20231031"}, "qid": "AutoScholarQuery_test_140"}
{"question": "Can you provide works that evaluate RL agents by changing the initial states in the same environment?", "answer": ["Teacher algorithms for curriculum learning of Deep RL in continuously parameterized environments", "Sim-to-Real Transfer of Robotic Control with Dynamics Randomization", "Continual Reinforcement Learning with Complex Synapses"], "answer_arxiv_id": ["1910.07224", "1710.06537", "1802.07239"], "source_meta": {"published_time": "20230426"}, "qid": "AutoScholarQuery_test_141"}
{"question": "Which research paper introduced the DenseCL method that applies contrastive learning on patches with highest similarity?", "answer": ["Dense Contrastive Learning for Self-Supervised Visual Pre-Training"], "answer_arxiv_id": ["2011.09157"], "source_meta": {"published_time": "20230609"}, "qid": "AutoScholarQuery_test_142"}
{"question": "Which work outlines a way to calculate adversarial perturbations during training by first randomly perturbing the initial point then applying a single step of projected gradient descent?", "answer": ["Fast is better than free: Revisiting adversarial training"], "answer_arxiv_id": ["2001.03994"], "source_meta": {"published_time": "20230724"}, "qid": "AutoScholarQuery_test_143"}
{"question": "Could you provide some works that discuss multimodal prompting methods?", "answer": ["Large Language Models are Zero-Shot Reasoners", "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning\n  by Large Language Models", "Better Zero-Shot Reasoning with Self-Adaptive Prompting", "Language Models are Few-Shot Learners", "Rethinking the Role of Demonstrations: What Makes In-Context Learning\n  Work?", "A Survey on In-context Learning", "Fairness-guided Few-shot Prompting for Large Language Models", "ExpertPrompting: Instructing Large Language Models to be Distinguished\n  Experts", "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "Automatic Chain of Thought Prompting in Large Language Models", "Self-Consistency Improves Chain of Thought Reasoning in Language Models", "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "Graph of Thoughts: Solving Elaborate Problems with Large Language Models", "Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in\n  Language Models", "Boosting Logical Reasoning in Large Language Models through a New\n  Framework: The Graph of Thought"], "answer_arxiv_id": ["2205.11916", "2305.04091", "2305.14106", "2005.14165", "2202.12837", "2301.00234", "2303.13217", "2305.14688", "2201.11903", "2210.03493", "2203.11171", "2305.10601", "2308.09687", "2305.16582", "2308.08614"], "source_meta": {"published_time": "20231127"}, "qid": "AutoScholarQuery_test_144"}
{"question": "Which work proposes reweighting different activation channels based on the global context of the input samples?", "answer": ["Squeeze-and-Excitation Networks"], "answer_arxiv_id": ["1709.01507"], "source_meta": {"published_time": "20230402"}, "qid": "AutoScholarQuery_test_145"}
{"question": "What works focused on MAML and its variants?", "answer": ["Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks", "Sharp-MAML: Sharpness-Aware Model-Agnostic Meta Learning", "Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML", "Alpha MAML: Adaptive Model-Agnostic Meta-Learning", "Meta-Learning with Implicit Gradients"], "answer_arxiv_id": ["1703.03400", "2206.03996", "1909.09157", "1905.07435", "1909.04630"], "source_meta": {"published_time": "20230528"}, "qid": "AutoScholarQuery_test_146"}
{"question": "What research papers have addressed issues of equivariance by customizing kernel designs in the context of kernel methods and Gaussian processes (GPs)?", "answer": ["GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration", "Deep Kernel Learning", "Kernel Identification Through Transformers"], "answer_arxiv_id": ["1809.11165", "1511.02222", "2106.08185"], "source_meta": {"published_time": "20230619"}, "qid": "AutoScholarQuery_test_147"}
{"question": "What studies have leveraged extensive image-text pair datasets to broaden the detection vocabulary in Open-vocabulary detection?", "answer": ["Open-Vocabulary Object Detection Using Captions", "RegionCLIP: Region-based Language-Image Pretraining", "PromptDet: Towards Open-vocabulary Detection using Uncurated Images", "Grounded Language-Image Pre-training", "Learning Object-Language Alignments for Open-Vocabulary Object Detection", "DetCLIPv2: Scalable Open-Vocabulary Object Detection Pre-training via\n  Word-Region Alignment"], "answer_arxiv_id": ["2011.10678", "2112.09106", "2203.16513", "2112.03857", "2211.14843", "2304.04514"], "source_meta": {"published_time": "20231214"}, "qid": "AutoScholarQuery_test_148"}
{"question": "Could you mention research that addresses MMS approximations for suodular and subadditive valuations?", "answer": ["Approximation Algorithms for Maximin Fair Division"], "answer_arxiv_id": ["1703.01851"], "source_meta": {"published_time": "20230828"}, "qid": "AutoScholarQuery_test_149"}
{"question": "Which works can you provide that are focused on creating evaluation data on Indic languages?", "answer": ["Towards Leaving No Indic Language Behind: Building Monolingual Corpora,\n  Benchmark and Models for Indic Languages", "Naamapadam: A Large-Scale Named Entity Annotated Data for Indic\n  Languages", "MASSIVE: A 1M-Example Multilingual Natural Language Understanding\n  Dataset with 51 Typologically-Diverse Languages", "GLUECoS : An Evaluation Benchmark for Code-Switched NLP", "The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122\n  Language Variants"], "answer_arxiv_id": ["2212.05409", "2212.10168", "2204.08582", "2004.12376", "2308.16884"], "source_meta": {"published_time": "20240425"}, "qid": "AutoScholarQuery_test_150"}
{"question": "What work highlights the disconnect between benchmark results and real world impacts in NLP?", "answer": ["Machine Learning that Matters"], "answer_arxiv_id": ["1206.4656v1"], "source_meta": {"published_time": "20240613"}, "qid": "AutoScholarQuery_test_151"}
{"question": "What work applied CLIP to dense prediction tasks?", "answer": ["DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting"], "answer_arxiv_id": ["2112.01518"], "source_meta": {"published_time": "20230227"}, "qid": "AutoScholarQuery_test_152"}
{"question": "Which papers have proposed for extracting the specific style from reference images?", "answer": ["StyleAdapter: A Single-Pass LoRA-Free Model for Stylized Image\n  Generation", "Domain Enhanced Arbitrary Image Style Transfer via Contrastive Learning", "StyleDiffusion: Controllable Disentangled Style Transfer via Diffusion\n  Models", "Inversion-Based Style Transfer with Diffusion Models", "StyleDrop: Text-to-Image Generation in Any Style"], "answer_arxiv_id": ["2309.01770", "2205.09542", "2308.07863", "2211.13203", "2306.00983"], "source_meta": {"published_time": "20240329"}, "qid": "AutoScholarQuery_test_153"}
{"question": "What papers studied the construction of diffusion models for discrete categorical data using a categorical noise process?", "answer": ["Structured Denoising Diffusion Models in Discrete State-Spaces", "Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions"], "answer_arxiv_id": ["2107.03006", "2102.05379"], "source_meta": {"published_time": "20230531"}, "qid": "AutoScholarQuery_test_154"}
{"question": "Which works combine external knowledge from KGs into LLMs during the prompting stage?", "answer": ["Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering", "Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for\n  Knowledge-intensive Question Answering", "MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large\n  Language Models", "Reasoning on Graphs: Faithful and Interpretable Large Language Model\n  Reasoning", "Think-on-Graph: Deep and Responsible Reasoning of Large Language Model\n  on Knowledge Graph"], "answer_arxiv_id": ["2306.04136v1", "2308.13259", "2308.09729", "2310.01061", "2307.07697"], "source_meta": {"published_time": "20240614"}, "qid": "AutoScholarQuery_test_155"}
{"question": "What works have concentrated on generating free-form natural language explanations (NLEs) for justifying model predictions?", "answer": ["e-SNLI: Natural Language Inference with Natural Language Explanations", "WT5?! Training Text-to-Text Models to Explain their Predictions", "Teach Me to Explain: A Review of Datasets for Explainable Natural\n  Language Processing"], "answer_arxiv_id": ["1812.01193", "2004.14546", "2102.12060"], "source_meta": {"published_time": "20231113"}, "qid": "AutoScholarQuery_test_156"}
{"question": "What work proposes the intrinsic local dimension estimation scheme for the latent manifold through the robust rank estimate?", "answer": ["Analyzing the Latent Space of GAN through Local Dimension Estimation"], "answer_arxiv_id": ["2205.13182"], "source_meta": {"published_time": "20221011"}, "qid": "AutoScholarQuery_test_157"}
{"question": "Which paper first used the term hypercolumn in the context of neural network features?", "answer": ["Hypercolumns for Object Segmentation and Fine-grained Localization"], "answer_arxiv_id": ["1411.5752"], "source_meta": {"published_time": "20230523"}, "qid": "AutoScholarQuery_test_158"}
{"question": "Could you provide me some studies that used NeRFs for novel view synthesis and 3D scene reconstruction?", "answer": ["NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis", "Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields", "Instant Neural Graphics Primitives with a Multiresolution Hash Encoding"], "answer_arxiv_id": ["2003.08934", "2111.12077", "2201.05989"], "source_meta": {"published_time": "20240103"}, "qid": "AutoScholarQuery_test_159"}
{"question": "Could you provide me some works that incorporate ideas from multicalibration to improve conditional coverage?", "answer": ["Practical Adversarial Multivalid Conformal Prediction"], "answer_arxiv_id": ["2206.01067"], "source_meta": {"published_time": "20230731"}, "qid": "AutoScholarQuery_test_160"}
{"question": "Which papers have achieved progress in the field of graph contrastive learning?", "answer": ["Graph Contrastive Learning with Augmentations", "Adversarial Graph Augmentation to Improve Graph Contrastive Learning", "Graph Contrastive Learning Automated", "Adversarial Graph Contrastive Learning with Information Regularization", "Bringing Your Own View: Graph Contrastive Learning without Prefabricated Data Augmentations"], "answer_arxiv_id": ["2010.13902", "2106.05819", "2106.07594", "2202.06491", "2201.01702"], "source_meta": {"published_time": "20230508"}, "qid": "AutoScholarQuery_test_161"}
{"question": "Can you provide existing research that used contrastive methods in representation learning?", "answer": ["Deep Graph Infomax"], "answer_arxiv_id": ["1809.10341"], "source_meta": {"published_time": "20231003"}, "qid": "AutoScholarQuery_test_162"}
{"question": "What works introduced methodologies involving the learning of kernels to detect illuminant chromaticity?", "answer": ["Convolutional Color Constancy", "Fast Fourier Color Constancy"], "answer_arxiv_id": ["1507.00410", "1611.07596"], "source_meta": {"published_time": "20240228"}, "qid": "AutoScholarQuery_test_163"}
{"question": "Could you tell me what studies propose to bridge vision and language modalities through visual prompt generators?", "answer": ["Flamingo: a Visual Language Model for Few-Shot Learning", "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image\n  Encoders and Large Language Models", "InstructBLIP: Towards General-purpose Vision-Language Models with\n  Instruction Tuning", "Visual Instruction Tuning", "Language Is Not All You Need: Aligning Perception with Language Models"], "answer_arxiv_id": ["2204.14198", "2301.12597", "2305.06500", "2304.08485", "2302.14045"], "source_meta": {"published_time": "20240219"}, "qid": "AutoScholarQuery_test_164"}
{"question": "Could you provide me some studies about sign language translation and production using RWTH-Phoenix dataset?", "answer": ["Sign Language Transformers: Joint End-to-end Sign Language Recognition\n  and Translation", "Progressive Transformers for End-to-End Sign Language Production"], "answer_arxiv_id": ["2003.13830", "2004.14874"], "source_meta": {"published_time": "20231205"}, "qid": "AutoScholarQuery_test_165"}
{"question": "Which work talked about ODIN, a method where two networks jointly optimize the segmentation masks and representation of objects?", "answer": ["Object discovery and representation networks"], "answer_arxiv_id": ["2203.08777"], "source_meta": {"published_time": "20230524"}, "qid": "AutoScholarQuery_test_166"}
{"question": "Which works proposed modifications for discrete-time models, such as recurrent neural networks to handle irregular time series data?", "answer": ["On the Properties of Neural Machine Translation: Encoder–Decoder Approaches"], "answer_arxiv_id": ["1409.1259"], "source_meta": {"published_time": "20230303"}, "qid": "AutoScholarQuery_test_167"}
{"question": "Could you mention a study that generates diverse reasoning paths using CoT?", "answer": ["Self-Consistency Improves Chain of Thought Reasoning in Language Models"], "answer_arxiv_id": ["2203.11171"], "source_meta": {"published_time": "20230922"}, "qid": "AutoScholarQuery_test_168"}
{"question": "Could you provide me studies where the solution of the reverse-time SDE is theoretically derived?", "answer": ["Score-Based Generative Modeling through Stochastic Differential Equations"], "answer_arxiv_id": ["2011.13456"], "source_meta": {"published_time": "20220617"}, "qid": "AutoScholarQuery_test_169"}
{"question": "Could you provide me some researches about extensions for population-based training?", "answer": ["Population Based Training of Neural Networks", "A Generalized Framework for Population Based Training"], "answer_arxiv_id": ["1711.09846", "1902.01894"], "source_meta": {"published_time": "20230602"}, "qid": "AutoScholarQuery_test_170"}
{"question": "Any studies that focused on the problem of proving structured queries to knowledge base?", "answer": ["End-to-End Differentiable Proving", "Differentiable Reasoning on Large Knowledge Bases and Natural Language"], "answer_arxiv_id": ["1705.11040", "1912.10824"], "source_meta": {"published_time": "20230219"}, "qid": "AutoScholarQuery_test_171"}
{"question": "Which paper improved the scene text editing performance by incorporating stroke-level information?", "answer": ["Exploring Stroke-Level Modifications for Scene Text Editing"], "answer_arxiv_id": ["2212.01982"], "source_meta": {"published_time": "20230518"}, "qid": "AutoScholarQuery_test_172"}
{"question": "Which works implement the conceptual design of complex-valued activations for object binding in synchrony-based models?", "answer": ["Neuronal Synchrony in Complex-Valued Deep Networks", "Complex-Valued Autoencoders for Object Discovery"], "answer_arxiv_id": ["1312.6115", "2204.02075"], "source_meta": {"published_time": "20230524"}, "qid": "AutoScholarQuery_test_173"}
{"question": "Which works have implemented different kernels at various regions of the DP pair for single-task based deblurring?", "answer": ["Defocus Deblurring Using Dual-Pixel Data", "Learning to Reduce Defocus Blur by Realistically Modeling Dual-Pixel\n  Data", "Improving Single-Image Defocus Deblurring: How Dual-Pixel Images Help Through Multi-Task Learning"], "answer_arxiv_id": ["2005.00305", "2012.03255", "2108.05251v2"], "source_meta": {"published_time": "20230719"}, "qid": "AutoScholarQuery_test_174"}
{"question": "Where does the ControlNet, a finetuning model for synthetic face generation, discussed?", "answer": ["Adding Conditional Control to Text-to-Image Diffusion Models"], "answer_arxiv_id": ["2302.05543"], "source_meta": {"published_time": "20231228"}, "qid": "AutoScholarQuery_test_175"}
{"question": "Which works considered the physical world and social norms for affordance inference?", "answer": ["Learning to Act Properly: Predicting and Explaining Affordances from Images"], "answer_arxiv_id": ["1712.07576"], "source_meta": {"published_time": "20231026"}, "qid": "AutoScholarQuery_test_176"}
{"question": "Can you mention some studies that have explored quantization as a means of compressing the parameters of LLMs for efficient inference?", "answer": ["LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"], "answer_arxiv_id": ["2208.07339"], "source_meta": {"published_time": "20230522"}, "qid": "AutoScholarQuery_test_177"}
{"question": "Could you provide me with some works on memorization study that involve finding training dataset documents related to the output?", "answer": ["Do Language Models Plagiarize?", "Large Language Models Struggle to Learn Long-Tail Knowledge", "Data Contamination: From Memorization to Exploitation"], "answer_arxiv_id": ["2203.07618", "2211.08411", "2203.08242"], "source_meta": {"published_time": "20240306"}, "qid": "AutoScholarQuery_test_178"}
{"question": "Which references provided a dataset facilitating entity embedding through linked images?", "answer": ["MMKG: Multi-Modal Knowledge Graphs"], "answer_arxiv_id": ["1903.05485"], "source_meta": {"published_time": "20240723"}, "qid": "AutoScholarQuery_test_179"}
{"question": "What works have studied sampling for regret minimization in the framework of Unsupervised Environment Design?", "answer": ["Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design", "Replay-Guided Adversarial Environment Design"], "answer_arxiv_id": ["2012.02096", "2110.02439"], "source_meta": {"published_time": "20230126"}, "qid": "AutoScholarQuery_test_180"}
{"question": "Which works used middle-layer LLM outputs and inserted them into the VPG to identify differences between images?", "answer": ["Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative\n  Instructions"], "answer_arxiv_id": ["2308.04152"], "source_meta": {"published_time": "20240219"}, "qid": "AutoScholarQuery_test_181"}
{"question": "Which works achieve high identity preservation in personalized generation through massive datasets and expensive hardware resources?", "answer": ["Face0: Instantaneously Conditioning a Text-to-Image Model on a Face", "Subject-Diffusion:Open Domain Personalized Text-to-Image Generation\n  without Test-time Fine-tuning"], "answer_arxiv_id": ["2306.06638", "2307.11410"], "source_meta": {"published_time": "20231211"}, "qid": "AutoScholarQuery_test_182"}
{"question": "Are there any studies about efficient selection of source tasks in NLP?", "answer": ["NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework", "Task Compass: Scaling Multi-task Pre-training with Task Prefix"], "answer_arxiv_id": ["2111.04130", "2210.06277"], "source_meta": {"published_time": "20230605"}, "qid": "AutoScholarQuery_test_183"}
{"question": "What works discuss transformer-based methods for 3D instance segmentation?", "answer": ["Mask3D: Mask Transformer for 3D Semantic Instance Segmentation", "Superpoint Transformer for 3D Scene Instance Segmentation"], "answer_arxiv_id": ["2210.03105", "2211.15766"], "source_meta": {"published_time": "20240322"}, "qid": "AutoScholarQuery_test_184"}
{"question": "Which work explores model calibration for Uncertainty Estimation in the context of multiple-choice question answering?", "answer": ["How Can We Know When Language Models Know? On the Calibration of\n  Language Models for Question Answering"], "answer_arxiv_id": ["2012.00955"], "source_meta": {"published_time": "20240219"}, "qid": "AutoScholarQuery_test_185"}
{"question": "Could you provide me some studies that explored policy optimization via backpropagating through the dynamics model?", "answer": ["Model-Augmented Actor-Critic: Backpropagating through Paths"], "answer_arxiv_id": ["2005.08068"], "source_meta": {"published_time": "20221024"}, "qid": "AutoScholarQuery_test_186"}
{"question": "Which study proposed a complete graph kernel based on homomorphism counts in the context of graph learning tasks?", "answer": ["Lovász Meets Weisfeiler and Leman"], "answer_arxiv_id": ["1802.08876"], "source_meta": {"published_time": "20230609"}, "qid": "AutoScholarQuery_test_187"}
{"question": "Which study developed a Jacobi diffusion process for discrete data diffusion models?", "answer": ["Dirichlet Diffusion Score Model for Biological Sequence Generation"], "answer_arxiv_id": ["2305.10699"], "source_meta": {"published_time": "20230914"}, "qid": "AutoScholarQuery_test_188"}
{"question": "What research introduced DPMs and linked the generative model to a denoising diffusion model?", "answer": ["Auto-Encoding Variational Bayes", "Generative Adversarial Nets", "Towards Building A Group-based Unsupervised Representation Disentanglement Framework", "Learning Disentangled Representation by Exploiting Pretrained Generative Models: A Contrastive Learning View", "Deep Unsupervised Learning using Nonequilibrium Thermodynamics", "Denoising Diffusion Probabilistic Models", "Improved Denoising Diffusion Probabilistic Models", "Denoising Diffusion Implicit Models", "Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality", "Gotta Go Fast When Generating Data with Score-Based Models", "Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models", "Score-based Generative Modeling in Latent Space"], "answer_arxiv_id": ["1312.6114", "1406.2661", "2102.10303", "2102.10543", "1503.03585", "2006.11239", "2102.09672", "2010.02502", "2202.05830", "2105.14080", "2201.06503", "2106.05931"], "source_meta": {"published_time": "20220617"}, "qid": "AutoScholarQuery_test_189"}
{"question": "Which studies focused on structured neural networks embedding principles like equivariance, Euclidean symmetry and periodicity?", "answer": ["Group Equivariant Convolutional Networks", "Neural Descriptor Fields: \"SE\"⁢(3)-Equivariant Object Representations for Manipulation", "Periodic DMP formulation for Quaternion Trajectories"], "answer_arxiv_id": ["1602.07576", "2112.05124", "2110.10510"], "source_meta": {"published_time": "20230427"}, "qid": "AutoScholarQuery_test_190"}
{"question": "Which study proposed the use of memorization as a metric in CoreSet selection?", "answer": ["What Neural Networks Memorize and Why: Discovering the Long Tail via\n  Influence Estimation"], "answer_arxiv_id": ["2008.03703"], "source_meta": {"published_time": "20231206"}, "qid": "AutoScholarQuery_test_191"}
{"question": "Which papers discussed Parameter-efficient fine-tuning (PEFT) methods for reducing storage requirements in large-scale PLMs?", "answer": ["Parameter-Efficient Transfer Learning for NLP", "The Power of Scale for Parameter-Efficient Prompt Tuning", "Prefix-Tuning: Optimizing Continuous Prompts for Generation"], "answer_arxiv_id": ["1902.00751", "2104.08691", "2101.00190"], "source_meta": {"published_time": "20230601"}, "qid": "AutoScholarQuery_test_192"}
{"question": "What studies compare using a large web corpus versus Wikipedia?", "answer": ["Cloze-driven Pretraining of Self-attention Networks", "XLNet: Generalized Autoregressive Pretraining for Language Understanding"], "answer_arxiv_id": ["1903.07785", "1906.08237"], "source_meta": {"published_time": "20230206"}, "qid": "AutoScholarQuery_test_193"}
{"question": "Are there any studies that discuss about learning 'subspace juntas', functions of an unknown low-dimensional subspace?", "answer": ["Structure from Local Optima: Learning Subspace Juntas via Higher Order PCA"], "answer_arxiv_id": ["1108.3329v3"], "source_meta": {"published_time": "20231102"}, "qid": "AutoScholarQuery_test_194"}
{"question": "Which work attempted to explore the inner modeling of hierarchical Transformer-based backbone?", "answer": ["An Efficient Spatio-Temporal Pyramid Transformer for Action Detection"], "answer_arxiv_id": ["2207.10448"], "source_meta": {"published_time": "20231204"}, "qid": "AutoScholarQuery_test_195"}
{"question": "Which works on deep learning are primarily used as losses to train generative models?", "answer": ["Towards Principled Methods for Training Generative Adversarial Networks"], "answer_arxiv_id": ["1701.04862"], "source_meta": {"published_time": "20220530"}, "qid": "AutoScholarQuery_test_196"}
{"question": "Which references discussed the empirical observation of oversmoothing issue in attention-based GNNs such as GATs or transformers?", "answer": ["A Survey on Oversmoothing in Graph Neural Networks", "Revisiting Over-smoothing in BERT from the Perspective of Graph"], "answer_arxiv_id": ["2303.10993", "2202.08625"], "source_meta": {"published_time": "20230525"}, "qid": "AutoScholarQuery_test_197"}
{"question": "Which researchers proposed altering the memory-computation trade-off of the neural architecture for improving computational speed in neural scene representations?", "answer": ["DeRF: Decomposed Radiance Fields", "KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs", "FastNeRF: High-Fidelity Neural Rendering at 200FPS", "Plenoxels: Radiance Fields without Neural Networks", "Direct Voxel Grid Optimization: Super-fast Convergence for Radiance\n  Fields Reconstruction"], "answer_arxiv_id": ["2011.12490", "2103.13744", "2103.10380", "2112.05131", "2111.11215"], "source_meta": {"published_time": "20231129"}, "qid": "AutoScholarQuery_test_198"}
{"question": "What are the recent works that employed clustering for personalized Federated Learning?", "answer": ["Federated learning with hierarchical clustering of local updates to improve training on non-IID data", "Clustered Federated Learning: Model-Agnostic Distributed Multi-Task Optimization under Privacy Constraints", "TiFL: A Tier-based Federated Learning System"], "answer_arxiv_id": ["2004.11791v2", "1910.01991", "2001.09249"], "source_meta": {"published_time": "20230504"}, "qid": "AutoScholarQuery_test_199"}
{"question": "What research demonstrated that the Felzenswalb algorithm can generate useful geometric segment clusters in the ScanNet dataset?", "answer": ["ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes"], "answer_arxiv_id": ["1702.04405"], "source_meta": {"published_time": "20230325"}, "qid": "AutoScholarQuery_test_200"}
{"question": "Which studies showed the success of Diffusion Models in the field of image synthesis?", "answer": ["Diffusion Models Beat GANs on Image Synthesis", "Variational Diffusion Models", "Denoising Diffusion Probabilistic Models"], "answer_arxiv_id": ["2105.05233", "2107.00630", "2006.11239"], "source_meta": {"published_time": "20240331"}, "qid": "AutoScholarQuery_test_201"}
{"question": "Which researches focused on batchifying queries in few-shot settings for LLMs?", "answer": ["Batch Prompting: Efficient Inference with Large Language Model APIs"], "answer_arxiv_id": ["2301.08721"], "source_meta": {"published_time": "20230522"}, "qid": "AutoScholarQuery_test_202"}
{"question": "What are the papers that applied zero/few-shot learning using large language models?", "answer": ["Language Models are Few-Shot Learners", "PaLM: Scaling Language Modeling with Pathways"], "answer_arxiv_id": ["2005.14165", "2204.02311"], "source_meta": {"published_time": "20230219"}, "qid": "AutoScholarQuery_test_203"}
{"question": "Which works have achieved impressive results in the field of image generation using Diffusion Probabilistic Models?", "answer": ["Denoising Diffusion Probabilistic Models", "Diffusion Models in Vision: A Survey"], "answer_arxiv_id": ["2006.11239", "2209.04747"], "source_meta": {"published_time": "20231030"}, "qid": "AutoScholarQuery_test_204"}
{"question": "Which work resulted in the creation of the BLOOM model and ROOTS corpus as part of open science community initiatives?", "answer": ["BLOOM: A 176B-Parameter Open-Access Multilingual Language Model", "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset"], "answer_arxiv_id": ["2211.05100", "2303.03915"], "source_meta": {"published_time": "20240209"}, "qid": "AutoScholarQuery_test_205"}
{"question": "Could you detail any works about answering questions given counterfactual conditions?", "answer": ["IfQA: A Dataset for Open-domain Question Answering under Counterfactual\n  Presuppositions", "WIQA: A dataset for \"What if...\" reasoning over procedural text", "TIMEDIAL: Temporal Commonsense Reasoning in Dialog"], "answer_arxiv_id": ["2305.14010", "1909.04739", "2106.04571"], "source_meta": {"published_time": "20231129"}, "qid": "AutoScholarQuery_test_206"}
{"question": "Which works focused on leveraging the internal states of LLMs to study hallucinated content?", "answer": ["A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of\n  LLMs by Validating Low-Confidence Generation", "Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of\n  Language Models", "The Internal State of an LLM Knows When It's Lying"], "answer_arxiv_id": ["2307.03987", "2309.15098", "2304.13734"], "source_meta": {"published_time": "20240106"}, "qid": "AutoScholarQuery_test_207"}
{"question": "Can you provide references for grouping-based methods of 3D instance segmentation?", "answer": ["PointGroup: Dual-Set Point Grouping for 3D Instance Segmentation", "Hierarchical Aggregation for 3D Instance Segmentation", "Instance Segmentation in 3D Scenes using Semantic Superpoint Tree\n  Networks", "MaskGroup: Hierarchical Point Grouping and Masking for 3D Instance\n  Segmentation", "SoftGroup for 3D Instance Segmentation on Point Clouds", "3D Instances as 1D Kernels", "ISBNet: a 3D Point Cloud Instance Segmentation Network with\n  Instance-aware Sampling and Box-aware Dynamic Convolution"], "answer_arxiv_id": ["2004.01658", "2108.02350", "2108.07478", "2203.14662", "2203.01509", "2207.07372", "2303.00246"], "source_meta": {"published_time": "20240322"}, "qid": "AutoScholarQuery_test_208"}
{"question": "In which studies has the focus been on translating natural language proofs into formal representations?", "answer": ["MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics", "NaturalProofs: Mathematical Theorem Proving in Natural Language"], "answer_arxiv_id": ["2109.00110", "2104.01112"], "source_meta": {"published_time": "20240221"}, "qid": "AutoScholarQuery_test_209"}
{"question": "Which works extend the concept of estimating a transformation between two point clouds into finding an ideal set of primitives to represent a shape collection?", "answer": ["Learning elementary structures for 3D shape generation and matching"], "answer_arxiv_id": ["1908.04725"], "source_meta": {"published_time": "20231128"}, "qid": "AutoScholarQuery_test_210"}
{"question": "Which works achieved impressive results in scene understanding using multi-modal models?", "answer": ["EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action\n  Recognition"], "answer_arxiv_id": ["1908.08498"], "source_meta": {"published_time": "20240328"}, "qid": "AutoScholarQuery_test_211"}
{"question": "Who are the researchers that attempted to close the gap between QM calculations and ML potentials?", "answer": ["SpookyNet: Learning Force Fields with Electronic Degrees of Freedom and Nonlocal Effects", "OrbNet: Deep Learning for Quantum Chemistry Using Symmetry-Adapted Atomic-Orbital Features", "Finding Density Functionals with Machine Learning", "Ab-Initio Potential Energy Surfaces by Pairing GNNs with Neural Wave Functions", "Generalizing Neural Wave Functions", "Sampling-free Inference for Ab-Initio Potential Energy Surface Networks"], "answer_arxiv_id": ["2105.00304", "2007.08026", "1112.5441", "2110.05064", "2302.04168", "2205.14962"], "source_meta": {"published_time": "20230620"}, "qid": "AutoScholarQuery_test_212"}
{"question": "Any references that applied DDPM based on PVCNNs on the point-voxel representation of 3D shapes?", "answer": ["3D Shape Generation and Completion through Point-Voxel Diffusion"], "answer_arxiv_id": ["2104.03670"], "source_meta": {"published_time": "20230704"}, "qid": "AutoScholarQuery_test_213"}
{"question": "What papers discussed multi-modal models?", "answer": ["EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action\n  Recognition", "Multi-Modal Fusion Transformer for End-to-End Autonomous Driving", "SNE-RoadSeg: Incorporating Surface Normal Information into Semantic\n  Segmentation for Accurate Freespace Detection"], "answer_arxiv_id": ["1908.08498", "2104.09224", "2008.11351"], "source_meta": {"published_time": "20240328"}, "qid": "AutoScholarQuery_test_214"}
{"question": "Which paper provides a thorough discussion on quantisation-aware training?", "answer": ["A White Paper on Neural Network Quantization"], "answer_arxiv_id": ["2106.08295"], "source_meta": {"published_time": "20230320"}, "qid": "AutoScholarQuery_test_215"}
{"question": "What works propose the usage of vector-quantized variational autoencoder and point-based diffusion model in the context of LiDAR scene generation?", "answer": ["UltraLiDAR: Learning Compact Representations for LiDAR Completion and\n  Generation", "Learning to Generate Realistic LiDAR Point Clouds"], "answer_arxiv_id": ["2311.01448", "2209.03954"], "source_meta": {"published_time": "20240331"}, "qid": "AutoScholarQuery_test_216"}
{"question": "What papers proposed IL+RL methods that initialize policy search methods with policies trained via behavioral cloning?", "answer": ["Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations"], "answer_arxiv_id": ["1709.10087"], "source_meta": {"published_time": "20220405"}, "qid": "AutoScholarQuery_test_217"}
{"question": "Could you provide some studies about LiDAR perception techniques using active learning and domain adaptation methods?", "answer": ["Label-Efficient Point Cloud Semantic Segmentation: An Active Learning Approach", "A Survey on Deep Domain Adaptation for LiDAR Perception"], "answer_arxiv_id": ["2101.06931", "2106.02377"], "source_meta": {"published_time": "20231031"}, "qid": "AutoScholarQuery_test_218"}
{"question": "Which studies used deep learning in vision tasks like classification and object detection?", "answer": ["Reconstruction of 3D Porous Media From 2D Slices"], "answer_arxiv_id": ["1901.10233"], "source_meta": {"published_time": "20240328"}, "qid": "AutoScholarQuery_test_219"}
{"question": "Which works construct a generalized decoding model capable of predicting pixel-level segmentation and language tokens?", "answer": ["Generalized Decoding for Pixel, Image, and Language", "Segment Everything Everywhere All at Once"], "answer_arxiv_id": ["2212.11270", "2304.06718"], "source_meta": {"published_time": "20231214"}, "qid": "AutoScholarQuery_test_220"}
{"question": "What works showed that tighter dynamic regret rates are possible in non-stationary multi-armed bandits?", "answer": ["A New Look at Dynamic Regret for Non-Stationary Stochastic Bandits", "Tracking Most Significant Arm Switches in Bandits"], "answer_arxiv_id": ["2201.06532", "2112.13838"], "source_meta": {"published_time": "20230711"}, "qid": "AutoScholarQuery_test_221"}
{"question": "What work combines an original NeRF with an edited NeRF generated using text-based SDS?", "answer": ["Vox-E: Text-guided Voxel Editing of 3D Objects"], "answer_arxiv_id": ["2303.12048"], "source_meta": {"published_time": "20240103"}, "qid": "AutoScholarQuery_test_222"}
{"question": "What works establish the first connection between replicability and differential privacy in the context of PAC learning?", "answer": ["User-Level Private Learning via Correlated Sampling"], "answer_arxiv_id": ["2110.11208"], "source_meta": {"published_time": "20230523"}, "qid": "AutoScholarQuery_test_223"}
{"question": "Could you provide me some works that focus on specific applications such as dialogue, structured knowledge grounding, or chain-of-thought reasoning?", "answer": ["OpenAssistant Conversations -- Democratizing Large Language Model\n  Alignment", "UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding\n  with Text-to-Text Language Models", "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "The CoT Collection: Improving Zero-shot and Few-shot Learning of\n  Language Models via Chain-of-Thought Fine-Tuning"], "answer_arxiv_id": ["2304.07327", "2201.05966", "2201.11903", "2305.14045"], "source_meta": {"published_time": "20240209"}, "qid": "AutoScholarQuery_test_224"}
{"question": "What papers are about updating text generation metrics?", "answer": ["The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics", "How Robust are Model Rankings : A Leaderboard Customization Approach for Equitable Evaluation", "Bidimensional Leaderboards: Generate and Evaluate Language Hand in Hand"], "answer_arxiv_id": ["2102.01672", "2106.05532", "2112.04139"], "source_meta": {"published_time": "20220727"}, "qid": "AutoScholarQuery_test_225"}
{"question": "Which work provided upper and lower bounds on optimal regret based on a variant of the DEC?", "answer": ["Tight Guarantees for Interactive Decision Making with the Decision-Estimation Coefficient"], "answer_arxiv_id": ["2301.08215v1"], "source_meta": {"published_time": "20221125"}, "qid": "AutoScholarQuery_test_226"}
{"question": "Which papers propose graph-based approaches for capturing longer-term dependencies in 3D human pose forecasting?", "answer": ["Learning Trajectory Dependencies for Human Motion Prediction", "MSR-GCN: Multi-Scale Residual Graph Convolution Networks for Human\n  Motion Prediction", "Dynamic Multiscale Graph Neural Networks for 3D Skeleton-Based Human\n  Motion Prediction", "Space-Time-Separable Graph Convolutional Network for Pose Forecasting", "Spatio-Temporal Gating-Adjacency GCN for Human Motion Prediction", "Multitask Non-Autoregressive Model for Human Motion Prediction", "Diverse Human Motion Prediction Guided by Multi-Level Spatial-Temporal\n  Anchors"], "answer_arxiv_id": ["1908.05436", "2108.07152", "2003.08802", "2110.04573", "2203.01474", "2007.06426", "2302.04860"], "source_meta": {"published_time": "20221125"}, "qid": "AutoScholarQuery_test_227"}
{"question": "Could you provide me with the works using unified maximum likelihood estimation and object retrieval to support various tasks?", "answer": ["Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and\n  Vision-Language Tasks", "Universal Instance Perception as Object Discovery and Retrieval"], "answer_arxiv_id": ["2211.09808", "2303.06674"], "source_meta": {"published_time": "20231214"}, "qid": "AutoScholarQuery_test_228"}
{"question": "Which works classify samples as hard based on the presence of large gradient norm and large norm of error vectors?", "answer": ["Deep Learning on a Data Diet: Finding Important Examples Early in Training"], "answer_arxiv_id": ["2107.07075"], "source_meta": {"published_time": "20231021"}, "qid": "AutoScholarQuery_test_229"}
{"question": "What work uses meta-learning methods in the field of knowledge editing?", "answer": ["Editing Factual Knowledge in Language Models", "Fast Model Editing at Scale"], "answer_arxiv_id": ["2104.08164", "2110.11309"], "source_meta": {"published_time": "20230916"}, "qid": "AutoScholarQuery_test_230"}
{"question": "What are some studies that discuss non-linear front-door adjustment?", "answer": ["Kernel Methods for Causal Functions: Dose, Heterogeneous, and Incremental Response Curves"], "answer_arxiv_id": ["2010.04855"], "source_meta": {"published_time": "20221012"}, "qid": "AutoScholarQuery_test_231"}
{"question": "What works apply the mentioned technique to the first-order methods?", "answer": ["Gradient Sliding for Composite Optimization", "Accelerated gradient sliding for structured convex optimization"], "answer_arxiv_id": ["1406.0919v2", "1609.04905"], "source_meta": {"published_time": "20230415"}, "qid": "AutoScholarQuery_test_232"}
{"question": "What papers explored the combination of GANs and implicit neural representation for 3D-aware image synthesis?", "answer": ["GIRAFFE: Representing Scenes as Compositional Generative Neural Feature\n  Fields", "StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation", "Efficient Geometry-aware 3D Generative Adversarial Networks", "pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware\n  Image Synthesis"], "answer_arxiv_id": ["2011.12100", "2112.11427", "2112.07945", "2012.00926"], "source_meta": {"published_time": "20231203"}, "qid": "AutoScholarQuery_test_233"}
{"question": "Could you provide me some research that demonstrated zero-shot cross-lingual transfer?", "answer": ["A Neural Pairwise Ranking Model for Readability Assessment"], "answer_arxiv_id": ["2203.07450"], "source_meta": {"published_time": "20240603"}, "qid": "AutoScholarQuery_test_234"}
{"question": "What papers discuss the application of non-Euclidean diffusion equation leading to a scheme with adaptive spatial derivatives?", "answer": ["Beltrami Flow and Neural Diffusion on Graphs"], "answer_arxiv_id": ["2110.09443"], "source_meta": {"published_time": "20221002"}, "qid": "AutoScholarQuery_test_235"}
{"question": "Which paper used mutual information for invariant representation learning outside of RL?", "answer": ["Invariant Representations with Stochastically Quantized Neural Networks"], "answer_arxiv_id": ["2208.02656"], "source_meta": {"published_time": "20230523"}, "qid": "AutoScholarQuery_test_236"}
{"question": "Which paper introduces DataComp, a benchmark for designing better pre-training datasets for CLIP?", "answer": ["DataComp: In search of the next generation of multimodal datasets"], "answer_arxiv_id": ["2304.14108"], "source_meta": {"published_time": "20230719"}, "qid": "AutoScholarQuery_test_237"}
{"question": "What paper advances the representative Seq2Edit model, GECToR, by improving its multi-round correction?", "answer": ["Type-Driven Multi-Turn Corrections for Grammatical Error Correction"], "answer_arxiv_id": ["2203.09136"], "source_meta": {"published_time": "20240528"}, "qid": "AutoScholarQuery_test_238"}
{"question": "Which works discuss provably efficient algorithms in the context of function approximation in linear mixture MDPs?", "answer": ["Provably Efficient Exploration in Policy Optimization", "Nearly Minimax Optimal Reinforcement Learning for Linear Mixture Markov Decision Processes", "Provably Efficient Reinforcement Learning for Discounted MDPs with Feature Mapping"], "answer_arxiv_id": ["1912.05830", "2012.08507", "2006.13165"], "source_meta": {"published_time": "20231029"}, "qid": "AutoScholarQuery_test_239"}
{"question": "Could you provide me some critiques on benchmarks in NLP about annotation artifacts?", "answer": ["Annotation Artifacts in Natural Language Inference Data"], "answer_arxiv_id": ["1803.02324"], "source_meta": {"published_time": "20240613"}, "qid": "AutoScholarQuery_test_240"}
{"question": "What studies presented method for selecting single target-state pair with stochastic batch acquisition in a BOED setting?", "answer": ["Interventions, Where and How? Experimental Design for Causal Models at Scale"], "answer_arxiv_id": ["2203.02016"], "source_meta": {"published_time": "20230221"}, "qid": "AutoScholarQuery_test_241"}
{"question": "Which work introduced API specially designed for content moderation in vast amounts of data?", "answer": ["A Holistic Approach to Undesired Content Detection in the Real World"], "answer_arxiv_id": ["2208.03274"], "source_meta": {"published_time": "20240221"}, "qid": "AutoScholarQuery_test_242"}
{"question": "What is the research paper that discusses instruction tuning?", "answer": ["Training language models to follow instructions with human feedback"], "answer_arxiv_id": ["2203.02155"], "source_meta": {"published_time": "20230522"}, "qid": "AutoScholarQuery_test_243"}
{"question": "Which work proposed the method of calculating the inner integral of Sampled Policy Gradients (SPG) using a Q-network?", "answer": ["Mean Actor-Critic"], "answer_arxiv_id": ["1709.00503"], "source_meta": {"published_time": "20221024"}, "qid": "AutoScholarQuery_test_244"}
{"question": "Which work applied perturbations to word embeddings in the NLP domain?", "answer": ["Adversarial Training Methods for Semi-Supervised Text Classification"], "answer_arxiv_id": ["1605.07725"], "source_meta": {"published_time": "20240531"}, "qid": "AutoScholarQuery_test_245"}
{"question": "What studies have considered extending beta diffusion to encompass the exponential family?", "answer": ["Variational Inference: A Review for Statisticians"], "answer_arxiv_id": ["1601.00670"], "source_meta": {"published_time": "20230914"}, "qid": "AutoScholarQuery_test_246"}
{"question": "What papers explored metrics that either emphasize a single dimension or lack human relevance?", "answer": ["Asking and Answering Questions to Evaluate the Factual Consistency of\n  Summaries", "GRADE: Automatic Graph-Enhanced Coherence Metric for Evaluating\n  Open-Domain Dialogue Systems", "USR: An Unsupervised and Reference Free Evaluation Metric for Dialog\n  Generation", "Towards a Unified Multi-Dimensional Evaluator for Text Generation"], "answer_arxiv_id": ["2004.04228", "2010.03994", "2005.00456", "2210.07197"], "source_meta": {"published_time": "20240224"}, "qid": "AutoScholarQuery_test_247"}
{"question": "Are there any studies that provide convergence guarantees for PFL with the bounded gradients assumption on heterogeneous data?", "answer": ["On the Convergence of FedAvg on Non-IID Data"], "answer_arxiv_id": ["1907.02189"], "source_meta": {"published_time": "20231106"}, "qid": "AutoScholarQuery_test_248"}
{"question": "Can you give examples of research that altered the parameter of the transition function (like gravity strength or the weight of the ball) for RL agent evaluation?", "answer": ["Assessing Generalization in Deep Reinforcement Learning", "Robust Adversarial Reinforcement Learning"], "answer_arxiv_id": ["1810.12282", "1703.02702"], "source_meta": {"published_time": "20230426"}, "qid": "AutoScholarQuery_test_249"}
{"question": "What subsequent work improved the complexities of the method developed by bib.bib60?", "answer": ["Accelerating Stochastic Composition Optimization"], "answer_arxiv_id": ["1607.07329"], "source_meta": {"published_time": "20230613"}, "qid": "AutoScholarQuery_test_250"}
{"question": "What works proposed methods based on the primary form of the Optimal Transport problem?", "answer": ["On Scalable and Efficient Computation of Large Scale Optimal Transport", "Large-Scale Optimal Transport via Adversarial Training with Cycle-Consistency"], "answer_arxiv_id": ["1905.00158", "2003.06635"], "source_meta": {"published_time": "20230524"}, "qid": "AutoScholarQuery_test_251"}
{"question": "Could you provide me some works that focus on aligning the learning trajectories for emulating long-range training dynamics of real data when performing dataset condensation?", "answer": ["Dataset Distillation by Matching Training Trajectories"], "answer_arxiv_id": ["2203.11932"], "source_meta": {"published_time": "20231021"}, "qid": "AutoScholarQuery_test_252"}
{"question": "Which studies succeeded in addressing regression tasks through deep learning?", "answer": ["MPIIGaze: Real-World Dataset and Deep Appearance-Based Gaze Estimation", "It’s Written All Over Your Face: Full-Face Appearance-Based Gaze Estimation"], "answer_arxiv_id": ["1711.09017", "1611.08860"], "source_meta": {"published_time": "20221003"}, "qid": "AutoScholarQuery_test_253"}
{"question": "What works exist that provide corrective measures for issues concerning the RP gradient?", "answer": ["Gradients are Not All You Need"], "answer_arxiv_id": ["2111.05803"], "source_meta": {"published_time": "20231214"}, "qid": "AutoScholarQuery_test_254"}
{"question": "What works have been done to extract answers from question-specific subgraphs generated with text corpora?", "answer": ["Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text", "PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text"], "answer_arxiv_id": ["1809.00782", "1904.09537"], "source_meta": {"published_time": "20231024"}, "qid": "AutoScholarQuery_test_255"}
{"question": "Which works propose methods to handle continuous treatments in the back-door adjustment?", "answer": ["Learning Counterfactual Representations for Estimating Individual Dose-Response Curves", "Nonparametric methods for doubly robust estimation of continuous treatment effects", "Double Debiased Machine Learning Nonparametric Inference with Continuous Treatments", "Automatic Debiased Machine Learning of Causal and Structural Effects"], "answer_arxiv_id": ["1902.00981", "1507.00747", "2004.03036", "1809.05224"], "source_meta": {"published_time": "20221012"}, "qid": "AutoScholarQuery_test_256"}
{"question": "Can you identify any papers that analysed the use of target networks with linear function approximation, needed in theoretical properties of target networks?", "answer": ["Breaking the Deadly Triad with a Target Network"], "answer_arxiv_id": ["2101.08862"], "source_meta": {"published_time": "20230224"}, "qid": "AutoScholarQuery_test_257"}
{"question": "Which works adapt diffusion models to condition generation on protein pockets?", "answer": ["Equivariant 3D-Conditional Diffusion Models for Molecular Linker Design", "Structure-based Drug Design with Equivariant Diffusion Models", "DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking"], "answer_arxiv_id": ["2210.05274", "2210.13695", "2210.01776"], "source_meta": {"published_time": "20230613"}, "qid": "AutoScholarQuery_test_258"}
{"question": "What are some of the works that provide a detailed analysis of the stationary distribution of SGD iterations with a constant learning rate?", "answer": ["Stochastic Gradient Descent as Approximate Bayesian Inference"], "answer_arxiv_id": ["1704.04289"], "source_meta": {"published_time": "20220924"}, "qid": "AutoScholarQuery_test_259"}
{"question": "What works measure bias based on the difference in ground-truth object-group co-occurrences in the training set and test set co-occurrences predicted by a model?", "answer": ["Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints"], "answer_arxiv_id": ["1707.09457"], "source_meta": {"published_time": "20221021"}, "qid": "AutoScholarQuery_test_260"}
{"question": "What works are there on incorporating interactive discussions and human collaboration into LLM-based evaluation methodologies?", "answer": ["ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate", "Collaborative Evaluation: Exploring the Synergy of Large Language Models\n  and Humans for Open-ended Generation Evaluation"], "answer_arxiv_id": ["2308.07201", "2310.19740"], "source_meta": {"published_time": "20240224"}, "qid": "AutoScholarQuery_test_261"}
{"question": "What works made advancements in achieving certified robustness with state-of-the-art randomized ablation?", "answer": ["Robustness Certificates for Sparse Adversarial Attacks by Randomized\n  Ablation", "Almost Tight L0-norm Certified Robustness of Top-k Predictions against\n  Adversarial Perturbations"], "answer_arxiv_id": ["1911.09272", "2011.07633"], "source_meta": {"published_time": "20240328"}, "qid": "AutoScholarQuery_test_262"}
{"question": "Which works perform multi-view refinement with flow or dense features in Structure-from-Motion (SfM)?", "answer": ["Multi-View Optimization of Local Feature Geometry", "Pixel-Perfect Structure-from-Motion with Featuremetric Refinement"], "answer_arxiv_id": ["2003.08348", "2108.08291"], "source_meta": {"published_time": "20230627"}, "qid": "AutoScholarQuery_test_263"}
{"question": "Could you give me examples of research achieving progress in sentiment analysis using multi-modal models?", "answer": ["Self-attention fusion for audiovisual emotion recognition with incomplete data", "MOSI: Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis in Online Opinion Videos", "Gated Mechanism for Attention Based Multimodal Sentiment Analysis"], "answer_arxiv_id": ["2201.11095v1", "1606.06259v2", "2003.01043"], "source_meta": {"published_time": "20240328"}, "qid": "AutoScholarQuery_test_264"}
{"question": "Which works propose techniques for improving the quality of image-text datasets in multimodal networks?", "answer": ["Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training", "Less is More: Removing Text-regions Improves CLIP Training Efficiency and Robustness", "T-MARS: Improving Visual Representations by Circumventing Text Feature Learning", "SemDeDup: Data-efficient learning at web-scale through semantic deduplication"], "answer_arxiv_id": ["2301.02280", "2305.05095", "2307.03132v2", "2303.09540"], "source_meta": {"published_time": "20230719"}, "qid": "AutoScholarQuery_test_265"}
{"question": "In what works can I find large-scale unsupervised pre-training on unstructured text for multilingual corpora?", "answer": ["BLOOM: A 176B-Parameter Open-Access Multilingual Language Model", "What Language Model to Train if You Have One Million GPU Hours?", "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset", "MADLAD-400: A Multilingual And Document-Level Large Audited Dataset", "LLM-powered Data Augmentation for Enhanced Cross-lingual Performance"], "answer_arxiv_id": ["2211.05100", "2210.15424", "2303.03915", "2309.04662", "2305.14288"], "source_meta": {"published_time": "20240209"}, "qid": "AutoScholarQuery_test_266"}
{"question": "Which papers focus on broader applications of NeRF, including generative modeling, video synthesis, and scene editing?", "answer": ["GET3D: A Generative Model of High Quality 3D Textured Shapes Learned\n  from Images", "VoLux-GAN: A Generative Model for 3D Face Synthesis with HDRI Relighting", "Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes", "Tensor4D : Efficient Neural 4D Decomposition for High-fidelity Dynamic\n  Reconstruction and Rendering", "Neural Radiance Flow for 4D View Synthesis and Video Processing", "Editing Conditional Radiance Fields", "NeRF-Editing: Geometry Editing of Neural Radiance Fields"], "answer_arxiv_id": ["2209.11163", "2201.04873", "2011.13084", "2211.11610", "2012.09790", "2105.06466", "2205.04978"], "source_meta": {"published_time": "20240326"}, "qid": "AutoScholarQuery_test_267"}
{"question": "What studies considered to initialise the vertices of the graphs with random labels in graph embedding?", "answer": ["The Surprising Power of Graph Neural Networks with Random Node Initialization", "Random Features Strengthen Graph Neural Networks"], "answer_arxiv_id": ["2010.01179v2", "2002.03155"], "source_meta": {"published_time": "20230609"}, "qid": "AutoScholarQuery_test_268"}
{"question": "What studies removed the strong assumptions about knowledge of non-stationarity in non-contextual bandits?", "answer": ["A New Algorithm for Non-stationary Contextual Bandits: Efficient, Optimal, and Parameter-free"], "answer_arxiv_id": ["1902.00980v3"], "source_meta": {"published_time": "20230711"}, "qid": "AutoScholarQuery_test_269"}
{"question": "What is the standout example of point-based methodologies that transformed the direct processing of Point Cloud?", "answer": ["PointNet: Deep Learning on Point Sets for 3D Classification and\n  Segmentation"], "answer_arxiv_id": ["1612.00593"], "source_meta": {"published_time": "20240328"}, "qid": "AutoScholarQuery_test_270"}
{"question": "Could you provide me some works about studying a special case of fractionally subadditive valuations?", "answer": ["The Fair Division of Hereditary Set Systems"], "answer_arxiv_id": ["1812.09561"], "source_meta": {"published_time": "20230828"}, "qid": "AutoScholarQuery_test_271"}
{"question": "Which works explored ad-hoc teamwork and zero-shot coordination in AI?", "answer": ["“Other-Play” for Zero-Shot Coordination", "A New Formalism, Method and Open Issues for Zero-Shot Coordination", "Equivariant Networks for Zero-Shot Coordination"], "answer_arxiv_id": ["2003.02979", "2106.06613", "2210.12124"], "source_meta": {"published_time": "20230615"}, "qid": "AutoScholarQuery_test_272"}
{"question": "What works explored the constraint of Neural Differential Equations by expensive training and prediction times?", "answer": ["Augmented Neural ODEs", "Learning differential equations that are easy to solve", "STEER: Simple Temporal Regularization For Neural ODEs", "Opening the Blackbox: Accelerating Neural Differential Equations by Regularizing Internal Solver Heuristics"], "answer_arxiv_id": ["1904.01681", "2007.04504", "2006.10711", "2105.03918"], "source_meta": {"published_time": "20230303"}, "qid": "AutoScholarQuery_test_273"}
{"question": "What work is the only public simulator that supports differentiable simulation for in-graph acceleration?", "answer": ["Imagining The Road Ahead: Multi-Agent Trajectory Prediction via Differentiable Simulation"], "answer_arxiv_id": ["2104.11212"], "source_meta": {"published_time": "20231012"}, "qid": "AutoScholarQuery_test_274"}
{"question": "What studies work on body motion conditioned on text descriptions?", "answer": ["FLAME: Free-form Language-based Motion Synthesis & Editing", "Action-Conditioned 3D Human Motion Synthesis with Transformer VAE", "Synthesizing Long-Term Human Motions with Diffusion Models via Coherent\n  Sampling", "TEMOS: Generating diverse human motions from textual descriptions", "Synthesis of Compositional Animations from Textual Descriptions"], "answer_arxiv_id": ["2209.00349", "2104.05670", "2308.01850", "2204.14109", "2103.14675"], "source_meta": {"published_time": "20240301"}, "qid": "AutoScholarQuery_test_275"}
{"question": "Which study examines users' discomfort or concern due to the lack of responsibility in LLMs' recommendations for emotional support response?", "answer": ["The Typing Cure: Experiences with Large Language Model Chatbots for\n  Mental Health Support"], "answer_arxiv_id": ["2401.14362"], "source_meta": {"published_time": "20240220"}, "qid": "AutoScholarQuery_test_276"}
{"question": "Which reference is about the TorchGeo, a Python library for the integration of remote sensing datasets into the PyTorch deep learning ecosystem?", "answer": ["TorchGeo: Deep Learning With Geospatial Data"], "answer_arxiv_id": ["2111.08872"], "source_meta": {"published_time": "20230606"}, "qid": "AutoScholarQuery_test_277"}
{"question": "Which studies have investigated factors like loss function, surrogate gradient estimation, and batch normalization that affect the learning behavior in direct training of SNNs?", "answer": ["Temporal Efficient Training of Spiking Neural Network via Gradient Re-weighting", "Membrane Potential Batch Normalization for Spiking Neural Networks"], "answer_arxiv_id": ["2202.11946", "2308.08359"], "source_meta": {"published_time": "20230402"}, "qid": "AutoScholarQuery_test_278"}
{"question": "Could you provide me works that exemplified an asymmetric discussion mechanism with different LLMs and using a weighted voting mechanism?", "answer": ["ReConcile: Round-Table Conference Improves Reasoning via Consensus among\n  Diverse LLMs"], "answer_arxiv_id": ["2309.13007"], "source_meta": {"published_time": "20240228"}, "qid": "AutoScholarQuery_test_279"}
{"question": "What references proposed methods to discover the global semantic structure underlying the whole dataset, shed light on graph contrastive learning?", "answer": ["Self-supervised Graph-level Representation Learning with Local and Global Structure", "Omni-Granular Ego-Semantic Propagation for Self-Supervised Graph Representation Learning"], "answer_arxiv_id": ["2106.04113", "2205.15746"], "source_meta": {"published_time": "20230508"}, "qid": "AutoScholarQuery_test_280"}
{"question": "Could you mention research studies that utilized features for minimizing the surrogate models in dataset distillation?", "answer": ["CAFE: Learning to Condense Dataset by Aligning Features"], "answer_arxiv_id": ["2203.01531"], "source_meta": {"published_time": "20231206"}, "qid": "AutoScholarQuery_test_281"}
{"question": "Could you name the papers where the research focus on the L2 convergence rate for KRR, and it can be easily extended to [ℋ]γ,γ≥0 convergence rate?", "answer": ["Sobolev Norm Learning Rates for Regularized Least-Squares Algorithms", "Optimal Rates for Spectral Algorithms with Least-Squares Regression over Hilbert Spaces"], "answer_arxiv_id": ["1702.07254", "1801.06720"], "source_meta": {"published_time": "20230512"}, "qid": "AutoScholarQuery_test_282"}
{"question": "What work used MCP when evaluating InstructGPT?", "answer": ["Can Large Language Models Reason about Medical Questions?"], "answer_arxiv_id": ["2207.08143"], "source_meta": {"published_time": "20221022"}, "qid": "AutoScholarQuery_test_283"}
{"question": "What studies in medical VLMs use diffusion-based methods in report-to-CXR generation task?", "answer": ["RoentGen: Vision-Language Foundation Model for Chest X-ray Generation"], "answer_arxiv_id": ["2211.12737"], "source_meta": {"published_time": "20231213"}, "qid": "AutoScholarQuery_test_284"}
{"question": "Which works initially proposed generating test cases and corresponding accurate assert statements with Transformer models?", "answer": ["Unit Test Case Generation with Transformers and Focal Context", "Generating Accurate Assert Statements for Unit Test Cases using Pretrained Transformers"], "answer_arxiv_id": ["2009.05617", "2009.05634"], "source_meta": {"published_time": "20230309"}, "qid": "AutoScholarQuery_test_285"}
{"question": "Which paper carried out an extensive empirical study which shows significant improvement in model performance by implementing SWAG with multiple randomly initialized models?", "answer": ["Bayesian Deep Learning and a Probabilistic Perspective of Generalization"], "answer_arxiv_id": ["2002.08791"], "source_meta": {"published_time": "20220924"}, "qid": "AutoScholarQuery_test_286"}
{"question": "What works have developed algorithms to solve the sparse coding problem?", "answer": ["Online Learning for Matrix Factorization and Sparse Coding", "New Algorithms for Learning Incoherent and Overcomplete Dictionaries", "More algorithms for provable dictionary learning", "Simple, Efficient, and Neural Algorithms for Sparse Coding"], "answer_arxiv_id": ["0908.0050", "1308.6273", "1401.0579", "1503.00778"], "source_meta": {"published_time": "20230224"}, "qid": "AutoScholarQuery_test_287"}
{"question": "What studies used concept activation vectors and multimodal models to annotate concepts for CBMs?", "answer": ["Post-hoc Concept Bottleneck Models", "Label-free Concept Bottleneck Models", "Learning Transferable Visual Models From Natural Language Supervision"], "answer_arxiv_id": ["2205.15480", "2304.06129", "2103.00020"], "source_meta": {"published_time": "20231118"}, "qid": "AutoScholarQuery_test_288"}
{"question": "Any works that discuss learning the GAN fingerprints towards image attribution?", "answer": ["Attributing Fake Images to GANs: Learning and Analyzing GAN Fingerprints"], "answer_arxiv_id": ["1811.08180"], "source_meta": {"published_time": "20230613"}, "qid": "AutoScholarQuery_test_289"}
{"question": "What works used pretrained GAN generators and text encoders to optimize images based on textual prompts?", "answer": ["Paint by Word", "StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators"], "answer_arxiv_id": ["2103.10951", "2108.00946"], "source_meta": {"published_time": "20230518"}, "qid": "AutoScholarQuery_test_290"}
{"question": "What research proposed GroundingSAM, a combination of GroundingDINO and SAM for generating segmentation masks?", "answer": ["Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set\n  Object Detection", "Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks"], "answer_arxiv_id": ["2303.05499", "2401.14159"], "source_meta": {"published_time": "20231201"}, "qid": "AutoScholarQuery_test_291"}
{"question": "What research is done about 3D diffusion models involving meshes?", "answer": ["3DGen: Triplane Latent Diffusion for Textured Mesh Generation", "MeshDiffusion: Score-based Generative 3D Mesh Modeling", "Controllable Mesh Generation Through Sparse Latent Point Diffusion\n  Models"], "answer_arxiv_id": ["2303.05371", "2303.08133", "2303.07938"], "source_meta": {"published_time": "20240331"}, "qid": "AutoScholarQuery_test_292"}
{"question": "Which papers deal with the utilization of context to rewrite the conversation into a standalone query in CQR models?", "answer": ["Conversational Question Reformulation via Sequence-to-Sequence\n  Architectures and Pretrained Language Models", "ConvGQR: Generative Query Reformulation for Conversational Search"], "answer_arxiv_id": ["2004.01909", "2305.15645"], "source_meta": {"published_time": "20240211"}, "qid": "AutoScholarQuery_test_293"}
{"question": "In what work was PoseNet’s innovative transfer learning first introduced?", "answer": ["PoseNet: A Convolutional Network for Real-Time 6-DOF Camera\n  Relocalization"], "answer_arxiv_id": ["1505.07427"], "source_meta": {"published_time": "20240328"}, "qid": "AutoScholarQuery_test_294"}
{"question": "Which studies describe model structures that implicitly generate reasoning processes?", "answer": ["Program Induction by Rationale Generation : Learning to Solve and\n  Explain Algebraic Word Problems", "TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and\n  Textual Content in Finance", "Answering Numerical Reasoning Questions in Table-Text Hybrid Contents\n  with Graph-based Encoder and Tree-based Decoder", "Chaining Simultaneous Thoughts for Numerical Reasoning", "ELASTIC: Numerical Reasoning with Adaptive Symbolic Compiler"], "answer_arxiv_id": ["1705.04146", "2105.07624", "2209.07692", "2211.16482", "2210.10105"], "source_meta": {"published_time": "20240216"}, "qid": "AutoScholarQuery_test_295"}
{"question": "What studies investigated phenomenon on invariant representations in the context of algorithmic fairness?", "answer": ["Censoring Representations with an Adversary", "Mitigating Unwanted Biases with Adversarial Learning", "Conditional Learning of Fair Representations"], "answer_arxiv_id": ["1511.05897", "1801.07593", "1910.07162"], "source_meta": {"published_time": "20201219"}, "qid": "AutoScholarQuery_test_296"}
{"question": "Any studies showcase the potential of non-attention architectures in language modeling?", "answer": ["Hungry Hungry Hippos: Towards Language Modeling with State Space Models", "Hyena Hierarchy: Towards Larger Convolutional Language Models", "RWKV: Reinventing RNNs for the Transformer Era", "Hierarchically Gated Recurrent Neural Network for Sequence Modeling"], "answer_arxiv_id": ["2212.14052", "2302.10866v3", "2305.13048", "2311.04823"], "source_meta": {"published_time": "20231130"}, "qid": "AutoScholarQuery_test_297"}
{"question": "What papers proposed iterative methods for transferable adversarial attacks?", "answer": ["Explaining and Harnessing Adversarial Examples", "Boosting Adversarial Attacks with Momentum", "Nesterov Accelerated Gradient and Scale Invariance for Adversarial Attacks", "Enhancing the Transferability of Adversarial Attacks through Variance Tuning", "Improving Transferability of Adversarial Examples with Input Diversity", "On Improving Adversarial Transferability of Vision Transformers", "Cross-Modal Transferable Adversarial Attacks from Images to Videos"], "answer_arxiv_id": ["1412.6572", "1710.06081", "1908.06281", "2103.15571", "1803.06978", "2106.04169", "2112.05379"], "source_meta": {"published_time": "20230223"}, "qid": "AutoScholarQuery_test_298"}
{"question": "Which papers proposed datasets for open domain question answering (QA) for English and other languages?", "answer": ["TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages", "XOR QA: Cross-lingual Open-Retrieval Question Answering", "MIA 2022 Shared Task: Evaluating Cross-lingual Open-Retrieval Question Answering for 16 Diverse Languages", "MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering", "Mr. TYDI: A Multi-lingual Benchmark for Dense Retrieval"], "answer_arxiv_id": ["2003.05002", "2010.11856v3", "2207.00758", "2007.15207", "2108.08787"], "source_meta": {"published_time": "20220727"}, "qid": "AutoScholarQuery_test_299"}
{"question": "What research work focuses on a criterion, named difference of confidences (DoC), which estimates and reflects model accuracy?", "answer": ["Predicting with Confidence on Unseen Distributions"], "answer_arxiv_id": ["2107.03315"], "source_meta": {"published_time": "20231023"}, "qid": "AutoScholarQuery_test_300"}
{"question": "What studies introduced the mask-reconstruction paradigm for unimodal pretraining?", "answer": ["BERT: Pre-training of Deep Bidirectional Transformers for Language\n  Understanding"], "answer_arxiv_id": ["1810.04805"], "source_meta": {"published_time": "20240125"}, "qid": "AutoScholarQuery_test_301"}
{"question": "Are there any studies that suggest the algorithm they propose cannot be directly applied to model-free reinforcement learning settings?", "answer": ["Unified Algorithms for RL with Decision-Estimation Coefficients: No-Regret, PAC, and Reward-Free Learning"], "answer_arxiv_id": ["2209.11745"], "source_meta": {"published_time": "20221125"}, "qid": "AutoScholarQuery_test_302"}
{"question": "Are there any studies that used meta-learning for updating the knowledge in LLMs through varying their parameters?", "answer": ["Editable Neural Networks", "Fast Model Editing at Scale", "Editing Factual Knowledge in Language Models"], "answer_arxiv_id": ["2004.00345", "2110.11309", "2104.08164"], "source_meta": {"published_time": "20231114"}, "qid": "AutoScholarQuery_test_303"}
{"question": "Which works were proposed that consider networks of weights matrices with bounded norms?", "answer": ["Spectrally-normalized margin bounds for neural networks", "Robust Large Margin Deep Neural Networks"], "answer_arxiv_id": ["1706.08498", "1605.08254"], "source_meta": {"published_time": "20230419"}, "qid": "AutoScholarQuery_test_304"}
{"question": "Which study proposed the method PixSfM for feature-metric keypoint adjustment and bundle adjustment in SfM?", "answer": ["Pixel-Perfect Structure-from-Motion with Featuremetric Refinement"], "answer_arxiv_id": ["2108.08291"], "source_meta": {"published_time": "20230627"}, "qid": "AutoScholarQuery_test_305"}
{"question": "What papers studied unbalanced optimal transport (UOT) using methods that estimate UOT potentials on discrete space?", "answer": ["Neural Unbalanced Optimal Transport via Cycle-Consistent Semi-Couplings", "On Unbalanced Optimal Transport: An Analysis of Sinkhorn Algorithm", "Unbalanced Optimal Transport through Non-negative Penalized Linear Regression", "Unbalanced minibatch Optimal Transport; applications to Domain Adaptation"], "answer_arxiv_id": ["2209.15621", "2002.03293", "2106.04145", "2103.03606"], "source_meta": {"published_time": "20230524"}, "qid": "AutoScholarQuery_test_306"}
{"question": "What works used open-loop imitation learning for predicting the behavior of the ego vehicle in autonomous driving?", "answer": ["End to End Learning for Self-Driving Cars", "PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings", "End-to-end Driving via Conditional Imitation Learning", "SafetyNet: Safe planning for real-world self-driving vehicles using machine-learned policies", "Learning by cheating"], "answer_arxiv_id": ["1604.07316", "1905.01296", "1710.02410", "2109.13602", "1912.12294"], "source_meta": {"published_time": "20231012"}, "qid": "AutoScholarQuery_test_307"}
{"question": "Which works formulates medical VQA datasets based on MIMIC-CXR?", "answer": ["Interpretable Medical Image Visual Question Answering via Multi-Modal Relationship Graph Learning"], "answer_arxiv_id": ["2302.09636"], "source_meta": {"published_time": "20231028"}, "qid": "AutoScholarQuery_test_308"}
{"question": "Can you name some studies that propose different metrics to prune networks at initialization?", "answer": ["Picking Winning Tickets Before Training by Preserving Gradient Flow", "Pruning neural networks without any data by iteratively conserving synaptic flow", "Progressive Skeletonization: Trimming more fat from a network at initialization", "PHEW : Constructing Sparse Networks that Learn Fast and Generalize Well Without Training Data", "Prospect Pruning: Finding Trainable Weights at Initialization using Meta-Gradients"], "answer_arxiv_id": ["2002.07376", "2006.05467", "2006.09081", "2010.11354", "2202.08132"], "source_meta": {"published_time": "20230228"}, "qid": "AutoScholarQuery_test_309"}
{"question": "What papers reviewed when focusing on methods for quantifying aleatoric segmentation uncertainty?", "answer": ["A Probabilistic U-Net for Segmentation of Ambiguous Images", "Stochastic Segmentation Networks: Modelling Spatially Correlated Aleatoric Uncertainty"], "answer_arxiv_id": ["1806.05034", "2006.06015"], "source_meta": {"published_time": "20230328"}, "qid": "AutoScholarQuery_test_310"}
{"question": "What papers outlined the studies on how to select the firing threshold to cover all the features in an ANN?", "answer": ["Theory and Tools for the Conversion of Analog to Spiking Convolutional Neural Networks"], "answer_arxiv_id": ["1612.04052"], "source_meta": {"published_time": "20230402"}, "qid": "AutoScholarQuery_test_311"}
{"question": "What papers express key concerns about the validity of performance measurements obtained with various NLP benchmarks?", "answer": ["What Will it Take to Fix Benchmarking in Natural Language Understanding?"], "answer_arxiv_id": ["2104.02145"], "source_meta": {"published_time": "20240613"}, "qid": "AutoScholarQuery_test_312"}
{"question": "Please provide papers advocating the use of scratchpads in Large Language Models.", "answer": ["Show Your Work: Scratchpads for Intermediate Computation with Language\n  Models"], "answer_arxiv_id": ["2112.00114"], "source_meta": {"published_time": "20230922"}, "qid": "AutoScholarQuery_test_313"}
{"question": "Which studies look at the convergence rates for PMD-type methods for Lipschitz and smooth policies?", "answer": ["Policy Optimization with Stochastic Mirror Descent", "Bregman Gradient Policy Optimization"], "answer_arxiv_id": ["1906.10462v5", "2106.12112"], "source_meta": {"published_time": "20230130"}, "qid": "AutoScholarQuery_test_314"}
{"question": "What papers discuss mitigation methods for temporal adaptation?", "answer": ["Time-Aware Language Models as Temporal Knowledge Bases", "TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models", "Towards Continual Knowledge Learning of Language Models", "Plug-and-Play Adaptation for Continuously-updated QA"], "answer_arxiv_id": ["2106.15110", "2204.14211", "2110.03215", "2204.12785"], "source_meta": {"published_time": "20220727"}, "qid": "AutoScholarQuery_test_315"}
{"question": "What are the papers that introduce various learning objectives to tackle data heterogeneity in Federated Learning?", "answer": ["Federated Optimization in Heterogeneous Networks", "Federated Visual Classification with Real-World Data Distribution", "SCAFFOLD: Stochastic Controlled Averaging for Federated Learning", "Model-Contrastive Federated Learning"], "answer_arxiv_id": ["1812.06127", "2003.08082", "1910.06378", "2103.16257"], "source_meta": {"published_time": "20231008"}, "qid": "AutoScholarQuery_test_316"}
{"question": "Could you provide me some works that use complex-valued activations with real-valued weights and a gating mechanism?", "answer": ["Neuronal Synchrony in Complex-Valued Deep Networks", "Complex-Valued Autoencoders for Object Discovery"], "answer_arxiv_id": ["1312.6115", "2204.02075"], "source_meta": {"published_time": "20230524"}, "qid": "AutoScholarQuery_test_317"}
{"question": "Which studies highlight the benefit of capturing long-distance relations in Graph Neural Networks (GNNs) by stacking more feature aggregation layers or unrolling various fixed point iterations?", "answer": ["Predict then Propagate: Graph Neural Networks meet Personalized PageRank", "Implicit Graph Neural Networks", "Towards Deeper Graph Neural Networks", "Simple and Deep Graph Convolutional Networks", "Training Graph Neural Networks with 1000 Layers", "A Unified View on Graph Neural Networks as Graph Signal Denoising", "Interpreting and Unifying Graph Neural Networks with An Optimization Framework"], "answer_arxiv_id": ["1810.05997", "2009.06211", "2007.09296", "2007.02133", "2106.07476", "2010.01777", "2101.11859"], "source_meta": {"published_time": "20230203"}, "qid": "AutoScholarQuery_test_318"}
{"question": "Could you mention some studies about learning-based MVS approaches?", "answer": ["Deep Stereo using Adaptive Thin Volume Representation with Uncertainty\n  Awareness", "Cascade Cost Volume for High-Resolution Multi-View Stereo and Stereo\n  Matching", "Cost Volume Pyramid Based Depth Inference for Multi-View Stereo"], "answer_arxiv_id": ["1911.12012", "1912.06378", "1912.08329"], "source_meta": {"published_time": "20240421"}, "qid": "AutoScholarQuery_test_319"}
{"question": "Which works opted to incorporate the backbone into the network's training process to form an end-to-end TAD framework?", "answer": ["Learning Salient Boundary Feature for Anchor-free Temporal Action\n  Localization", "TALLFormer: Temporal Action Localization with a Long-memory Transformer", "An Efficient Spatio-Temporal Pyramid Transformer for Action Detection", "Re^2TAL: Rewiring Pretrained Video Backbones for Reversible Temporal\n  Action Localization"], "answer_arxiv_id": ["2103.13137", "2204.01680", "2207.10448", "2211.14053"], "source_meta": {"published_time": "20231204"}, "qid": "AutoScholarQuery_test_320"}
{"question": "Which works are referred when discussing previous benchmarks for solving math word problems?", "answer": ["Program Induction by Rationale Generation : Learning to Solve and\n  Explain Algebraic Word Problems", "MathQA: Towards Interpretable Math Word Problem Solving with\n  Operation-Based Formalisms", "Training Verifiers to Solve Math Word Problems", "CMATH: Can Your Language Model Pass Chinese Elementary School Math Test?"], "answer_arxiv_id": ["1705.04146", "1905.13319", "2110.14168", "2306.16636"], "source_meta": {"published_time": "20240221"}, "qid": "AutoScholarQuery_test_321"}
{"question": "Are there works that use image groupings and pairs for disentanglement?", "answer": ["Weakly Supervised Disentanglement with Guarantees", "Weakly-Supervised Disentanglement Without Compromises"], "answer_arxiv_id": ["1910.09772", "2002.02886"], "source_meta": {"published_time": "20230523"}, "qid": "AutoScholarQuery_test_322"}
{"question": "Could you provide me some examples of research that discusses the application of data augmentations in the latent space?", "answer": ["FreeLB: Enhanced Adversarial Training for Natural Language Understanding", "AdvAug: Robust Adversarial Augmentation for Neural Machine Translation", "DoubleMix: Simple Interpolation-Based Data Augmentation for Text\n  Classification", "Text Smoothing: Enhance Various Data Augmentation Methods on Text\n  Classification Tasks", "Controlled Text Generation for Data Augmentation in Intelligent\n  Artificial Agents"], "answer_arxiv_id": ["1909.11764", "2006.11834", "2209.05297", "2202.13840", "1910.03487"], "source_meta": {"published_time": "20240627"}, "qid": "AutoScholarQuery_test_323"}
{"question": "Which works make a formal equivalence between differential privacy and replicability for finite domains?", "answer": ["Stability is Stable: Connections between Replicability, Privacy, and Adaptive Generalization"], "answer_arxiv_id": ["2303.12921"], "source_meta": {"published_time": "20230523"}, "qid": "AutoScholarQuery_test_324"}
{"question": "Could you find any works that proposed methods applicable to online non-parametric regression tasks?", "answer": ["A Chaining Algorithm for Online Nonparametric Regression", "Online Isotonic Regression", "Online Forecasting of Total-Variation-bounded Sequences", "Total Variation Classes Beyond 1d: Minimax Rates, and the Limitations of Linear Smoothers"], "answer_arxiv_id": ["1502.07697v2", "1603.04190", "1906.03364", "1605.08400"], "source_meta": {"published_time": "20230531"}, "qid": "AutoScholarQuery_test_325"}
{"question": "Which studies introduced text-to-SQL datasets associated with MIMIC-III and eICU?", "answer": ["EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records"], "answer_arxiv_id": ["2301.07695"], "source_meta": {"published_time": "20231028"}, "qid": "AutoScholarQuery_test_326"}
{"question": "Are there any works that used hypernetworks in Meta-SL?", "answer": ["Meta-Learning with Latent Embedding Optimization", "Meta Networks"], "answer_arxiv_id": ["1807.05960", "1703.00837"], "source_meta": {"published_time": "20230926"}, "qid": "AutoScholarQuery_test_327"}
{"question": "Which researches have covered the attributes of non-functional requirements in terms of time/space performance?", "answer": ["Learning Performance-Improving Code Edits", "WizardCoder: Empowering Code Large Language Models with Evol-Instruct"], "answer_arxiv_id": ["2302.07867", "2306.08568"], "source_meta": {"published_time": "20240802"}, "qid": "AutoScholarQuery_test_328"}
{"question": "Could you provide examples of works about certified defenses focused on unimodal models?", "answer": ["Certified Adversarial Robustness via Randomized Smoothing", "Certified Robustness for Top-k Predictions against Adversarial\n  Perturbations via Randomized Smoothing", "Robustness Certificates for Sparse Adversarial Attacks by Randomized\n  Ablation", "Certified Defenses for Adversarial Patches", "SAFER: A Structure-free Approach for Certified Robustness to Adversarial\n  Word Substitutions", "Certified Robustness to Adversarial Examples with Differential Privacy", "PointGuard: Provably Robust 3D Point Cloud Classification", "Certified Robustness to Text Adversarial Attacks by Randomized [MASK]", "PatchCleanser: Certifiably Robust Defense against Adversarial Patches\n  for Any Image Classifier", "MultiGuard: Provably Robust Multi-label Classification against\n  Adversarial Examples", "TextGuard: Provable Defense against Backdoor Attacks on Text\n  Classification", "PointCert: Point Cloud Classification with Deterministic Certified\n  Robustness Guarantees"], "answer_arxiv_id": ["1902.02918", "1912.09899", "1911.09272", "2003.06693", "2005.14424", "1802.03471", "2103.03046", "2105.03743", "2108.09135", "2210.01111", "2311.11225", "2303.01959"], "source_meta": {"published_time": "20240328"}, "qid": "AutoScholarQuery_test_329"}
{"question": "What are the early efforts that expanded traditional KG representation learning methods for single-modal knowledge graphs?", "answer": ["Image-embodied Knowledge Representation Learning"], "answer_arxiv_id": ["1609.07028"], "source_meta": {"published_time": "20240723"}, "qid": "AutoScholarQuery_test_330"}
{"question": "Could you provide me the work that incorporated PEFT-based layer adaptation to the shared attention and FFN modules in transformers?", "answer": ["EdgeFormer: A Parameter-Efficient Transformer for On-Device Seq2seq\n  Generation"], "answer_arxiv_id": ["2202.07959"], "source_meta": {"published_time": "20240224"}, "qid": "AutoScholarQuery_test_331"}
{"question": "What papers proposed domain adaptation (DA) methods?", "answer": ["Domain-Adversarial Training of Neural Networks", "A review of domain adaptation without target labels"], "answer_arxiv_id": ["1505.07818", "1901.05335"], "source_meta": {"published_time": "20240306"}, "qid": "AutoScholarQuery_test_332"}
{"question": "Which studies proposed the use of a seq2seq architecture and a progressive transformer for Sign Language Production (SLP)?", "answer": ["Progressive Transformers for End-to-End Sign Language Production"], "answer_arxiv_id": ["2004.14874"], "source_meta": {"published_time": "20240611"}, "qid": "AutoScholarQuery_test_333"}
{"question": "What work stored manual edits in a memory module to amend the output of LLMs?", "answer": ["Memory-Based Model Editing at Scale"], "answer_arxiv_id": ["2206.06520"], "source_meta": {"published_time": "20231114"}, "qid": "AutoScholarQuery_test_334"}
{"question": "What researches have been conducted about privacy-preserving learning with learning invariant representations?", "answer": ["Minimax Filter: Learning to Preserve Privacy from Inference Attacks", "Privacy-preserving Neural Representations of Text", "Adversarial Learning of Privacy-Preserving and Task-Oriented Representations"], "answer_arxiv_id": ["1610.03577", "1808.09408", "1911.10143"], "source_meta": {"published_time": "20201219"}, "qid": "AutoScholarQuery_test_335"}
{"question": "What research papers have explored multimodal tasks within UI contexts?", "answer": ["VUT: Versatile UI Transformer for Multi-Modal Multi-Task User Interface\n  Modeling", "UIBert: Learning Generic Multimodal Representations for UI Understanding", "ActionBert: Leveraging User Actions for Semantic Understanding of User\n  Interfaces"], "answer_arxiv_id": ["2112.05692", "2107.13731", "2012.12350"], "source_meta": {"published_time": "20240117"}, "qid": "AutoScholarQuery_test_336"}
{"question": "What studies focus on the different techniques utilized to fine-tune the pre-trained models?", "answer": ["Scaling Instruction-Finetuned Language Models", "Training language models to follow instructions with human feedback", "Parameter-Efficient Transfer Learning for NLP", "LoRA: Low-Rank Adaptation of Large Language Models", "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "The Power of Scale for Parameter-Efficient Prompt Tuning", "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally\n  Across Scales and Tasks"], "answer_arxiv_id": ["2210.11416", "2203.02155", "1902.00751", "2106.09685", "2101.00190", "2104.08691", "2110.07602"], "source_meta": {"published_time": "20231010"}, "qid": "AutoScholarQuery_test_337"}
{"question": "Which works discussed the relation between sinusoidal networks and networks with Fourier feature transformations?", "answer": ["Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains"], "answer_arxiv_id": ["2006.10739"], "source_meta": {"published_time": "20221126"}, "qid": "AutoScholarQuery_test_338"}
{"question": "Could you provide me some works that enhance unimodal language models with retrieval?", "answer": ["Improving language models by retrieving from trillions of tokens", "Few-shot Learning with Retrieval Augmented Language Models"], "answer_arxiv_id": ["2112.04426", "2208.03299"], "source_meta": {"published_time": "20221004"}, "qid": "AutoScholarQuery_test_339"}
{"question": "Which works have utilized instructional videos for online learning, especially in the medical field?", "answer": ["A Dataset for Medical Instructional Video Classification and Question Answering"], "answer_arxiv_id": ["2201.12888"], "source_meta": {"published_time": "20231020"}, "qid": "AutoScholarQuery_test_340"}
{"question": "Who designed attentional GNNs by discretizing parabolic diffusion-type PDEs?", "answer": ["GRAND: Graph Neural Diffusion"], "answer_arxiv_id": ["2106.10934"], "source_meta": {"published_time": "20221002"}, "qid": "AutoScholarQuery_test_341"}
{"question": "Which studies investigated zero-shot NAS methods aiming to reduce search costs?", "answer": ["Zero-Cost Proxies for Lightweight NAS", "Neural Architecture Search without Training"], "answer_arxiv_id": ["2101.08134v2", "2006.04647"], "source_meta": {"published_time": "20230608"}, "qid": "AutoScholarQuery_test_342"}
{"question": "Which papers approach the studies about adversarial attacks?", "answer": ["Intriguing properties of neural networks", "Evasion Attacks against Machine Learning at Test Time", "Towards Evaluating the Robustness of Neural Networks", "Towards Deep Learning Models Resistant to Adversarial Attacks", "Reliable evaluation of adversarial robustness with an ensemble of\n  diverse parameter-free attacks", "Obfuscated Gradients Give a False Sense of Security: Circumventing\n  Defenses to Adversarial Examples"], "answer_arxiv_id": ["1312.6199", "1708.06131", "1608.04644", "1706.06083", "2003.01690", "1802.00420"], "source_meta": {"published_time": "20231206"}, "qid": "AutoScholarQuery_test_343"}
{"question": "What papers give an account of the datasets developed for the vision-based 3D Semantic Occupancy Prediction?", "answer": ["SSCBench: A Large-Scale 3D Semantic Scene Completion Benchmark for Autonomous Driving", "OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic\n  Occupancy Perception", "Occ3D: A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous\n  Driving"], "answer_arxiv_id": ["2306.09001v3", "2303.03991", "2304.14365"], "source_meta": {"published_time": "20240212"}, "qid": "AutoScholarQuery_test_344"}
{"question": "Which studies surpassed traditional image compression standards by using neural image compression methods?", "answer": ["Causal Contextual Prediction for Learned Image Compression", "Learned Image Compression with Mixed Transformer-CNN Architectures", "High-Fidelity Generative Image Compression"], "answer_arxiv_id": ["2011.09704", "2303.14978", "2006.09965"], "source_meta": {"published_time": "20230530"}, "qid": "AutoScholarQuery_test_345"}
{"question": "Which research works have explored the organizational structures required to facilitate the development of research communities around under-represented languages?", "answer": ["AI4D -- African Language Program", "NLP for Ghanaian Languages"], "answer_arxiv_id": ["2104.02516", "2103.15475"], "source_meta": {"published_time": "20240209"}, "qid": "AutoScholarQuery_test_346"}
{"question": "Which studies proposed a homotopy algorithm in the context of a response vector parameterized with a real value?", "answer": ["Fast Exact Conformalization of Lasso using Piecewise Linear Homotopy"], "answer_arxiv_id": ["1708.00427"], "source_meta": {"published_time": "20230711"}, "qid": "AutoScholarQuery_test_347"}
{"question": "What works proposed kernel-based distillation methods?", "answer": ["Dataset Distillation using Neural Feature Regression", "Efficient Dataset Distillation Using Random Feature Approximation"], "answer_arxiv_id": ["2206.00719", "2210.12067"], "source_meta": {"published_time": "20231206"}, "qid": "AutoScholarQuery_test_348"}
{"question": "What research allows direct NeRF editing by changing a reference image in 2D space?", "answer": ["SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing\n  Field"], "answer_arxiv_id": ["2303.13277"], "source_meta": {"published_time": "20240103"}, "qid": "AutoScholarQuery_test_349"}
{"question": "What works focus on obtaining strong visual representations through self-supervised training on large-scale image data?", "answer": ["Emerging Properties in Self-Supervised Vision Transformers", "Masked Autoencoders Are Scalable Vision Learners", "EVA: Exploring the Limits of Masked Visual Representation Learning at\n  Scale"], "answer_arxiv_id": ["2104.14294", "2111.06377", "2211.07636"], "source_meta": {"published_time": "20231214"}, "qid": "AutoScholarQuery_test_350"}
{"question": "Can you refer any works that are used rounding or merging strategies to produce long feature tracks?", "answer": ["ASpanFormer: Detector-Free Image Matching with Adaptive Span Transformer"], "answer_arxiv_id": ["2208.14201"], "source_meta": {"published_time": "20230627"}, "qid": "AutoScholarQuery_test_351"}
{"question": "What is the research proposing a two-step approach involving foreground-background separation and text spatial alignment for scene text editing?", "answer": ["Editing Text in the Wild"], "answer_arxiv_id": ["1908.03047"], "source_meta": {"published_time": "20230518"}, "qid": "AutoScholarQuery_test_352"}
{"question": "Could you provide me some works about zero-shot classification?", "answer": ["Zero-Shot Learning - A Comprehensive Evaluation of the Good, the Bad and the Ugly"], "answer_arxiv_id": ["1707.00600"], "source_meta": {"published_time": "20230215"}, "qid": "AutoScholarQuery_test_353"}
{"question": "What research extended the work of Zaheer et al. by using hierarchical clustering to obtain fine-grained pseudo-labels?", "answer": ["A Coarse-to-Fine Pseudo-Labeling (C2FPL) Framework for Unsupervised\n  Video Anomaly Detection"], "answer_arxiv_id": ["2310.17650"], "source_meta": {"published_time": "20240401"}, "qid": "AutoScholarQuery_test_354"}
{"question": "Which paper discussed about the retrieval augmented generation (RAG) solutions?", "answer": ["Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"], "answer_arxiv_id": ["2005.11401"], "source_meta": {"published_time": "20240614"}, "qid": "AutoScholarQuery_test_355"}
{"question": "Which paper uses an Iterative Dataset Update(IDU) strategy to edit NeRF’s image dataset?", "answer": ["Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions"], "answer_arxiv_id": ["2303.12789"], "source_meta": {"published_time": "20240103"}, "qid": "AutoScholarQuery_test_356"}
{"question": "What studies showed that maximum distance from a dataset to its subset was an effective measure of risk for instance optimality?", "answer": ["Near Instance-Optimality in Differential Privacy"], "answer_arxiv_id": ["2005.10630"], "source_meta": {"published_time": "20230301"}, "qid": "AutoScholarQuery_test_357"}
{"question": "Which studies reported that the performance of LLMs declines as the proportion of noise in the retrieval context increases?", "answer": ["Benchmarking Large Language Models in Retrieval-Augmented Generation", "Making Retrieval-Augmented Language Models Robust to Irrelevant Context", "NoMIRACL: Knowing When You Don't Know for Robust Multilingual\n  Retrieval-Augmented Generation"], "answer_arxiv_id": ["2309.01431", "2310.01558", "2312.11361"], "source_meta": {"published_time": "20240531"}, "qid": "AutoScholarQuery_test_358"}
{"question": "What works have used hypercolumns for tasks like keypoint detection, segmentation and semantic correspondence?", "answer": ["Hypercolumns for Object Segmentation and Fine-grained Localization", "Deep Layer Aggregation", "Hyperpixel Flow: Semantic Correspondence with Multi-layer Neural Features", "AnchorNet: A Weakly Supervised Network to Learn Geometry-sensitive Features For Semantic Matching", "Learning to Compose Hypercolumns for Visual Correspondence", "Neural Best-Buddies: Sparse Cross-Domain Correspondence"], "answer_arxiv_id": ["1411.5752", "1707.06484", "1908.06537", "1704.04749", "2007.10587", "1805.04140v2"], "source_meta": {"published_time": "20230523"}, "qid": "AutoScholarQuery_test_359"}
{"question": "Which papers mentioned about automatic learning of prompts, often termed as 'Prompt Learning'?", "answer": ["The Power of Scale for Parameter-Efficient Prompt Tuning"], "answer_arxiv_id": ["2104.08691"], "source_meta": {"published_time": "20231219"}, "qid": "AutoScholarQuery_test_360"}
{"question": "What paper uses dynamically-updatable tree-sketches in the context of Kronecker regression?", "answer": ["Dynamic Tensor Product Regression"], "answer_arxiv_id": ["2210.03961v2"], "source_meta": {"published_time": "20230129"}, "qid": "AutoScholarQuery_test_361"}
{"question": "Which references mentioned that the estimation of a policy gradient in policy-gradient approaches requires more data than in their benchmarks?", "answer": ["Fast Context Adaptation via Meta-Learning", "A Survey of Meta-Reinforcement Learning"], "answer_arxiv_id": ["1810.03642", "2301.08028"], "source_meta": {"published_time": "20230926"}, "qid": "AutoScholarQuery_test_362"}
{"question": "Can you provide some examples of studies that used differentiable simulation for efficient co-optimization of soft robots?", "answer": ["DiffAqua: A Differentiable Computational Design Pipeline for Soft Underwater Swimmers with Shape Interpolation"], "answer_arxiv_id": ["2104.00837"], "source_meta": {"published_time": "20230316"}, "qid": "AutoScholarQuery_test_363"}
{"question": "Which studies have examined the Polyak-Lojasiewicz (PL) inequality, a generalization of strong-convexity?", "answer": ["Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-Łojasiewicz Condition"], "answer_arxiv_id": ["1608.04636"], "source_meta": {"published_time": "20220613"}, "qid": "AutoScholarQuery_test_364"}
{"question": "Which works have explored self-consistency techniques for refining language models in post-hoc correction?", "answer": ["Language Models (Mostly) Know What They Know", "Self-Evaluation Improves Selective Generation in Large Language Models", "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence\n  Scores from Language Models Fine-Tuned with Human Feedback", "Self-Refine: Iterative Refinement with Self-Feedback", "Chain-of-Verification Reduces Hallucination in Large Language Models", "Self-Consistency Improves Chain of Thought Reasoning in Language Models"], "answer_arxiv_id": ["2207.05221", "2312.09300", "2305.14975", "2303.17651", "2309.11495", "2203.11171"], "source_meta": {"published_time": "20240214"}, "qid": "AutoScholarQuery_test_365"}
{"question": "Could you provide me some studies that attempted to mitigate bias found in in-context learning by utilizing outputs distribution obtained from content-free texts?", "answer": ["Calibrate Before Use: Improving Few-Shot Performance of Language Models", "Mitigating Label Biases for In-context Learning"], "answer_arxiv_id": ["2102.09690", "2305.19148"], "source_meta": {"published_time": "20231116"}, "qid": "AutoScholarQuery_test_366"}
{"question": "What papers propose a reduction from list-global stability to pseudo-global stability via correlated sampling in finite domains?", "answer": ["User-Level Private Learning via Correlated Sampling"], "answer_arxiv_id": ["2110.11208"], "source_meta": {"published_time": "20230523"}, "qid": "AutoScholarQuery_test_367"}
{"question": "Which works provide a categorization of existing graph OOD generalization methodologies?", "answer": ["Out-Of-Distribution Generalization on Graphs: A Survey"], "answer_arxiv_id": ["2202.07987"], "source_meta": {"published_time": "20231023"}, "qid": "AutoScholarQuery_test_368"}
{"question": "What studies are about Mixture of Experts, a technique used to improve robustness and overall accuracy in ensemble learning?", "answer": ["Outrageously Large Neural Networks: The Sparsely-Gated\n  Mixture-of-Experts Layer", "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"], "answer_arxiv_id": ["1701.06538", "2112.06905"], "source_meta": {"published_time": "20230922"}, "qid": "AutoScholarQuery_test_369"}
{"question": "Can you indicate which studies have investigated the trade-off between personalization and performance?", "answer": ["Operationalizing the Legal Principle of Data Minimization for Personalization", "Learning to Limit Data Collection via Scaling Laws: A Computational Interpretation for the Legal Principle of Data Minimization"], "answer_arxiv_id": ["2005.13718", "2107.08096"], "source_meta": {"published_time": "20230208"}, "qid": "AutoScholarQuery_test_370"}
{"question": "Which paper proposed the ROAR algorithm for finding the closest and robust counterfactuals?", "answer": ["Towards Robust and Reliable Algorithmic Recourse"], "answer_arxiv_id": ["2102.13620"], "source_meta": {"published_time": "20230519"}, "qid": "AutoScholarQuery_test_371"}
{"question": "Which studies deal with aligning visual features with pre-trained LLMs for multimodal comprehension tasks?", "answer": ["MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large\n  Language Models", "Image as a Foreign Language: BEiT Pretraining for All Vision and\n  Vision-Language Tasks", "Visual Instruction Tuning", "mPLUG-Owl: Modularization Empowers Large Language Models with\n  Multimodality", "Language Is Not All You Need: Aligning Perception with Language Models", "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image\n  Encoders and Large Language Models", "VisionLLM: Large Language Model is also an Open-Ended Decoder for\n  Vision-Centric Tasks", "Otter: A Multi-Modal Model with In-Context Instruction Tuning"], "answer_arxiv_id": ["2304.10592", "2208.10442", "2304.08485", "2304.14178", "2302.14045", "2301.12597", "2305.11175", "2305.03726"], "source_meta": {"published_time": "20231205"}, "qid": "AutoScholarQuery_test_372"}
{"question": "Which studies have designed additional supervision signals or training processes for models in context of CoT reasoning?", "answer": ["Tailoring Self-Rationalizers with Multi-Reward Distillation", "Crystal: Introspective Reasoners Reinforced with Self-Feedback"], "answer_arxiv_id": ["2311.02805", "2310.04921"], "source_meta": {"published_time": "20240228"}, "qid": "AutoScholarQuery_test_373"}
{"question": "What works present the intervention technique of steering model output?", "answer": ["Plug and Play Language Models: a Simple Approach to Controlled Text Generation", "Back to the Future: Unsupervised Backprop-based Decoding for Counterfactual and Abductive Commonsense Reasoning", "GeDi: Generative Discriminator guided Sequence Generation", "Diffusion-LM Improves Controllable Text Generation"], "answer_arxiv_id": ["1912.02164", "2010.05906", "2009.06367", "2205.14217"], "source_meta": {"published_time": "20221024"}, "qid": "AutoScholarQuery_test_374"}
{"question": "What research papers focus on minimizing the surrogate models learned from synthetic and original datasets in dataset distillation using matching gradients?", "answer": ["Dataset Condensation with Gradient Matching", "Dataset Condensation via Efficient Synthetic-Data Parameterization", "Accelerating Dataset Distillation via Model Augmentation"], "answer_arxiv_id": ["2006.05929", "2205.14959v2", "2212.06152"], "source_meta": {"published_time": "20231206"}, "qid": "AutoScholarQuery_test_375"}
{"question": "What works study the geometry of embeddings that decomposes the shifted pointwise mutual information matrix?", "answer": ["Towards Understanding Linear Word Analogies"], "answer_arxiv_id": ["1810.04882"], "source_meta": {"published_time": "20231026"}, "qid": "AutoScholarQuery_test_376"}
{"question": "Are there any papers that use MIAs to assess whether a given data point was used to train an LLM?", "answer": ["Membership Inference Attacks From First Principles", "Membership Inference Attacks Against Machine Learning Models"], "answer_arxiv_id": ["2112.03570", "1610.05820"], "source_meta": {"published_time": "20230524"}, "qid": "AutoScholarQuery_test_377"}
{"question": "What research directly trains sparse GANs from scratch?", "answer": ["Don’t Be So Dense: Sparse-to-Sparse GAN Training Without Sacrificing Performance"], "answer_arxiv_id": ["2203.02770"], "source_meta": {"published_time": "20230228"}, "qid": "AutoScholarQuery_test_378"}
{"question": "Could you name any studies that focus on report-to-CXR generation?", "answer": ["High-Resolution Image Synthesis with Latent Diffusion Models", "Adapting Pretrained Vision-Language Foundational Models to Medical\n  Imaging Domains", "RoentGen: Vision-Language Foundation Model for Chest X-ray Generation"], "answer_arxiv_id": ["2112.10752", "2210.04133", "2211.12737"], "source_meta": {"published_time": "20231213"}, "qid": "AutoScholarQuery_test_379"}
{"question": "What paper provides a connection between the matrix mechanism and the line of work about generating differentially private synthetic data?", "answer": ["Graphical-model based estimation and inference for differential privacy"], "answer_arxiv_id": ["1901.09136"], "source_meta": {"published_time": "20230514"}, "qid": "AutoScholarQuery_test_380"}
{"question": "What works employed human pose estimation using 3D shape rendering from multiple views?", "answer": ["Multi-View Multi-Person 3D Pose Estimation with Plane Sweep Stereo"], "answer_arxiv_id": ["2104.02273"], "source_meta": {"published_time": "20231128"}, "qid": "AutoScholarQuery_test_381"}
{"question": "Which studies regarded Kernel ridge regression as a special kind of spectral regularization algorithm?", "answer": ["Regularization in kernel learning"], "answer_arxiv_id": ["1001.2094v1"], "source_meta": {"published_time": "20230512"}, "qid": "AutoScholarQuery_test_382"}
{"question": "Which paper first proposed Graph Contrastive Learning (GCL) with random edge dropping and feature masking as data augmentations?", "answer": ["Deep Graph Contrastive Representation Learning"], "answer_arxiv_id": ["2006.04131"], "source_meta": {"published_time": "20230925"}, "qid": "AutoScholarQuery_test_383"}
{"question": "What studies have suggested the usage of produced clusters for multi-document summarization?", "answer": ["Multi-News: a Large-Scale Multi-Document Summarization Dataset and\n  Abstractive Hierarchical Model", "Multi-XScience: A Large-scale Dataset for Extreme Multi-document\n  Summarization of Scientific Articles"], "answer_arxiv_id": ["1906.01749", "2010.14235"], "source_meta": {"published_time": "20240215"}, "qid": "AutoScholarQuery_test_384"}
{"question": "Which research gave regret bounds for a E2D that incorporates randomized estimators, but not optimism?", "answer": ["Unified Algorithms for RL with Decision-Estimation Coefficients: No-Regret, PAC, and Reward-Free Learning"], "answer_arxiv_id": ["2209.11745"], "source_meta": {"published_time": "20221125"}, "qid": "AutoScholarQuery_test_385"}
{"question": "What studies introduced strategies to improve the computational and parameter efficiency of transformers by refining parameter-sharing mechanisms?", "answer": ["Lessons on Parameter Sharing across Layers in Transformers"], "answer_arxiv_id": ["2104.06022"], "source_meta": {"published_time": "20240224"}, "qid": "AutoScholarQuery_test_386"}
{"question": "Could you provide me with research papers about learning a disentangled representation for RL?", "answer": ["DARLA: Improving Zero-Shot Transfer in Reinforcement Learning", "Temporal Disentanglement of Representations for Improved Generalisation in Reinforcement Learning"], "answer_arxiv_id": ["1707.08475", "2207.05480"], "source_meta": {"published_time": "20230523"}, "qid": "AutoScholarQuery_test_387"}
{"question": "Which studies treated Transformer-based backbones as a 'black box'? ", "answer": ["Faster-TAD: Towards Temporal Action Detection with Proposal Generation\n  and Classification in a Unified Network", "TALLFormer: Temporal Action Localization with a Long-memory Transformer", "Re^2TAL: Rewiring Pretrained Video Backbones for Reversible Temporal\n  Action Localization"], "answer_arxiv_id": ["2204.02674", "2204.01680", "2211.14053"], "source_meta": {"published_time": "20231204"}, "qid": "AutoScholarQuery_test_388"}
{"question": "Which studies first used CLIP to optimize an underlying 3D representation in text-guided 3D generation?", "answer": ["Zero-Shot Text-Guided Object Generation with Dream Fields", "CLIP-Mesh: Generating textured meshes from text using pretrained image-text models", "Learning Transferable Visual Models From Natural Language Supervision"], "answer_arxiv_id": ["2112.01455", "2203.13333", "2103.00020"], "source_meta": {"published_time": "20230521"}, "qid": "AutoScholarQuery_test_389"}
{"question": "What papers mention the vulnerability of DNNs to common corruptions, random noises, and adversarial perturbations?", "answer": ["Benchmarking Neural Network Robustness to Common Corruptions and Perturbations", "A Study and Comparison of Human and Deep Learning Recognition Performance Under Visual Distortions", "Evasion attacks against machine learning at test time", "Intriguing properties of neural networks"], "answer_arxiv_id": ["1903.12261", "1705.02498", "1708.06131", "1312.6199"], "source_meta": {"published_time": "20230608"}, "qid": "AutoScholarQuery_test_390"}
{"question": "Are there any studies regarding the creation of evaluation benchmarks for Natural Language Generation (NLG) on Indic languages?", "answer": ["IndicNLG Benchmark: Multilingual Datasets for Diverse NLG Tasks in Indic Languages"], "answer_arxiv_id": ["2203.05437v2"], "source_meta": {"published_time": "20240425"}, "qid": "AutoScholarQuery_test_391"}
{"question": "Which work first initiated the efforts to manipulate Neural Radiance Fields (NeRFs)?", "answer": ["NeRF-Editing: Geometry Editing of Neural Radiance Fields"], "answer_arxiv_id": ["2205.04978"], "source_meta": {"published_time": "20240103"}, "qid": "AutoScholarQuery_test_392"}
{"question": "Could you refer me to some studies that use score-based models for graph generation?", "answer": ["Permutation Invariant Graph Generation via Score-Based Generative Modeling", "Score-based Generative Modeling of Graphs via the System of Stochastic Differential Equations", "Score-Based Generative Modeling through Stochastic Differential Equations", "DiGress: Discrete Denoising diffusion for graph generation", "Diffusion Models for Graphs Benefit From Discrete State Spaces"], "answer_arxiv_id": ["2003.00638", "2202.02514", "2011.13456", "2209.14734", "2210.01549"], "source_meta": {"published_time": "20230511"}, "qid": "AutoScholarQuery_test_393"}
{"question": "Could you provide some studies about unfaithful hallucination?", "answer": ["Truthful AI: Developing and governing AI that does not lie", "AI Deception: A Survey of Examples, Risks, and Potential Solutions", "Inference-Time Intervention: Eliciting Truthful Answers from a Language\n  Model"], "answer_arxiv_id": ["2110.06674", "2308.14752", "2306.03341"], "source_meta": {"published_time": "20240214"}, "qid": "AutoScholarQuery_test_394"}
{"question": "Any works that introduced a method of learning policy by simulating states via the dynamics model?", "answer": ["Model Based Reinforcement Learning for Atari"], "answer_arxiv_id": ["1903.00374"], "source_meta": {"published_time": "20221024"}, "qid": "AutoScholarQuery_test_395"}
{"question": "Which research made a claim that oversmoothing is asymptotically inevitable in GATs?", "answer": ["Improving Graph Attention Networks with Large Margin-based Constraints"], "answer_arxiv_id": ["1910.11945"], "source_meta": {"published_time": "20230525"}, "qid": "AutoScholarQuery_test_396"}
{"question": "Which work considers a different approach to limiting the amount of predicted information within learning-augmented paging?", "answer": ["Parsimonious Learning-Augmented Caching"], "answer_arxiv_id": ["2202.04262"], "source_meta": {"published_time": "20221006"}, "qid": "AutoScholarQuery_test_397"}
{"question": "What work demonstrates the use of transformers in 3D pose estimation task?", "answer": ["Epipolar Transformers"], "answer_arxiv_id": ["2005.04551"], "source_meta": {"published_time": "20240228"}, "qid": "AutoScholarQuery_test_398"}
{"question": "Are there any studies using a sequence-encoding VQ-VAE to learn the discrete codebook of listener motion?", "answer": ["Learning to Listen: Modeling Non-Deterministic Dyadic Facial Motion"], "answer_arxiv_id": ["2204.08451"], "source_meta": {"published_time": "20240301"}, "qid": "AutoScholarQuery_test_399"}
{"question": "What paper presents a form of data augmentation for training CLIP models?", "answer": ["Improving CLIP Training with Language Rewrites"], "answer_arxiv_id": ["2305.20088"], "source_meta": {"published_time": "20230719"}, "qid": "AutoScholarQuery_test_400"}
{"question": "Can you indicate some works that use 2D diffusion models to generate multi-view images then use them for 3D reconstruction with NeRF?", "answer": ["Novel View Synthesis with Diffusion Models", "Zero-1-to-3: Zero-shot One Image to 3D Object", "Viewset Diffusion: (0-)Image-Conditioned 3D Generative Models from 2D\n  Data", "One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape\n  Optimization"], "answer_arxiv_id": ["2210.04628", "2303.11328", "2306.07881", "2306.16928"], "source_meta": {"published_time": "20231226"}, "qid": "AutoScholarQuery_test_401"}
{"question": "Which works propose consistency-based methods for detecting non-factual generations in LLM generated content?", "answer": ["Measuring and Improving Consistency in Pretrained Language Models", "Self-contradictory Hallucinations of Large Language Models: Evaluation,\n  Detection and Mitigation", "How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking\n  Unrelated Questions", "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for\n  Generative Large Language Models", "LM vs LM: Detecting Factual Errors via Cross Examination", "The Internal State of an LLM Knows When It's Lying", "Chain-of-Verification Reduces Hallucination in Large Language Models", "Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation\n  in Natural Language Generation", "Language Models (Mostly) Know What They Know", "Representation Engineering: A Top-Down Approach to AI Transparency", "Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic\n  Fact-checkers", "RARR: Researching and Revising What Language Models Say, Using Language\n  Models", "FacTool: Factuality Detection in Generative AI -- A Tool Augmented\n  Framework for Multi-Task and Multi-Domain Scenarios"], "answer_arxiv_id": ["2102.01017", "2305.15852", "2309.15840", "2303.08896", "2305.13281", "2304.13734", "2309.11495", "2302.09664", "2207.05221", "2310.01405", "2311.09000", "2210.08726", "2307.13528"], "source_meta": {"published_time": "20240410"}, "qid": "AutoScholarQuery_test_402"}
{"question": "Can you list the works that followed the concept of neural fields for 3D scene and object representation?", "answer": ["NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis", "Multiview Neural Surface Reconstruction by Disentangling Geometry and\n  Appearance"], "answer_arxiv_id": ["2003.08934", "2003.09852"], "source_meta": {"published_time": "20231129"}, "qid": "AutoScholarQuery_test_403"}
{"question": "Who uses similarities between word sense definitions to approximate human judgments on semantic proximity?", "answer": ["Interpretable Word Sense Representations via Definition Generation: The\n  Case of Semantic Change Analysis"], "answer_arxiv_id": ["2305.11993"], "source_meta": {"published_time": "20240605"}, "qid": "AutoScholarQuery_test_404"}
{"question": "Could you provide me some works about human-curated instruction datasets in languages outside of English?", "answer": ["Crosslingual Generalization through Multitask Finetuning", "M$^3$IT: A Large-Scale Dataset towards Multi-Modal Multilingual\n  Instruction Tuning"], "answer_arxiv_id": ["2211.01786", "2306.04387"], "source_meta": {"published_time": "20240209"}, "qid": "AutoScholarQuery_test_405"}
{"question": "Who are the pioneer researchers in providing different contexts for a single visual concept using multiple images as part of personalized visual content generation?", "answer": ["An Image is Worth One Word: Personalizing Text-to-Image Generation using\n  Textual Inversion", "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for\n  Subject-Driven Generation"], "answer_arxiv_id": ["2208.01618", "2208.12242"], "source_meta": {"published_time": "20231211"}, "qid": "AutoScholarQuery_test_406"}
{"question": "Are there any studies in sports video understanding which involves benchmarks for spatio-temporal reasoning?", "answer": ["UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild", "MultiSports: A Multi-Person Video Dataset of Spatio-Temporally Localized\n  Sports Actions", "FineGym: A Hierarchical Video Dataset for Fine-grained Action\n  Understanding", "SoccerNet: A Scalable Dataset for Action Spotting in Soccer Videos", "SportsMOT: A Large Multi-Object Tracking Dataset in Multiple Sports\n  Scenes", "Social Adaptive Module for Weakly-supervised Group Activity Recognition", "A Hierarchical Deep Temporal Model for Group Activity Recognition"], "answer_arxiv_id": ["1212.0402", "2105.07404", "2004.06704", "1804.04527", "2304.05170", "2007.09470", "1511.06040"], "source_meta": {"published_time": "20240406"}, "qid": "AutoScholarQuery_test_407"}
{"question": "Could you provide me some studies of the use of beam search on KGs using LLMs to dynamically extract the most relevant reasoning paths?", "answer": ["Think-on-Graph: Deep and Responsible Reasoning of Large Language Model\n  on Knowledge Graph"], "answer_arxiv_id": ["2307.07697"], "source_meta": {"published_time": "20240614"}, "qid": "AutoScholarQuery_test_408"}
{"question": "What research observed the detrimental effect of adding irrelevant noise to the context on model performance?", "answer": ["Adversarial Examples for Evaluating Reading Comprehension Systems", "Selection-Inference: Exploiting Large Language Models for Interpretable\n  Logical Reasoning"], "answer_arxiv_id": ["1707.07328", "2205.09712"], "source_meta": {"published_time": "20240531"}, "qid": "AutoScholarQuery_test_409"}
{"question": "What papers explored hallucination source and detection in Linguistically-Informed Language Models (LLMs)?", "answer": ["A Survey on Hallucination in Large Language Models: Principles,\n  Taxonomy, Challenges, and Open Questions", "Survey of Hallucination in Natural Language Generation", "Siren's Song in the AI Ocean: A Survey on Hallucination in Large\n  Language Models", "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large\n  Language Models"], "answer_arxiv_id": ["2311.05232", "2202.03629", "2309.01219", "2305.11747"], "source_meta": {"published_time": "20240106"}, "qid": "AutoScholarQuery_test_410"}
{"question": "Could you provide me some works that provide convergence guarantees of stochastic methods for solving quasi-strongly monotone Variational inequality problems (VIPs)?", "answer": ["Stochastic Extragradient: General Analysis and Improved Rates", "On the Convergence of Single-Call Stochastic Extra-Gradient Methods"], "answer_arxiv_id": ["2111.08611v3", "1908.08465"], "source_meta": {"published_time": "20230227"}, "qid": "AutoScholarQuery_test_411"}
{"question": "What references were made to works related to transformer-based models in hierarchical classification research?", "answer": ["An Image is Worth 16x16 Words: Transformers for Image Recognition at\n  Scale", "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows", "Swin Transformer V2: Scaling Up Capacity and Resolution"], "answer_arxiv_id": ["2010.11929", "2103.14030", "2111.09883"], "source_meta": {"published_time": "20230604"}, "qid": "AutoScholarQuery_test_412"}
{"question": "Which paper tackles the problem of registration of two scene fragments captured from two 3D viewpoints with low overlap?", "answer": ["ObjectMatch: Robust Registration using Canonical Object Correspondences"], "answer_arxiv_id": ["2212.01985"], "source_meta": {"published_time": "20231201"}, "qid": "AutoScholarQuery_test_413"}
{"question": "Which studies have leveraged object template information for identifying potential object locations?", "answer": ["Incremental Class Discovery for Semantic Segmentation with RGBD Sensing"], "answer_arxiv_id": ["1907.10008"], "source_meta": {"published_time": "20230325"}, "qid": "AutoScholarQuery_test_414"}
{"question": "Which paper is about the Transformers architecture that most recent large language models (LLM) are based on?", "answer": ["Attention Is All You Need"], "answer_arxiv_id": ["1706.03762"], "source_meta": {"published_time": "20240208"}, "qid": "AutoScholarQuery_test_415"}
{"question": "Which studies highlight that increased model capacity is needed to achieve robustness against adversarial examples?", "answer": ["Adversarial Robustness May Be at Odds With Simplicity"], "answer_arxiv_id": ["1901.00532"], "source_meta": {"published_time": "20240326"}, "qid": "AutoScholarQuery_test_416"}
{"question": "Could you provide me some works that apply Gaussian corruptions to token-vector embeddings in diffusion models?", "answer": ["Self-conditioned Embedding Diffusion for Text Generation", "Continuous diffusion for categorical data"], "answer_arxiv_id": ["2211.04236", "2211.15089"], "source_meta": {"published_time": "20230531"}, "qid": "AutoScholarQuery_test_417"}
{"question": "Could you list down studies that convert building footprint extraction into roof segmentation and roof-to-footprint offset estimation tasks?", "answer": ["Learning to Extract Building Footprints from Off-Nadir Aerial Images"], "answer_arxiv_id": ["2204.13637"], "source_meta": {"published_time": "20240407"}, "qid": "AutoScholarQuery_test_418"}
{"question": "Which paper proposed a Mixed Integer Linear Programming based method to relax GNN certification?", "answer": ["Collective Robustness Certificates: Exploiting Interdependence in Graph Neural Networks"], "answer_arxiv_id": ["2302.02829"], "source_meta": {"published_time": "20230925"}, "qid": "AutoScholarQuery_test_419"}
{"question": "What works discuss initializations from the optimization perspective?", "answer": ["Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification", "Mean Field Residual Networks: On the Edge of Chaos", "Tunable Efficient Unitary Neural Networks (EUNN) and their application to RNNs", "Revisiting Initialization of Neural Networks"], "answer_arxiv_id": ["1502.01852", "1712.08969", "1612.05231", "2004.09506v3"], "source_meta": {"published_time": "20221003"}, "qid": "AutoScholarQuery_test_420"}
{"question": "In which work is explained how LLMs have also been trained on instruction datasets, meaning they can generate text using a custom instruction prompt?", "answer": ["Training language models to follow instructions with human feedback"], "answer_arxiv_id": ["2203.02155"], "source_meta": {"published_time": "20240208"}, "qid": "AutoScholarQuery_test_421"}
{"question": "Could you provide me studies that apply CoT prompting for multiple reasoning traces and diversifying reasoning paths?", "answer": ["Self-Consistency Improves Chain of Thought Reasoning in Language Models", "Cross-lingual Prompting: Improving Zero-shot Chain-of-Thought Reasoning\n  across Languages"], "answer_arxiv_id": ["2203.11171", "2310.14799"], "source_meta": {"published_time": "20240628"}, "qid": "AutoScholarQuery_test_422"}
{"question": "Which research papers employ recurrent attention for routing but not for inferring slots?", "answer": ["MONet: Unsupervised Scene Decomposition and Representation", "Genesis: Generative Scene Inference and Sampling with Object-Centric Latent Representations"], "answer_arxiv_id": ["1901.11390", "1907.13052"], "source_meta": {"published_time": "20230524"}, "qid": "AutoScholarQuery_test_423"}
{"question": "Which papers talk about the upper bounds for general activation functions?", "answer": ["Universal Approximation with Deep Narrow Networks", "Minimum Width for Universal Approximation"], "answer_arxiv_id": ["1905.08539", "2006.08859"], "source_meta": {"published_time": "20220923"}, "qid": "AutoScholarQuery_test_424"}
{"question": "Can you provide the references of studies that have employed diffusion models for text-driven image editing?", "answer": ["SEGA: Instructing Diffusion using Semantic Dimensions", "InstructPix2Pix: Learning to Follow Image Editing Instructions"], "answer_arxiv_id": ["2301.12247", "2211.09800"], "source_meta": {"published_time": "20230518"}, "qid": "AutoScholarQuery_test_425"}
{"question": "Which are the studies that have done efforts via multi-task learning in personalized Federated Learning?", "answer": ["Federated Multi-Task Learning", "Personalized Cross-Silo Federated Learning on Non-IID Data", "Federated Multi-Task Learning under a Mixture of Distributions"], "answer_arxiv_id": ["1705.10467", "2007.03797", "2108.10252v4"], "source_meta": {"published_time": "20230504"}, "qid": "AutoScholarQuery_test_426"}
{"question": "What studies discuss the field of 'learning from human feedback'?", "answer": ["Neural Machine Translation by Jointly Learning to Align and Translate", "WebGPT: Browser-assisted question-answering with human feedback", "Training language models to follow instructions with human feedback", "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback", "Constitutional AI: Harmlessness from AI Feedback", "ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation", "Text-guided Image-and-Shape Editing and Generation: A Short Survey", "Aligning Text-to-Image Models using Human Feedback", "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment"], "answer_arxiv_id": ["1409.0473", "2112.09332", "2203.02155", "2204.05862", "2212.08073", "2304.05977", "2304.09244", "2302.12192", "2304.06767"], "source_meta": {"published_time": "20230616"}, "qid": "AutoScholarQuery_test_427"}
{"question": "What work discusses the limitation of how answer generation can be updated based on retrievd documents?", "answer": ["Entity-Based Knowledge Conflicts in Question Answering"], "answer_arxiv_id": ["2109.05052"], "source_meta": {"published_time": "20220727"}, "qid": "AutoScholarQuery_test_428"}
{"question": "What study proposed a hierarchical transformer architecture that unifies semantic tokens and stacked hierarchical acoustic tokens within one stage?", "answer": ["UniAudio: An Audio Foundation Model Toward Universal Audio Generation"], "answer_arxiv_id": ["2310.00704"], "source_meta": {"published_time": "20240603"}, "qid": "AutoScholarQuery_test_429"}
{"question": "What are some existing methods proposed for Federated Domain Generalization?", "answer": ["FedDG: Federated Domain Generalization on Medical Image Segmentation via Episodic Learning in Continuous Frequency Space", "Federated Domain Generalization for Image Recognition via Cross-Client Style Transfer"], "answer_arxiv_id": ["2103.06030", "2210.00912"], "source_meta": {"published_time": "20231101"}, "qid": "AutoScholarQuery_test_430"}
{"question": "Which papers present locate-then-edit methods for knowledge editing?", "answer": ["Knowledge Neurons in Pretrained Transformers", "Locating and Editing Factual Associations in GPT", "Mass-Editing Memory in a Transformer"], "answer_arxiv_id": ["2104.08696", "2202.05262", "2210.07229"], "source_meta": {"published_time": "20230916"}, "qid": "AutoScholarQuery_test_431"}
{"question": "What are some works that have focused on how LLMs can be connected to visual foundation models?", "answer": ["Flamingo: a Visual Language Model for Few-Shot Learning", "Language-based Action Concept Spaces Improve Video Self-Supervised\n  Learning", "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image\n  Encoders and Large Language Models", "Visual Instruction Tuning", "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and\n  Language Models"], "answer_arxiv_id": ["2204.14198", "2307.10922", "2301.12597", "2304.08485", "2306.05424"], "source_meta": {"published_time": "20240411"}, "qid": "AutoScholarQuery_test_432"}
{"question": "Could you provide me some works that reveal the limitations of LLMs' ability in providing emotional support?", "answer": ["Challenges of Large Language Models for Mental Health Counseling", "ChatGPT as a Therapist Assistant: A Suitability Study", "The Typing Cure: Experiences with Large Language Model Chatbots for\n  Mental Health Support"], "answer_arxiv_id": ["2311.13857", "2304.09873", "2401.14362"], "source_meta": {"published_time": "20240220"}, "qid": "AutoScholarQuery_test_433"}
{"question": "Could you provide me some studies about Neural Radiance Field (NeRF)?", "answer": ["NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis"], "answer_arxiv_id": ["2003.08934"], "source_meta": {"published_time": "20230411"}, "qid": "AutoScholarQuery_test_434"}
{"question": "Could you provide me some studies about imitation learning methods that doesn't strictly fit into IDM or GAIL based approaches?", "answer": ["Imitating Latent Policies from Observation", "Reinforcement Learning with Videos: Combining Offline Observations with Interaction"], "answer_arxiv_id": ["1805.07914", "2011.06507"], "source_meta": {"published_time": "20231204"}, "qid": "AutoScholarQuery_test_435"}
{"question": "Which research proposed discrete adaptations of the diffusion model in the context of text generation?", "answer": ["Structured Denoising Diffusion Models in Discrete State-Spaces"], "answer_arxiv_id": ["2107.03006"], "source_meta": {"published_time": "20230629"}, "qid": "AutoScholarQuery_test_436"}
{"question": "Could you provide me some works where human feedback was utilised to finetune large language models?", "answer": ["Neural Machine Translation by Jointly Learning to Align and Translate", "WebGPT: Browser-assisted question-answering with human feedback", "Training language models to follow instructions with human feedback", "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback", "Constitutional AI: Harmlessness from AI Feedback"], "answer_arxiv_id": ["1409.0473", "2112.09332", "2203.02155", "2204.05862", "2212.08073"], "source_meta": {"published_time": "20230616"}, "qid": "AutoScholarQuery_test_437"}
{"question": "What papers study the integration of visual encoding modules, like ViT, with LLMs such as LLaMa?", "answer": ["An Image is Worth 16x16 Words: Transformers for Image Recognition at\n  Scale", "LLaMA: Open and Efficient Foundation Language Models"], "answer_arxiv_id": ["2010.11929", "2302.13971"], "source_meta": {"published_time": "20240117"}, "qid": "AutoScholarQuery_test_438"}
{"question": "Which study was the first to consider the problem of differentially private submodular maximization in the context of CPPP?", "answer": ["Differentially Private Combinatorial Optimization"], "answer_arxiv_id": ["0903.4510"], "source_meta": {"published_time": "20221025"}, "qid": "AutoScholarQuery_test_439"}
{"question": "What research papers proposed using attention mechanisms to combine optical flow and deformable convolution for feature alignment?", "answer": ["Recurrent Video Restoration Transformer with Guided Deformable Attention", "Spatio-Temporal Deformable Attention Network for Video Deblurring", "BasicVSR++: Improving Video Super-Resolution with Enhanced Propagation\n  and Alignment", "Video Dehazing via a Multi-Range Temporal Alignment Network with\n  Physical Prior"], "answer_arxiv_id": ["2206.02146", "2207.10852", "2104.13371", "2303.09757"], "source_meta": {"published_time": "20240516"}, "qid": "AutoScholarQuery_test_440"}
{"question": "Which paper suggested that LLMs internally need to infer latent variables for better prediction in the context of in-context learning?", "answer": ["An Explanation of In-context Learning as Implicit Bayesian Inference"], "answer_arxiv_id": ["2111.02080"], "source_meta": {"published_time": "20230522"}, "qid": "AutoScholarQuery_test_441"}
{"question": "Are there any works on analytical reconstruction attacks which recursively reconstruct activation maps layer per layer?", "answer": ["R-GAP: Recursive Gradient Attack on Privacy"], "answer_arxiv_id": ["2010.07733"], "source_meta": {"published_time": "20230613"}, "qid": "AutoScholarQuery_test_442"}
{"question": "What is the first work that proposed a question-answering dataset that requires the model to perform counterfactual reasoning?", "answer": ["IfQA: A Dataset for Open-domain Question Answering under Counterfactual\n  Presuppositions"], "answer_arxiv_id": ["2305.14010"], "source_meta": {"published_time": "20231010"}, "qid": "AutoScholarQuery_test_443"}
{"question": "Which works focus on neural architecture search (NAS) for discovering optimal network structures?", "answer": ["Global Convergence of MAML and Theory-Inspired Neural Architecture Search for Few-Shot Learning", "DARTS: Differentiable Architecture Search"], "answer_arxiv_id": ["2203.09137", "1806.09055"], "source_meta": {"published_time": "20230228"}, "qid": "AutoScholarQuery_test_444"}
{"question": "Which works have used pre-computing or post-computing methods for feature aggregation in GNN models?", "answer": ["Simplifying Graph Convolutional Networks", "SIGN: Scalable Inception Graph Neural Networks", "Scalable and Adaptive Graph Neural Networks with Self-Label-Enhanced Training", "Graph Attention Multi-Layer Perceptron", "Scaling Graph Neural Networks with Approximate PageRank", "Combining Label Propagation and Simple Models out-performs Graph Neural Networks"], "answer_arxiv_id": ["1902.07153", "2004.11198", "2104.09376", "2206.04355", "2007.01570", "2010.13993"], "source_meta": {"published_time": "20230203"}, "qid": "AutoScholarQuery_test_445"}
{"question": "Could you provide me some works about the problem of exploding variance of the RP gradients due to the chaotic nature of environments?", "answer": ["PIPPS: Flexible Model-Based Policy Search Robust to the Curse of Chaos", "Gradients are Not All You Need"], "answer_arxiv_id": ["1902.01240", "2111.05803"], "source_meta": {"published_time": "20231214"}, "qid": "AutoScholarQuery_test_446"}
{"question": "What research introduced methods that adapt the training procedure of the classifier itself?", "answer": ["Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty", "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning", "Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches", "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles", "When Does Label Smoothing Help?", "Transferable Calibration with Lower Bias and Variance in Domain Adaptation", "On Mixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks", "mixup: Beyond Empirical Risk Minimization", "Evidential Deep Learning to Quantify Classification Uncertainty", "Towards Trustworthy Predictions from Deep Neural Networks with Fast Adversarial Calibration", "Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning"], "answer_arxiv_id": ["1906.12340", "1506.02142", "1803.04386", "1612.01474", "1906.02629", "2007.08259", "1905.11001", "1710.09412", "1806.01768", "2012.10923", "2002.06470"], "source_meta": {"published_time": "20230210"}, "qid": "AutoScholarQuery_test_447"}
{"question": "Which works look at achieving ϵ-optimal reward in non-convex settings?", "answer": ["Optimal Gradient-based Algorithms for Non-concave Bandit Optimization", "Optimal Stochastic Nonconvex Optimization with Bandit Feedback"], "answer_arxiv_id": ["2107.04518", "2103.16082"], "source_meta": {"published_time": "20230621"}, "qid": "AutoScholarQuery_test_448"}
{"question": "Which work proposed the Mirror Learning algorithm?", "answer": ["Mirror Learning: A Unifying Framework of Policy Optimisation"], "answer_arxiv_id": ["2201.02373"], "source_meta": {"published_time": "20230130"}, "qid": "AutoScholarQuery_test_449"}
{"question": "What works have explored video human-object interaction detection?", "answer": ["ST-HOI: A Spatial-Temporal Baseline for Human-Object Interaction\n  Detection in Videos"], "answer_arxiv_id": ["2105.11731"], "source_meta": {"published_time": "20240406"}, "qid": "AutoScholarQuery_test_450"}
{"question": "What are some representative works about graph embedding-based methods?", "answer": ["RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space", "Convolutional 2D Knowledge Graph Embeddings", "Complex Embeddings for Simple Link Prediction", "Holographic Embeddings of Knowledge Graphs", "kbgan: Adversarial Learning for Knowledge Graph Embeddings", "TuckER: Tensor Factorization for Knowledge Graph Completion", "Embedding Entities and Relations for Learning and Inference in Knowledge Bases", "Query2box: Reasoning over Knowledge Graphs in Vector Space using Box Embeddings", "BoxE: A Box Embedding Model for Knowledge Base Completion", "Modeling Fine-Grained Entity Types with Box Embeddings"], "answer_arxiv_id": ["1902.10197", "1707.01476", "1606.06357", "1510.04935", "1711.04071", "1901.09590", "1412.6575", "2002.05969", "2007.06267v2", "2101.00345"], "source_meta": {"published_time": "20230522"}, "qid": "AutoScholarQuery_test_451"}
{"question": "What are some works related to the Mean Teacher paradigm?", "answer": ["Self-supervised Augmentation Consistency for Adapting Semantic\n  Segmentation", "DAFormer: Improving Network Architectures and Training Strategies for\n  Domain-Adaptive Semantic Segmentation", "Prototypical Pseudo Label Denoising and Target Structure Learning for\n  Domain Adaptive Semantic Segmentation", "End-to-End Semi-Supervised Object Detection with Soft Teacher", "Active Teacher for Semi-Supervised Object Detection", "Consistent-Teacher: Towards Reducing Inconsistent Pseudo-targets in Semi-supervised Object Detection", "Omni-DETR: Omni-Supervised Object Detection with Transformers", "ALWOD: Active Learning for Weakly-Supervised Object Detection", "Contrastive Mean Teacher for Domain Adaptive Object Detectors", "Cross-Domain Adaptive Teacher for Object Detection", "Mutual Mean-Teaching: Pseudo Label Refinery for Unsupervised Domain\n  Adaptation on Person Re-identification", "Exploiting Sample Uncertainty for Domain Adaptive Person\n  Re-Identification", "Delving into Probabilistic Uncertainty for Unsupervised Domain Adaptive\n  Person Re-Identification"], "answer_arxiv_id": ["2105.00097", "2111.14887", "2101.10979", "2106.09018", "2303.08348", "2209.01589v3", "2203.16089", "2309.07914", "2305.03034", "2111.13216", "2001.01526", "2012.08733", "2112.14025"], "source_meta": {"published_time": "20240322"}, "qid": "AutoScholarQuery_test_452"}
{"question": "Could you provide me studies where they utilized self-supervision and few-shot prompting to address the scalability of large language model tool-use?", "answer": ["Toolformer: Language Models Can Teach Themselves to Use Tools", "TALM: Tool Augmented Language Models"], "answer_arxiv_id": ["2302.04761", "2205.12255"], "source_meta": {"published_time": "20240223"}, "qid": "AutoScholarQuery_test_453"}
{"question": "What papers depict studies of sample complexity of posterior sampling in tabular settings?", "answer": ["Generalization and Exploration via Randomized Value Functions", "Deep Exploration via Randomized Value Functions", "Worst-Case Regret Bounds for Exploration via Randomized Value Functions"], "answer_arxiv_id": ["1402.0635", "1703.07608v5", "1906.02870"], "source_meta": {"published_time": "20231029"}, "qid": "AutoScholarQuery_test_454"}
{"question": "Can you provide the studies that use additional guiding signals to incorporate explicit control in conditional diffusion models?", "answer": ["SpaText: Spatio-Textual Representation for Controllable Image Generation", "High-Resolution Image Synthesis with Latent Diffusion Models", "InstructPix2Pix: Learning to Follow Image Editing Instructions"], "answer_arxiv_id": ["2211.14305", "2112.10752", "2211.09800"], "source_meta": {"published_time": "20230216"}, "qid": "AutoScholarQuery_test_455"}
{"question": "What research papers discuss memory-based methods as a knowledge editing paradigm?", "answer": ["Memory-Based Model Editing at Scale", "Transformer-Patcher: One Mistake worth One Neuron", "Calibrating Factual Knowledge in Pretrained Language Models", "Can We Edit Factual Knowledge by In-Context Learning?"], "answer_arxiv_id": ["2206.06520", "2301.09785", "2210.03329", "2305.12740"], "source_meta": {"published_time": "20230916"}, "qid": "AutoScholarQuery_test_456"}
{"question": "Any studies about generating adversarial examples in textual domains?", "answer": ["Adversarial Examples for Evaluating Reading Comprehension Systems", "Generating Natural Language Adversarial Examples", "Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA\n  Models", "HotFlip: White-Box Adversarial Examples for Text Classification", "Universal Adversarial Triggers for Attacking and Analyzing NLP"], "answer_arxiv_id": ["1707.07328", "1804.07998", "2106.00245", "1712.06751", "1908.07125"], "source_meta": {"published_time": "20231206"}, "qid": "AutoScholarQuery_test_457"}
{"question": "What work highlights computational challenges and memory issues in bi-level optimization-based methods?", "answer": ["Scaling Up Dataset Distillation to ImageNet-1K with Constant Memory"], "answer_arxiv_id": ["2211.10586"], "source_meta": {"published_time": "20231206"}, "qid": "AutoScholarQuery_test_458"}
{"question": "Which works describe FL strategies for handling communication burden issues?", "answer": ["FedPAQ: A Communication-Efficient Federated Learning Method with Periodic Averaging and Quantization"], "answer_arxiv_id": ["1909.13014v4"], "source_meta": {"published_time": "20231101"}, "qid": "AutoScholarQuery_test_459"}
{"question": "What papers discussed handcrafted prompts for specific tasks?", "answer": ["AutoPrompt: Eliciting Knowledge from Language Models with Automatically\n  Generated Prompts"], "answer_arxiv_id": ["2010.15980"], "source_meta": {"published_time": "20231219"}, "qid": "AutoScholarQuery_test_460"}
{"question": "What are some works that have advanced image editing methods achieving more accurate attribute modification textually?", "answer": ["Prompt-to-Prompt Image Editing with Cross Attention Control", "InstructPix2Pix: Learning to Follow Image Editing Instructions", "Null-text Inversion for Editing Real Images using Guided Diffusion\n  Models", "Negative-prompt Inversion: Fast Image Inversion for Editing with\n  Text-guided Diffusion Models"], "answer_arxiv_id": ["2208.01626", "2211.09800", "2211.09794", "2305.16807"], "source_meta": {"published_time": "20240329"}, "qid": "AutoScholarQuery_test_461"}
{"question": "Which research papers studied methods based on model mixture for Full Model Personalization?", "answer": ["Federated Learning of a Mixture of Global and Local Models", "Adaptive Personalized Federated Learning", "Three Approaches for Personalization with Applications to Federated\n  Learning"], "answer_arxiv_id": ["2002.05516", "2003.13461", "2002.10619"], "source_meta": {"published_time": "20230213"}, "qid": "AutoScholarQuery_test_462"}
{"question": "Can you mention studies that are trained on massive pairs of images and captions, enabling them to generate detailed image content conditioned on textual instruction?", "answer": ["Zero-Shot Text-to-Image Generation", "Hierarchical Text-Conditional Image Generation with CLIP Latents", "High-Resolution Image Synthesis with Latent Diffusion Models"], "answer_arxiv_id": ["2102.12092", "2204.06125", "2112.10752"], "source_meta": {"published_time": "20231214"}, "qid": "AutoScholarQuery_test_463"}
{"question": "What studies further propagated the token-level edit operation approach proposed by LaserTagger?", "answer": ["Parallel Iterative Edit Models for Local Sequence Transduction", "GECToR -- Grammatical Error Correction: Tag, Not Rewrite"], "answer_arxiv_id": ["1910.02893", "2005.12592"], "source_meta": {"published_time": "20240528"}, "qid": "AutoScholarQuery_test_464"}
{"question": "Which paper first introduced the concept of fully unsupervised anomaly detection?", "answer": ["Generative Cooperative Learning for Unsupervised Video Anomaly Detection"], "answer_arxiv_id": ["2203.03962"], "source_meta": {"published_time": "20240401"}, "qid": "AutoScholarQuery_test_465"}
{"question": "Which study introduced a metric based on the sample influence score of the optimal empirical risk in CoreSet selection?", "answer": ["Data Pruning via Moving-one-Sample-out"], "answer_arxiv_id": ["2310.14664"], "source_meta": {"published_time": "20231206"}, "qid": "AutoScholarQuery_test_466"}
{"question": "What papers study hallucination issues in MLLMs?", "answer": ["Evaluation and Analysis of Hallucination in Large Vision-Language Models"], "answer_arxiv_id": ["2308.15126"], "source_meta": {"published_time": "20240119"}, "qid": "AutoScholarQuery_test_467"}
{"question": "What works learn disentangled representations from time-series data?", "answer": ["Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA"], "answer_arxiv_id": ["1605.06336"], "source_meta": {"published_time": "20230523"}, "qid": "AutoScholarQuery_test_468"}
{"question": "Could you provide some examples of diffusion models that involve different number of denoising steps and parameterization of transformation?", "answer": ["Deep Unsupervised Learning using Nonequilibrium Thermodynamics", "Denoising Diffusion Probabilistic Models", "Denoising Diffusion Implicit Models", "DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps", "DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models", "Pseudo Numerical Methods for Diffusion Models on Manifolds"], "answer_arxiv_id": ["1503.03585", "2006.11239", "2010.02502", "2206.00927", "2211.01095", "2202.09778"], "source_meta": {"published_time": "20230216"}, "qid": "AutoScholarQuery_test_469"}
{"question": "Which works use consistency between model generated content and external information for factuality detection in LLM?", "answer": ["Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic\n  Fact-checkers", "RARR: Researching and Revising What Language Models Say, Using Language\n  Models", "FacTool: Factuality Detection in Generative AI -- A Tool Augmented\n  Framework for Multi-Task and Multi-Domain Scenarios"], "answer_arxiv_id": ["2311.09000", "2210.08726", "2307.13528"], "source_meta": {"published_time": "20240410"}, "qid": "AutoScholarQuery_test_470"}
{"question": "Which papers discussed the use of gradient guidance to increase the occurrence of a desired attribute in discrete generative models?", "answer": ["Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space"], "answer_arxiv_id": ["1612.00005"], "source_meta": {"published_time": "20230531"}, "qid": "AutoScholarQuery_test_471"}
{"question": "Which works experimentally proved that adding Gaussian noise during training increases adversarial robustness?", "answer": ["Robustness of classifiers: from adversarial to random noise", "Adversarial Examples Are a Natural Consequence of Test Error in Noise"], "answer_arxiv_id": ["1608.08967", "1901.10513"], "source_meta": {"published_time": "20230724"}, "qid": "AutoScholarQuery_test_472"}
{"question": "Could you name some studies that have explored the influence of asking LLMs to respond as a particular person?", "answer": ["Meet Your Favorite Character: Open-domain Chatbot Mimicking Fictional Characters with only a Few Utterances", "CTRL: A Conditional Transformer Language Model for Controllable Generation"], "answer_arxiv_id": ["2204.10825", "1909.05858"], "source_meta": {"published_time": "20230524"}, "qid": "AutoScholarQuery_test_473"}
{"question": "What papers provide a discussion or implementation of the method of 'prompting'?", "answer": ["Learning to Prompt for Vision-Language Models", "Conditional Prompt Learning for Vision-Language Models", "Exploring Visual Prompts for Adapting Large-Scale Models", "Task Bias in Vision-Language Models"], "answer_arxiv_id": ["2109.01134", "2203.05557", "2203.17274", "2212.04412"], "source_meta": {"published_time": "20230523"}, "qid": "AutoScholarQuery_test_474"}
{"question": "Are there any studies that have applied large-scale pre-trained models to the UCDR task?", "answer": ["Episodic Training for Domain Generalization", "Semantic Data Augmentation based Distance Metric Learning for Domain\n  Generalization", "Universal Cross-Domain Retrieval: Generalizing Across Classes and\n  Domains"], "answer_arxiv_id": ["1902.00113", "2208.02803", "2108.08356"], "source_meta": {"published_time": "20231219"}, "qid": "AutoScholarQuery_test_475"}
{"question": "What work is similar to the researcher's use of context distillation in knowledge editing?", "answer": ["A General Language Assistant as a Laboratory for Alignment"], "answer_arxiv_id": ["2112.00861"], "source_meta": {"published_time": "20230615"}, "qid": "AutoScholarQuery_test_476"}
{"question": "Which studies focused on vision-based GUI navigation using GPT-4V?", "answer": ["GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone\n  GUI Navigation", "ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation", "AppAgent: Multimodal Agents as Smartphone Users"], "answer_arxiv_id": ["2311.07562", "2312.13108", "2312.13771"], "source_meta": {"published_time": "20240117"}, "qid": "AutoScholarQuery_test_477"}
{"question": "Could you provide me the works that introduced policy-based methods for performing adaptive experimentation?", "answer": ["Implicit Deep Adaptive Design: Policy–Based Experimental Design without Likelihoods", "Deep Adaptive Design: Amortizing Sequential Bayesian Experimental Design"], "answer_arxiv_id": ["2111.02329", "2103.02438"], "source_meta": {"published_time": "20230221"}, "qid": "AutoScholarQuery_test_478"}
{"question": "Which works focus on developing more effective pretraining losses beyond the autoregressive or masked language modeling objectives?", "answer": ["XLNet: Generalized Autoregressive Pretraining for Language Understanding", "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training"], "answer_arxiv_id": ["1906.08237", "2003.10555", "2002.12804"], "source_meta": {"published_time": "20230219"}, "qid": "AutoScholarQuery_test_479"}
{"question": "Are there any works on multilingual ARA models?", "answer": ["Automatic Readability Assessment for Closely Related Languages"], "answer_arxiv_id": ["2305.13478"], "source_meta": {"published_time": "20240603"}, "qid": "AutoScholarQuery_test_480"}
{"question": "Are there studies adapting popular uncertainty tools in Bayesian Uncertainty Estimation literature to generative LLMs?", "answer": ["Uncertainty Estimation in Autoregressive Structured Prediction"], "answer_arxiv_id": ["2002.07650"], "source_meta": {"published_time": "20240219"}, "qid": "AutoScholarQuery_test_481"}
{"question": "Could you provide me some works about knowledge retrieval from prompts?", "answer": ["Label Words are Anchors: An Information Flow Perspective for\n  Understanding In-Context Learning"], "answer_arxiv_id": ["2305.14160"], "source_meta": {"published_time": "20240228"}, "qid": "AutoScholarQuery_test_482"}
{"question": "What studies have been conducted on multilingual Large Language Models (LLMs) with a focus on hundreds of languages?", "answer": ["mT5: A massively multilingual pre-trained text-to-text transformer", "GPT-4 Technical Report", "Gemini: A Family of Highly Capable Multimodal Models"], "answer_arxiv_id": ["2010.11934", "2303.08774", "2312.11805v4"], "source_meta": {"published_time": "20240425"}, "qid": "AutoScholarQuery_test_483"}
{"question": "Could you provide me some works about visual pretraining that emphasized contrastive learning?", "answer": ["A Simple Framework for Contrastive Learning of Visual Representations", "Momentum Contrast for Unsupervised Visual Representation Learning", "Emerging Properties in Self-Supervised Vision Transformers"], "answer_arxiv_id": ["2002.05709", "1911.05722", "2104.14294"], "source_meta": {"published_time": "20240125"}, "qid": "AutoScholarQuery_test_484"}
{"question": "What studies have focused on architectures that exhibit combinatorial generalization such as transformers, graph neural networks, and bilinear models?", "answer": ["Attention Is All You Need", "Combinatorial Optimization and Reasoning with Graph Neural Networks"], "answer_arxiv_id": ["1706.03762", "2102.09544"], "source_meta": {"published_time": "20230427"}, "qid": "AutoScholarQuery_test_485"}
{"question": "What paper proposed the averaged stochastic approximation (NASA) to obtain a better rate for non-convex objectives?", "answer": ["A Single Time-Scale Stochastic Approximation Method for Nested Stochastic Optimization"], "answer_arxiv_id": ["1812.01094"], "source_meta": {"published_time": "20230613"}, "qid": "AutoScholarQuery_test_486"}
{"question": "Which papers concentrate on decomposing complex questions into sub-questions?", "answer": ["Least-to-Most Prompting Enables Complex Reasoning in Large Language\n  Models", "Successive Prompting for Decomposing Complex Questions"], "answer_arxiv_id": ["2205.10625", "2212.04092"], "source_meta": {"published_time": "20240628"}, "qid": "AutoScholarQuery_test_487"}
{"question": "Which paper proposed multi-channel equivariant graph networks?", "answer": ["Conditional Antibody Design as 3D Equivariant Graph Translation"], "answer_arxiv_id": ["2208.06073"], "source_meta": {"published_time": "20230201"}, "qid": "AutoScholarQuery_test_488"}
{"question": "Can you name the papers where VeRA, the method that reduces the number of parameters with the help of random projections, is mentioned?", "answer": ["VeRA: Vector-based Random Matrix Adaptation"], "answer_arxiv_id": ["2310.11454"], "source_meta": {"published_time": "20240224"}, "qid": "AutoScholarQuery_test_489"}
{"question": "Any studies about prompt fine-tuning for vision models to adapt image models from one image task to another?", "answer": ["Visual Prompt Tuning", "Adversarial Reprogramming of Neural Networks"], "answer_arxiv_id": ["2203.12119", "1806.11146"], "source_meta": {"published_time": "20230223"}, "qid": "AutoScholarQuery_test_490"}
{"question": "Can you mention some sequence models proposing the use of multiple time scales, akin to the multi-rate mechanism in this study?", "answer": ["UnICORNN: A recurrent model for learning very long time dependencies", "Long Expressive Memory for Sequence Modeling"], "answer_arxiv_id": ["2103.05487", "2110.04744"], "source_meta": {"published_time": "20221002"}, "qid": "AutoScholarQuery_test_491"}
{"question": "What works propose strategies for face capture that are more easily accessible and convenient for daily users?", "answer": ["AvatarMe: Realistically Renderable 3D Facial Reconstruction\n  \"in-the-wild\"", "Relightify: Relightable 3D Faces from a Single Image via Diffusion\n  Models", "Learning a 3D Morphable Face Reflectance Model from Low-cost Data", "A Morphable Face Albedo Model", "Learning Formation of Physically-Based Face Attributes", "FitMe: Deep Photorealistic 3D Morphable Model Avatars", "Practical Face Reconstruction via Differentiable Ray Tracing"], "answer_arxiv_id": ["2003.13845", "2305.06077", "2303.11686", "2004.02711", "2004.03458", "2305.09641", "2101.05356"], "source_meta": {"published_time": "20231206"}, "qid": "AutoScholarQuery_test_492"}
{"question": "Which early works employed ConvNets and inverse perspective mapping (IPM) for mapping features from perspective view to BEV view?", "answer": ["Multi-View 3D Object Detection Network for Autonomous Driving", "Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by\n  Implicitly Unprojecting to 3D", "Cross-view Semantic Segmentation for Sensing Surroundings"], "answer_arxiv_id": ["1611.07759", "2008.05711", "1906.03560"], "source_meta": {"published_time": "20240313"}, "qid": "AutoScholarQuery_test_493"}
{"question": "What researches made use of auxiliary data such as human-annotated attribute information, text description or knowledge graph in Zero-Shot Learning (ZSL)?", "answer": ["Learning Deep Representations of Fine-grained Visual Descriptions", "Multi-Label Zero-Shot Learning with Structured Knowledge Graphs"], "answer_arxiv_id": ["1605.05395", "1711.06526"], "source_meta": {"published_time": "20231219"}, "qid": "AutoScholarQuery_test_494"}
{"question": "What research papers prioritize salient weights for PTQ in LLMs?", "answer": ["SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight\n  Compression", "AWQ: Activation-aware Weight Quantization for LLM Compression and\n  Acceleration", "SqueezeLLM: Dense-and-Sparse Quantization"], "answer_arxiv_id": ["2306.03078", "2306.00978", "2306.07629"], "source_meta": {"published_time": "20240216"}, "qid": "AutoScholarQuery_test_495"}
{"question": "Any studies showing that LLM hidden states can effectively represent a task defined by input-output pairs?", "answer": ["Function Vectors in Large Language Models"], "answer_arxiv_id": ["2310.15213"], "source_meta": {"published_time": "20240215"}, "qid": "AutoScholarQuery_test_496"}
{"question": "What studies gave rise to prompt-based learning in LLMs?", "answer": ["BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "Language Models are Few-Shot Learners", "GPT-4 Technical Report", "OPT: Open Pre-trained Transformer Language Models"], "answer_arxiv_id": ["1810.04805", "2005.14165", "2303.08774", "2205.01068"], "source_meta": {"published_time": "20230524"}, "qid": "AutoScholarQuery_test_497"}
{"question": "What studies apply slot attention for object discovery?", "answer": ["Object-Centric Learning with Slot Attention", "GENESIS: Generative Scene Inference and Sampling with Object-Centric\n  Latent Representations", "Conditional Object-Centric Learning from Video", "Shepherding Slots to Objects: Towards Stable and Robust Object-Centric\n  Learning"], "answer_arxiv_id": ["2006.15055", "1907.13052", "2111.12594", "2303.17842"], "source_meta": {"published_time": "20240228"}, "qid": "AutoScholarQuery_test_498"}
{"question": "What works proposed method for experimental design for causal discovery in a non-BOED setting in the presence of cycles?", "answer": ["A Unified Experiment Design Approach for Cyclic and Acyclic Causal Models"], "answer_arxiv_id": ["2205.10083"], "source_meta": {"published_time": "20230221"}, "qid": "AutoScholarQuery_test_499"}
{"question": "What study employs a contextual word retrieval task where the model is tasked with finding corresponding words and sentences across parallel corpora?", "answer": ["Multilingual Alignment of Contextual Word Representations"], "answer_arxiv_id": ["2002.03518"], "source_meta": {"published_time": "20240523"}, "qid": "AutoScholarQuery_test_500"}
{"question": "Which work leverages graph generation as the training objective in generative self-supervised learning?", "answer": ["GPT-GNN: Generative Pre-Training of Graph Neural Networks"], "answer_arxiv_id": ["2006.15437"], "source_meta": {"published_time": "20230622"}, "qid": "AutoScholarQuery_test_501"}
{"question": "Could you provide me some studies that propose non-parametric approaches to post-hoc calibration methods?", "answer": ["Mix-n-Match: Ensemble and Compositional Methods for Uncertainty Calibration in Deep Learning", "Non-Parametric Calibration for Classification", "Dirichlet-based Gaussian Processes for Large-scale Calibrated Classification"], "answer_arxiv_id": ["2003.07329", "1906.04933v3", "1805.10915"], "source_meta": {"published_time": "20230210"}, "qid": "AutoScholarQuery_test_502"}
{"question": "Could you provide me some studies about object hallucination in MLLMs?", "answer": ["Evaluating Object Hallucination in Large Vision-Language Models"], "answer_arxiv_id": ["2305.10355"], "source_meta": {"published_time": "20240119"}, "qid": "AutoScholarQuery_test_503"}
{"question": "Which studies proposed methods for learning 3D-aware image and geometry generation with implicit neural radiance fields as generators?", "answer": ["GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis", "pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware\n  Image Synthesis"], "answer_arxiv_id": ["2007.02442", "2012.00926"], "source_meta": {"published_time": "20231203"}, "qid": "AutoScholarQuery_test_504"}
{"question": "Could you provide me some studies on global stability that contributed to the Probably Eventually Correct (PEC) learning model?", "answer": ["An Equivalence Between Private Classification and Online Prediction", "Sample-efficient proper PAC learning with approximate differential privacy"], "answer_arxiv_id": ["2003.00563", "2012.03893"], "source_meta": {"published_time": "20230523"}, "qid": "AutoScholarQuery_test_505"}
{"question": "Which studies have aimed at generalization to underrepresented languages in LLM-based evaluation methodologies?", "answer": ["Are Large Language Model-based Evaluators the Solution to Scaling Up\n  Multilingual Evaluation?"], "answer_arxiv_id": ["2309.07462"], "source_meta": {"published_time": "20240224"}, "qid": "AutoScholarQuery_test_506"}
{"question": "What studies have aimed to capture functional requirements in software development by generating code-like outlines using in-context learning?", "answer": ["Self-planning Code Generation with Large Language Models", "Think Outside the Code: Brainstorming Boosts Large Language Models in\n  Code Generation"], "answer_arxiv_id": ["2303.06689v3", "2305.10679"], "source_meta": {"published_time": "20240802"}, "qid": "AutoScholarQuery_test_507"}
{"question": "Could you provide me some references that evaluate the ability of LMs to reason about emerging entities?", "answer": ["Mind the Gap: Assessing Temporal Generalization in Neural Language Models", "Time-Aware Language Models as Temporal Knowledge Bases", "RealTime QA: What’s the Answer Right Now?"], "answer_arxiv_id": ["2102.01951", "2106.15110", "2207.13332"], "source_meta": {"published_time": "20230615"}, "qid": "AutoScholarQuery_test_508"}
{"question": "What papers are about utilizing machine-learned predictions for designing efficient offline algorithms?", "answer": ["Faster Matchings via Learned Duals", "Faster Fundamental Graph Algorithms via Learned Predictions", "Discrete-Convex-Analysis-Based Framework for Warm-Starting Algorithms with Predictions", "Learning-Augmented Maximum Flow"], "answer_arxiv_id": ["2107.09770", "2204.12055", "2205.09961", "2207.12911"], "source_meta": {"published_time": "20221006"}, "qid": "AutoScholarQuery_test_509"}
{"question": "Could you provide me a work that introduced EL2N score as a measure of importance in CoreSet selection?", "answer": ["Deep Learning on a Data Diet: Finding Important Examples Early in\n  Training"], "answer_arxiv_id": ["2107.07075"], "source_meta": {"published_time": "20231206"}, "qid": "AutoScholarQuery_test_510"}
{"question": "Which works proposed reinforcement learning methods to tackle challenges of behavior cloning in autonomous driving?", "answer": ["Learning to Drive in a Day", "Navigating Occluded Intersections with Autonomous Vehicles using Deep Reinforcement Learning", "A Reinforcement Learning Based Approach for Automated Lane Change Maneuvers", "Imitation Is Not Enough: Robustifying Imitation with Reinforcement Learning for Challenging Driving Scenarios"], "answer_arxiv_id": ["1807.00412", "1705.01196", "1804.07871v1", "2212.11419"], "source_meta": {"published_time": "20231012"}, "qid": "AutoScholarQuery_test_511"}
{"question": "Which paper involves the use of the Faster-RCNN model in Vision-Language Pre-training?", "answer": ["Faster R-CNN: Towards Real-Time Object Detection with Region Proposal\n  Networks"], "answer_arxiv_id": ["1506.01497"], "source_meta": {"published_time": "20231219"}, "qid": "AutoScholarQuery_test_512"}
{"question": "What works examined training agents through reinforcement learning on the MiniWob web environment?", "answer": ["Reinforcement Learning on Web Interfaces Using Workflow-Guided\n  Exploration", "Learning to Navigate the Web"], "answer_arxiv_id": ["1802.08802", "1812.09195"], "source_meta": {"published_time": "20240117"}, "qid": "AutoScholarQuery_test_513"}
{"question": "What works proposed hierarchical BERT models designed for extractive summarization?", "answer": ["HIBERT: Document Level Pre-training of Hierarchical Bidirectional\n  Transformers for Document Summarization", "Unsupervised Extractive Summarization by Pre-training Hierarchical\n  Transformers", "HiStruct+: Improving Extractive Text Summarization with Hierarchical\n  Structure Information"], "answer_arxiv_id": ["1905.06566", "2010.08242", "2203.09629"], "source_meta": {"published_time": "20240227"}, "qid": "AutoScholarQuery_test_514"}
{"question": "Which work related to the researcher's work examines tasks like distilling a persona-conditioned language model?", "answer": ["Prompt Injection: Parameterization of Fixed Inputs"], "answer_arxiv_id": ["2206.11349"], "source_meta": {"published_time": "20230615"}, "qid": "AutoScholarQuery_test_515"}
{"question": "Could you provide me studies focused on the grounding capabilities of LVLMs?", "answer": ["VisionLLM: Large Language Model is also an Open-Ended Decoder for\n  Vision-Centric Tasks", "MiniGPT-v2: large language model as a unified interface for\n  vision-language multi-task learning"], "answer_arxiv_id": ["2305.11175", "2310.09478"], "source_meta": {"published_time": "20240117"}, "qid": "AutoScholarQuery_test_516"}
{"question": "Which work is related to private prediction?", "answer": ["Privacy-preserving Prediction"], "answer_arxiv_id": ["1803.10266"], "source_meta": {"published_time": "20230516"}, "qid": "AutoScholarQuery_test_517"}
{"question": "Which studies incorporated the use of diffusion models in their work?", "answer": ["Any-to-Any Generation via Composable Diffusion", "Emu: Generative Pretraining in Multimodality"], "answer_arxiv_id": ["2305.11846", "2307.05222"], "source_meta": {"published_time": "20231013"}, "qid": "AutoScholarQuery_test_518"}
{"question": "What work proposed a method of retraining classifiers on 're-weighting' data to reduce reliance on spurious features?", "answer": ["Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations"], "answer_arxiv_id": ["2204.02937"], "source_meta": {"published_time": "20230323"}, "qid": "AutoScholarQuery_test_519"}
{"question": "Could you provide me research that allows for controlled text-based scene editing by finetuning an image diffusion model?", "answer": ["DreamEditor: Text-Driven 3D Scene Editing with Neural Fields"], "answer_arxiv_id": ["2306.13455"], "source_meta": {"published_time": "20240103"}, "qid": "AutoScholarQuery_test_520"}
{"question": "Could you provide me some work that applied transformer or its variants into TAD head?", "answer": ["An Empirical Study of End-to-End Temporal Action Detection", "Relaxed Transformer Decoders for Direct Action Proposal Generation", "ActionFormer: Localizing Moments of Actions with Transformers", "ReAct: Temporal Action Detection with Relational Queries"], "answer_arxiv_id": ["2204.02932", "2102.01894", "2202.07925", "2207.07097"], "source_meta": {"published_time": "20231204"}, "qid": "AutoScholarQuery_test_521"}
{"question": "What papers have incorporated the use of NLEs in fields beyond NLP, such as in computer vision, medical field, and self-driving cars?", "answer": ["Grounding Visual Explanations", "From Recognition to Cognition: Visual Commonsense Reasoning", "Knowledge-Grounded Self-Rationalization via Extractive and Natural\n  Language Explanations", "Explaining Chest X-ray Pathologies in Natural Language", "Textual Explanations for Self-Driving Vehicles"], "answer_arxiv_id": ["1807.09685", "1811.10830", "2106.13876", "2207.04343", "1807.11546"], "source_meta": {"published_time": "20231113"}, "qid": "AutoScholarQuery_test_522"}
{"question": "Which research proposes to enhance the reasoning process in LLMs by framing thoughts as graphs?", "answer": ["Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in\n  Language Models"], "answer_arxiv_id": ["2305.16582"], "source_meta": {"published_time": "20240228"}, "qid": "AutoScholarQuery_test_523"}
{"question": "Any studies arguing for centering language models’ evaluation on how models will be used in practice?", "answer": ["Rethinking Model Evaluation as Narrowing the Socio-Technical Gap"], "answer_arxiv_id": ["2306.03100"], "source_meta": {"published_time": "20240613"}, "qid": "AutoScholarQuery_test_524"}
{"question": "Could you name some studies employing distributional semantic models such as count-based or Word2Vec approaches for the LSC task?", "answer": ["Cultural Shift or Linguistic Drift? Comparing Two Computational Measures\n  of Semantic Change"], "answer_arxiv_id": ["1606.02821"], "source_meta": {"published_time": "20240605"}, "qid": "AutoScholarQuery_test_525"}
{"question": "Which works addressed the issue of inadequate data quality in large corpora and extended cleaning efforts?", "answer": ["Towards a Cleaner Document-Oriented Multilingual Crawled Corpus"], "answer_arxiv_id": ["2201.06642"], "source_meta": {"published_time": "20240209"}, "qid": "AutoScholarQuery_test_526"}
{"question": "Can you provide papers that discussed the concept of latent embeddings?", "answer": ["RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space", "Convolutional 2D Knowledge Graph Embeddings", "Complex Embeddings for Simple Link Prediction", "Holographic Embeddings of Knowledge Graphs", "kbgan: Adversarial Learning for Knowledge Graph Embeddings", "TuckER: Tensor Factorization for Knowledge Graph Completion", "Query2box: Reasoning over Knowledge Graphs in Vector Space using Box Embeddings", "BoxE: A Box Embedding Model for Knowledge Base Completion", "Modeling Fine-Grained Entity Types with Box Embeddings"], "answer_arxiv_id": ["1902.10197", "1707.01476", "1606.06357", "1510.04935", "1711.04071", "1901.09590", "2002.05969", "2007.06267v2", "2101.00345"], "source_meta": {"published_time": "20230522"}, "qid": "AutoScholarQuery_test_527"}
{"question": "What kinds of researches have been conducted using reinforcement learning to train policies for adaptive experimental design?", "answer": ["Optimizing Sequential Experimental Design with Deep Reinforcement Learning", "Policy-Based Bayesian Experimental Design for Non-Differentiable Implicit Models"], "answer_arxiv_id": ["2202.00821", "2203.04272v1"], "source_meta": {"published_time": "20230221"}, "qid": "AutoScholarQuery_test_528"}
{"question": "Which research investigations in language finetuning consider data selection as crucial?", "answer": ["Selection via Proxy: Efficient Data Selection for Deep Learning", "Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt"], "answer_arxiv_id": ["1906.11829", "2206.07137"], "source_meta": {"published_time": "20230206"}, "qid": "AutoScholarQuery_test_529"}
{"question": "What works covered methods that factor in agreement between positive pixel pairs for dense predictions?", "answer": ["Propagate Yourself: Exploring Pixel-Level Consistency for Unsupervised Visual Representation Learning", "Self-supervised Learning with Local Contrastive Loss for Detection and Semantic Segmentation"], "answer_arxiv_id": ["2011.10043", "2207.04398"], "source_meta": {"published_time": "20230609"}, "qid": "AutoScholarQuery_test_530"}
{"question": "Could you name some research papers that utilized pre-trained text-to-image diffusion models and SMPL models for avatar generation?", "answer": ["DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models", "AvatarCraft: Transforming Text into Neural Human Avatars with Parameterized Shape and Pose Control"], "answer_arxiv_id": ["2304.00916", "2303.17606"], "source_meta": {"published_time": "20230521"}, "qid": "AutoScholarQuery_test_531"}
{"question": "What research proposed multi-plane images for novel-view synthesis?", "answer": ["Stereo Magnification: Learning View Synthesis using Multiplane Images"], "answer_arxiv_id": ["1805.09817"], "source_meta": {"published_time": "20240421"}, "qid": "AutoScholarQuery_test_532"}
{"question": "What are some studies that have used data statistics, representations, logits, and embedding to avoid exposing privacy in Federated Learning?", "answer": ["XOR Mixup: Privacy-Preserving Data Augmentation for One-Shot Federated Learning", "Towards Fair Federated Learning with Zero-Shot Data Augmentation", "Cronus: Robust and Heterogeneous Collaborative Learning with Black-Box Knowledge Transfer", "No Fear of Heterogeneity: Classifier Calibration for Federated Learning with Non-IID Data", "FedProto: Federated Prototype Learning across Heterogeneous Clients"], "answer_arxiv_id": ["2006.05148", "2104.13417", "1912.11279v1", "2106.05001", "2105.00243"], "source_meta": {"published_time": "20231008"}, "qid": "AutoScholarQuery_test_533"}
{"question": "What are the key works in the field of diffusion models which are a class of generative probabilistic models?", "answer": ["Deep Unsupervised Learning using Nonequilibrium Thermodynamics", "Diffusion Models in Vision: A Survey", "Diffusion Models Beat GANs on Image Synthesis", "Denoising Diffusion Probabilistic Models", "Improved Denoising Diffusion Probabilistic Models"], "answer_arxiv_id": ["1503.03585", "2209.04747", "2105.05233", "2006.11239", "2102.09672"], "source_meta": {"published_time": "20230216"}, "qid": "AutoScholarQuery_test_534"}
{"question": "Are there any works that reported that the volume of input knowledge for each query in ICL is constrained by the maximum input length of PLMs?", "answer": ["Structured Prompting: Scaling In-Context Learning to 1,000 Examples"], "answer_arxiv_id": ["2212.06713"], "source_meta": {"published_time": "20240627"}, "qid": "AutoScholarQuery_test_535"}
{"question": "Can you name works that have used specific methodology like Android’s View Hierarchy, Regions of Interest, or screenshots for representing interfaces?", "answer": ["Spotlight: Mobile UI Understanding using Vision-Language Models with a\n  Focus", "Reinforced UI Instruction Grounding: Towards a Generic UI Task\n  Automation API"], "answer_arxiv_id": ["2209.14927", "2310.04716"], "source_meta": {"published_time": "20240117"}, "qid": "AutoScholarQuery_test_536"}
{"question": "Which papers demonstrate that language models can map conceptual domains onto grounded world representations?", "answer": ["Language Models are Few-Shot Learners"], "answer_arxiv_id": ["2005.14165"], "source_meta": {"published_time": "20221024"}, "qid": "AutoScholarQuery_test_537"}
{"question": "What are the task-informed models such as XL-LEXEME based on?", "answer": ["Interpretable Word Sense Representations via Definition Generation: The\n  Case of Semantic Change Analysis"], "answer_arxiv_id": ["2305.11993"], "source_meta": {"published_time": "20240605"}, "qid": "AutoScholarQuery_test_538"}
{"question": "Any studies that employ the Masked Language Model to obtain corrections in GEC tasks?", "answer": ["Felix: Flexible Text Editing Through Tagging and Insertion", "GEC-DePenD: Non-Autoregressive Grammatical Error Correction with Decoupled Permutation and Decoding"], "answer_arxiv_id": ["2003.10687", "2311.08191v1"], "source_meta": {"published_time": "20240528"}, "qid": "AutoScholarQuery_test_539"}
{"question": "Could you provide me the works on finding architectures with high accuracy on clean examples not considering their robustness?", "answer": ["AdvRush: Searching for Adversarially Robust Neural Architectures", "Neural Architecture Design and Robustness: A Dataset"], "answer_arxiv_id": ["2108.01289", "2306.06712"], "source_meta": {"published_time": "20230608"}, "qid": "AutoScholarQuery_test_540"}
{"question": "What does paper `bib.bib31` propose?", "answer": ["A Generalist Agent"], "answer_arxiv_id": ["2205.06175"], "source_meta": {"published_time": "20230125"}, "qid": "AutoScholarQuery_test_541"}
{"question": "Who looked at prompting closed-source LLMs to leverage their reasoning and planning abilities for web tasks through in-context learning and self-refine?", "answer": ["Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer\n  Control", "Language Models can Solve Computer Tasks"], "answer_arxiv_id": ["2306.07863", "2303.17491"], "source_meta": {"published_time": "20240117"}, "qid": "AutoScholarQuery_test_542"}
{"question": "Which research papers introduced initial vision-language pre-training models?", "answer": ["UNITER: UNiversal Image-TExt Representation Learning", "Unified Vision-Language Pre-Training for Image Captioning and VQA", "Unifying Vision-and-Language Tasks via Text Generation", "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks", "ViLT: Vision-and-Language Transformer Without Convolution or Region\n  Supervision", "Large-Scale Adversarial Training for Vision-and-Language Representation\n  Learning", "Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal\n  Transformers", "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for\n  Vision-and-Language Tasks", "VL-BERT: Pre-training of Generic Visual-Linguistic Representations", "12-in-1: Multi-Task Vision and Language Representation Learning", "Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language\n  Representation Learning", "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"], "answer_arxiv_id": ["1909.11740", "1909.11059", "2102.02779", "2004.06165", "2102.03334", "2006.06195", "2004.00849", "1908.02265", "1908.08530", "1912.02315", "2104.03135", "2108.10904"], "source_meta": {"published_time": "20231211"}, "qid": "AutoScholarQuery_test_543"}
{"question": "What studies have explored incorporating additional conditioning, to generate images with precise control?", "answer": ["Adding Conditional Control to Text-to-Image Diffusion Models", "Composer: Creative and Controllable Image Synthesis with Composable Conditions", "HumanSD: A Native Skeleton-Guided Diffusion Model for Human Image Generation"], "answer_arxiv_id": ["2302.05543", "2302.09778", "2304.04269"], "source_meta": {"published_time": "20230521"}, "qid": "AutoScholarQuery_test_544"}
{"question": "Which studies primarily improve ACI by setting the learning rate adaptively?", "answer": ["Conformal Inference for Online Prediction with Arbitrary Distribution Shifts", "Adaptive Conformal Predictions for Time Series", "Improved Online Conformal Prediction via Strongly Adaptive Online Learning"], "answer_arxiv_id": ["2208.08401", "2202.07282", "2302.07869"], "source_meta": {"published_time": "20230731"}, "qid": "AutoScholarQuery_test_545"}
{"question": "In what papers do the researchers use the dual potentials to recover the OT map?", "answer": ["Large-Scale Optimal Transport and Mapping Estimation"], "answer_arxiv_id": ["1711.02283"], "source_meta": {"published_time": "20230524"}, "qid": "AutoScholarQuery_test_546"}
{"question": "What paper proposed an unsupervised global disentanglement score called Distortion?", "answer": ["Analyzing the Latent Space of GAN through Local Dimension Estimation"], "answer_arxiv_id": ["2205.13182"], "source_meta": {"published_time": "20221011"}, "qid": "AutoScholarQuery_test_547"}
{"question": "Any works achieve higher accuracy on low-textured regions with the help of Transformer in semi-dense matching methods?", "answer": ["LoFTR: Detector-Free Local Feature Matching with Transformers", "ASpanFormer: Detector-Free Image Matching with Adaptive Span Transformer", "MatchFormer: Interleaving Attention in Transformers for Feature Matching"], "answer_arxiv_id": ["2104.00680", "2208.14201", "2203.09645"], "source_meta": {"published_time": "20230627"}, "qid": "AutoScholarQuery_test_548"}
{"question": "Which work is related to the theoretical foundations and implementation of Proximal Policy Optimization?", "answer": ["Proximal Policy Optimization Algorithms"], "answer_arxiv_id": ["1707.06347v2"], "source_meta": {"published_time": "20230130"}, "qid": "AutoScholarQuery_test_549"}
{"question": "What research papers utilized the embedding-based metrics which make use of PLM embeddings like BERT?", "answer": ["BERT: Pre-training of Deep Bidirectional Transformers for Language\n  Understanding", "BERTScore: Evaluating Text Generation with BERT", "MoverScore: Text Generation Evaluating with Contextualized Embeddings\n  and Earth Mover Distance"], "answer_arxiv_id": ["1810.04805", "1904.09675", "1909.02622"], "source_meta": {"published_time": "20240224"}, "qid": "AutoScholarQuery_test_550"}
{"question": "What etudies are on Transformer-based models for speech that have been used to test their brain alignment for speech-evoked brain activity?", "answer": ["Vector-Quantized Autoregressive Predictive Coding", "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech\n  Representations", "HuBERT: Self-Supervised Speech Representation Learning by Masked\n  Prediction of Hidden Units", "Toward a realistic model of speech processing in the brain with\n  self-supervised learning", "Self-supervised models of audio effectively explain human cortical\n  responses to speech"], "answer_arxiv_id": ["2005.08392", "2006.11477", "2106.07447", "2206.01685", "2205.14252"], "source_meta": {"published_time": "20231108"}, "qid": "AutoScholarQuery_test_551"}
{"question": "Could you name some research papers on exploratory attacks to achieve different attack goals such as link re-identification, property inference, membership inference, and model stealing?", "answer": ["Stealing Links from Graph Neural Networks", "Inference Attacks Against Graph Neural Networks", "Model Extraction Attacks on Graph Neural Networks: Taxonomy and Realization", "Quantifying Privacy Leakage in Graph Embedding"], "answer_arxiv_id": ["2005.02131", "2110.02631", "2010.12751v2", "2010.00906"], "source_meta": {"published_time": "20230613"}, "qid": "AutoScholarQuery_test_552"}
{"question": "What studies introduce methods to reduce the learning time by reducing the communication overheads of Federated Learning?", "answer": ["Adaptive Gradient Sparsification for Efficient Federated Learning: An Online Learning Approach"], "answer_arxiv_id": ["2001.04756"], "source_meta": {"published_time": "20230705"}, "qid": "AutoScholarQuery_test_553"}
{"question": "What works represent the approach of contrastive learning for image-text pre-training?", "answer": ["Learning Transferable Visual Models From Natural Language Supervision", "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision", "LiT: Zero-Shot Transfer with Locked-image text Tuning", "Florence: A New Foundation Model for Computer Vision"], "answer_arxiv_id": ["2103.00020", "2102.05918", "2111.07991", "2111.11432"], "source_meta": {"published_time": "20220914"}, "qid": "AutoScholarQuery_test_554"}
{"question": "Are there any methods using hierarchical Reinforcement Learning to decompose complex tasks into sub-tasks?", "answer": ["Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation", "The Option-Critic Architecture", "Near-Optimal Representation Learning for Hierarchical Reinforcement Learning", "Language as an Abstraction for Hierarchical Deep Reinforcement Learning", "Unsupervised Skill Discovery with Bottleneck Option Learning", "Toward Robust Long Range Policy Transfer"], "answer_arxiv_id": ["1604.06057", "1609.05140", "1810.01257", "1906.07343", "2106.14305", "2103.02957"], "source_meta": {"published_time": "20221007"}, "qid": "AutoScholarQuery_test_555"}
{"question": "Can you give examples of research that aligns the source and target point clouds with an orientation estimation module before using a teacher-student model and a DGCNN backbone to find the correspondence?", "answer": ["SE-ORNet: Self-Ensembling Orientation-aware Network for Unsupervised\n  Point Cloud Shape Correspondence"], "answer_arxiv_id": ["2304.05395"], "source_meta": {"published_time": "20231128"}, "qid": "AutoScholarQuery_test_556"}
{"question": "What works discuss defense mechanisms that generate adversarially perturbed images to improve neural network's robustness?", "answer": ["Explaining and Harnessing Adversarial Examples", "Towards Deep Learning Models Resistant to Adversarial Attacks", "DeepFool: a simple and accurate method to fool deep neural networks"], "answer_arxiv_id": ["1412.6572", "1706.06083", "1511.04599"], "source_meta": {"published_time": "20230608"}, "qid": "AutoScholarQuery_test_557"}
{"question": "In what studies LMMs directly reason over embedded visual features?", "answer": ["Visual Instruction Tuning", "Improved Baselines with Visual Instruction Tuning", "Flamingo: a Visual Language Model for Few-Shot Learning", "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image\n  Encoders and Large Language Models", "InstructBLIP: Towards General-purpose Vision-Language Models with\n  Instruction Tuning", "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large\n  Language Models", "mPLUG-Owl: Modularization Empowers Large Language Models with\n  Multimodality", "mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with\n  Modality Collaboration", "MultiModal-GPT: A Vision and Language Model for Dialogue with Humans", "PaLM-E: An Embodied Multimodal Language Model"], "answer_arxiv_id": ["2304.08485", "2310.03744", "2204.14198", "2301.12597", "2305.06500", "2304.10592", "2304.14178", "2311.04257", "2305.04790", "2303.03378"], "source_meta": {"published_time": "20231127"}, "qid": "AutoScholarQuery_test_558"}
{"question": "Could you give me some references about adversarial learning approach in domain adaptation?", "answer": ["Domain-Adversarial Training of Neural Networks", "CyCADA: Cycle-Consistent Adversarial Domain Adaptation", "Progressive Domain Adaptation for Object Detection"], "answer_arxiv_id": ["1505.07818", "1711.03213", "1910.11319"], "source_meta": {"published_time": "20240306"}, "qid": "AutoScholarQuery_test_559"}
{"question": "What works proposed methods to improve computational efficiency by reducing the number of times the underlying field needs to be evaluated in the context of neural scene representations?", "answer": ["Neural Sparse Voxel Fields", "Plenoxels: Radiance Fields without Neural Networks", "PlenOctrees for Real-time Rendering of Neural Radiance Fields", "Instant Neural Graphics Primitives with a Multiresolution Hash Encoding"], "answer_arxiv_id": ["2007.11571", "2112.05131", "2103.14024", "2201.05989"], "source_meta": {"published_time": "20231129"}, "qid": "AutoScholarQuery_test_560"}
{"question": "Which paper uses counterfactual explanations in explainable recommender systems?", "answer": ["Counterfactual Explanations for Neural Recommenders"], "answer_arxiv_id": ["2105.05008"], "source_meta": {"published_time": "20240208"}, "qid": "AutoScholarQuery_test_561"}
{"question": "Can you name the methods that tackle the high-dimensional importance weight estimation problem?", "answer": ["Telescoping Density-Ratio Estimation", "Featurized Density Ratio Estimation", "Density Ratio Estimation via Infinitesimal Classification"], "answer_arxiv_id": ["2006.12204", "2107.02212", "2111.11010v2"], "source_meta": {"published_time": "20230206"}, "qid": "AutoScholarQuery_test_562"}
{"question": "Which works have pioneered in text-editing as a category of generative data augmentation?", "answer": ["EDA: Easy Data Augmentation Techniques for Boosting Performance on Text\n  Classification Tasks"], "answer_arxiv_id": ["1901.11196"], "source_meta": {"published_time": "20240606"}, "qid": "AutoScholarQuery_test_563"}
{"question": "Which papers proposed an algorithm to handle non-stationary MDPs with linear mixture function approximation of both transitions and rewards?", "answer": ["Optimistic Policy Optimization is Provably Efficient in Non-stationary MDPs"], "answer_arxiv_id": ["2110.08984"], "source_meta": {"published_time": "20230601"}, "qid": "AutoScholarQuery_test_564"}
{"question": "What studies discuss solutions involving interpolation between LR and RP gradients?", "answer": ["PIPPS: Flexible Model-Based Policy Search Robust to the Curse of Chaos", "Do Differentiable Simulators Give Better Policy Gradients?"], "answer_arxiv_id": ["1902.01240", "2202.00817"], "source_meta": {"published_time": "20231214"}, "qid": "AutoScholarQuery_test_565"}
{"question": "Which works have implemented multimodal understanding and generative capacities across modalities?", "answer": ["ImageBind: One Embedding Space To Bind Them All", "Any-to-Any Generation via Composable Diffusion", "Generating Images with Multimodal Language Models", "NExT-GPT: Any-to-Any Multimodal LLM", "Emu: Generative Pretraining in Multimodality", "Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction\n  Tuning"], "answer_arxiv_id": ["2305.05665v2", "2305.11846", "2305.17216", "2309.05519", "2307.05222", "2309.02591"], "source_meta": {"published_time": "20231205"}, "qid": "AutoScholarQuery_test_566"}
{"question": "What research conducted studies on zero-shot tasks on images using foundation models?", "answer": ["Learning Transferable Visual Models From Natural Language Supervision", "CoCa: Contrastive Captioners are Image-Text Foundation Models", "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision", "Florence: A New Foundation Model for Computer Vision"], "answer_arxiv_id": ["2103.00020", "2205.01917", "2102.05918", "2111.11432"], "source_meta": {"published_time": "20230215"}, "qid": "AutoScholarQuery_test_567"}
{"question": "What paper used shifted window attention to enable information propagation in the area of action recongnition?", "answer": ["Video Swin Transformer"], "answer_arxiv_id": ["2106.13230"], "source_meta": {"published_time": "20231204"}, "qid": "AutoScholarQuery_test_568"}
{"question": "What works have explored the field of zero-shot segmentation recently?", "answer": ["Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks", "ReCo: Retrieve and Co-segment for Zero-shot Transfer", "Image Segmentation Using Text and Image Prompts", "Zero-Shot Semantic Segmentation", "DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic\n  Segmentation Using Diffusion Models", "Extract Free Dense Labels from CLIP"], "answer_arxiv_id": ["2401.14159", "2206.07045", "2112.10003", "1906.00817", "2303.11681", "2112.01071"], "source_meta": {"published_time": "20230823"}, "qid": "AutoScholarQuery_test_569"}
{"question": "Could you provide me the research where local visual features aligned with textual concepts in CLIP were revealed?", "answer": ["Extract Free Dense Labels from CLIP"], "answer_arxiv_id": ["2112.01071"], "source_meta": {"published_time": "20230602"}, "qid": "AutoScholarQuery_test_570"}
{"question": "Could you provide me some works about localization strategies using region-proposal detector or a segmentation network in vision-language models?", "answer": ["Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks"], "answer_arxiv_id": ["2401.14159"], "source_meta": {"published_time": "20231201"}, "qid": "AutoScholarQuery_test_571"}
{"question": "Any research on probablistic personalized page rank (ProPPR) models?", "answer": ["Programming with Personalized PageRank: A Locally Groundable First-Order Probabilistic Logic"], "answer_arxiv_id": ["1305.2254"], "source_meta": {"published_time": "20230522"}, "qid": "AutoScholarQuery_test_572"}
{"question": "Which studies offer solutions to deal with accuracy loss when a sub-model is sent to slower devices?", "answer": ["FjORD: Fair and Accurate Federated Learning under heterogeneous targets with Ordered Dropout", "HeteroFL: Computation and Communication Efficient Federated Learning for Heterogeneous Clients"], "answer_arxiv_id": ["2102.13451", "2010.01264"], "source_meta": {"published_time": "20230705"}, "qid": "AutoScholarQuery_test_573"}
{"question": "What paper proposed SuperGLUE after seeing that many models were surpassing non-expert humans on GLUE?", "answer": ["SuperGLUE: A Stickier Benchmark for General-Purpose Language\n  Understanding Systems"], "answer_arxiv_id": ["1905.00537"], "source_meta": {"published_time": "20240613"}, "qid": "AutoScholarQuery_test_574"}
{"question": "Who is responsible for the best-known result for the MMS approximation in additive valuations, 3/4+3/3836?", "answer": ["Breaking the 3/4 Barrier for Approximate Maximin Share"], "answer_arxiv_id": ["2307.07304"], "source_meta": {"published_time": "20230828"}, "qid": "AutoScholarQuery_test_575"}
{"question": "Which works pioneered the use of Hessian-based influence functions to understand how training data affects a model's prediction?", "answer": ["Understanding Black-box Predictions via Influence Functions"], "answer_arxiv_id": ["1703.04730"], "source_meta": {"published_time": "20240126"}, "qid": "AutoScholarQuery_test_576"}
{"question": "What work has used self-supervised pre-training to design a flexible reward function by utilizing a broad dataset of human videos and a small dataset of robot videos?", "answer": ["Learning Generalizable Robotic Reward Functions from “In-The-Wild” Human Videos"], "answer_arxiv_id": ["2103.16817"], "source_meta": {"published_time": "20230227"}, "qid": "AutoScholarQuery_test_577"}
{"question": "Could you provide some works where delayed feedback is studied in stochastic settings for UCB-based methods?", "answer": ["Bandit Learning with Delayed Impact of Actions", "Linear Bandits with Stochastic Delayed Feedback", "Stochastic bandits with arm-dependent delays"], "answer_arxiv_id": ["2002.10316", "1807.02089v3", "2006.10459"], "source_meta": {"published_time": "20231029"}, "qid": "AutoScholarQuery_test_578"}
{"question": "Which papers discuss solutions to commonsense reasoning problems?", "answer": ["CommonsenseQA: A Question Answering Challenge Targeting Commonsense\n  Knowledge", "CommonsenseQA 2.0: Exposing the Limits of AI through Gamification", "Cosmos QA: Machine Reading Comprehension with Contextual Commonsense\n  Reasoning", "Abductive Commonsense Reasoning", "SocialIQA: Commonsense Reasoning about Social Interactions"], "answer_arxiv_id": ["1811.00937", "2201.05320", "1909.00277", "1908.05739", "1904.09728"], "source_meta": {"published_time": "20231129"}, "qid": "AutoScholarQuery_test_579"}
{"question": "Could you name some works where diffusion models were applied to represent 3D scenes and motion sequences?", "answer": ["DiffRF: Rendering-Guided 3D Radiance Field Diffusion", "PhysDiff: Physics-Guided Human Motion Diffusion Model", "Human Motion Diffusion Model"], "answer_arxiv_id": ["2212.01206", "2212.02500", "2209.14916"], "source_meta": {"published_time": "20230216"}, "qid": "AutoScholarQuery_test_580"}
{"question": "Where is the concept of Generative Adversarial Networks (GANs) introduced?", "answer": ["Generative Adversarial Networks"], "answer_arxiv_id": ["1406.2661"], "source_meta": {"published_time": "20231203"}, "qid": "AutoScholarQuery_test_581"}
{"question": "Any works about searching for the optimal scaling factor in the context of quantization-aware training?", "answer": ["PACT: Parameterized Clipping Activation for Quantized Neural Networks", "DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients", "Learned Step Size Quantization"], "answer_arxiv_id": ["1805.06085", "1606.06160", "1902.08153"], "source_meta": {"published_time": "20230204"}, "qid": "AutoScholarQuery_test_582"}
{"question": "Who applied the mutual information in the context of maximising similarity between successive observations for representation learning?", "answer": ["Deep Reinforcement and InfoMax Learning"], "answer_arxiv_id": ["2006.07217"], "source_meta": {"published_time": "20230523"}, "qid": "AutoScholarQuery_test_583"}
{"question": "Are there any studies that have explored teacher assistant-based and student-friendly distillation to alleviate the problem of performance degradation in larger LMs?", "answer": ["Improved Knowledge Distillation via Teacher Assistant", "On the Efficacy of Knowledge Distillation", "Decoupled Knowledge Distillation", "Lifting the Curse of Capacity Gap in Distilling Language Models"], "answer_arxiv_id": ["1902.03393", "1910.01348", "2203.08679", "2305.12129"], "source_meta": {"published_time": "20240219"}, "qid": "AutoScholarQuery_test_584"}
{"question": "What works used data-driven approaches in AutoRL methods to learn various algorithmic components?", "answer": ["Meta-Learning via Learned Loss", "Meta-Gradient Reinforcement Learning with an Objective Discovered Online", "VeLO: Training Versatile Learned Optimizers by Scaling Up"], "answer_arxiv_id": ["1906.05374", "2007.08433", "2211.09760"], "source_meta": {"published_time": "20230602"}, "qid": "AutoScholarQuery_test_585"}
{"question": "What works have modified compressed sensing approaches to include a cross-validation step?", "answer": ["Compressed Sensing with Cross Validation"], "answer_arxiv_id": ["0803.1845"], "source_meta": {"published_time": "20230224"}, "qid": "AutoScholarQuery_test_586"}
{"question": "Are there works that investigate the implementation of gradient descent and stochastic gradient descent based on the Polyak-Lojasiewicz (PL) assumption?", "answer": ["Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-Łojasiewicz Condition", "SGD for Structured Nonconvex Functions: Learning Rates, Minibatching and Interpolation"], "answer_arxiv_id": ["1608.04636", "2006.10311v3"], "source_meta": {"published_time": "20220613"}, "qid": "AutoScholarQuery_test_587"}
{"question": "Which work used counterfactual interventions to determine the unfaithfulness of explanations of a LLM's predictions?", "answer": ["Faithfulness Tests for Natural Language Explanations"], "answer_arxiv_id": ["2305.18029"], "source_meta": {"published_time": "20231113"}, "qid": "AutoScholarQuery_test_588"}
{"question": "Which work was the first to propose Generation-Augmented Retrieval in question answering?", "answer": ["Generation-Augmented Retrieval for Open-domain Question Answering"], "answer_arxiv_id": ["2009.08553"], "source_meta": {"published_time": "20240109"}, "qid": "AutoScholarQuery_test_589"}
{"question": "What studies discuss the introduction of large-scale continuous sign language datasets?", "answer": ["BSL-1K: Scaling up co-articulated sign language recognition using\n  mouthing cues", "How2Sign: A Large-scale Multimodal Dataset for Continuous American Sign\n  Language"], "answer_arxiv_id": ["2007.12131", "2008.08143"], "source_meta": {"published_time": "20231205"}, "qid": "AutoScholarQuery_test_590"}
{"question": "What papers proposed architectural modifications to recurrence equations for irregular time series data?", "answer": ["Discrete Event, Continuous Time RNNs", "Recurrent Neural Networks for Multivariate Time Series with Missing Values", "Unitary Evolution Recurrent Neural Networks"], "answer_arxiv_id": ["1710.04110", "1606.01865", "1511.06464"], "source_meta": {"published_time": "20230303"}, "qid": "AutoScholarQuery_test_591"}
{"question": "Could you provide me with the papers that proposed using graph homomorphism counts for feature embedding in learning tasks?", "answer": ["Graph Homomorphism Convolution"], "answer_arxiv_id": ["2005.01214"], "source_meta": {"published_time": "20230609"}, "qid": "AutoScholarQuery_test_592"}
{"question": "Which research applied inversion attacks to models across different domains like computational genetics, computer vision, and NLP?", "answer": ["Information Leakage in Embedding Models"], "answer_arxiv_id": ["2004.00053"], "source_meta": {"published_time": "20240122"}, "qid": "AutoScholarQuery_test_593"}
{"question": "Which works expanded on the contrastive methods of representation learning by integrating additional views into the mutual information maximization objective?", "answer": ["Contrastive Multi-View Representation Learning on Graphs", "Deep Graph Contrastive Representation Learning"], "answer_arxiv_id": ["2006.05582", "2006.04131"], "source_meta": {"published_time": "20231003"}, "qid": "AutoScholarQuery_test_594"}
{"question": "Can you provide me studies about decomposing MOCO problems into a series of single-objective combinatorial optimization problems?", "answer": ["Pareto Set Learning for Expensive Multi-Objective Optimization", "Learning the Pareto Front with Hypernetworks", "Pareto Multi-Task Learning"], "answer_arxiv_id": ["2210.08495", "2010.04104", "1912.12854"], "source_meta": {"published_time": "20231022"}, "qid": "AutoScholarQuery_test_595"}
{"question": "Which papers focused on the calibration of classifiers in foundation models for natural language processing?", "answer": ["How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering", "Calibration of Pre-trained Transformers"], "answer_arxiv_id": ["2012.00955", "2003.07892"], "source_meta": {"published_time": "20230219"}, "qid": "AutoScholarQuery_test_596"}
{"question": "Which works have been established for regret minimization in two types of MDPs under linear function approximation?", "answer": ["Reinforcement Learning in Feature Space: Matrix Bandit, Kernels, and Regret Bound", "Model-Based Reinforcement Learning with Value-Targeted Regression", "Sample-Optimal Parametric Q-Learning Using Linearly Additive Features", "Provably Efficient Reinforcement Learning with Linear Function Approximation"], "answer_arxiv_id": ["1905.10389", "2006.01107", "1902.04779", "1907.05388"], "source_meta": {"published_time": "20231029"}, "qid": "AutoScholarQuery_test_597"}
{"question": "What works have focused on integrating constraints into sequential decision problems?", "answer": ["Constrained Policy Optimization", "Exploration-Exploitation in Constrained MDPs", "A Lyapunov-based Approach to Safe Reinforcement Learning", "Constrained Reinforcement Learning Has Zero Duality Gap"], "answer_arxiv_id": ["1705.10528", "2003.02189", "1805.07708", "1910.13393"], "source_meta": {"published_time": "20230202"}, "qid": "AutoScholarQuery_test_598"}
{"question": "What work analysed VAE-based disentanglement techniques on correlated data?", "answer": ["On Disentangled Representations Learned from Correlated Data"], "answer_arxiv_id": ["2006.07886"], "source_meta": {"published_time": "20230523"}, "qid": "AutoScholarQuery_test_599"}
{"question": "Could you provide me some studies that investigate where factual information is stored in transformers?", "answer": ["Knowledge Neurons in Pretrained Transformers", "Locating and Editing Factual Associations in GPT", "Mass-Editing Memory in a Transformer"], "answer_arxiv_id": ["2104.08696", "2202.05262", "2210.07229"], "source_meta": {"published_time": "20230615"}, "qid": "AutoScholarQuery_test_600"}
{"question": "What publications discuss the usage of Bayesian neural network in enhancing the efficiency of FL?", "answer": ["Bayesian Nonparametric Federated Learning of Neural Networks", "Statistical Model Aggregation via Parameter Matching"], "answer_arxiv_id": ["1905.12022", "1911.00218"], "source_meta": {"published_time": "20230504"}, "qid": "AutoScholarQuery_test_601"}
{"question": "What papers delved into the conformal prediction methods for drug property prediction?", "answer": ["Concepts and Applications of Conformal Prediction in Computational Drug Discovery"], "answer_arxiv_id": ["1908.03569v1"], "source_meta": {"published_time": "20231018"}, "qid": "AutoScholarQuery_test_602"}
{"question": "What papers evaluated Generation-Augmented Retrieval in passage retrieval and fine-tuning?", "answer": ["Precise Zero-Shot Dense Retrieval without Relevance Labels", "Query2doc: Query Expansion with Large Language Models"], "answer_arxiv_id": ["2212.10496", "2303.07678"], "source_meta": {"published_time": "20240109"}, "qid": "AutoScholarQuery_test_603"}
{"question": "Can you provide any works that adopted transition sampling for long-term generation of motion?", "answer": ["Synthesizing Long-Term Human Motions with Diffusion Models via Coherent\n  Sampling"], "answer_arxiv_id": ["2308.01850"], "source_meta": {"published_time": "20240301"}, "qid": "AutoScholarQuery_test_604"}
{"question": "Which work is the only one investigating training a hypernetwork end-to-end to arbitrarily modify the weights of a policy in meta-RL?", "answer": ["Hypernetworks in Meta-Reinforcement Learning"], "answer_arxiv_id": ["2210.11348"], "source_meta": {"published_time": "20230926"}, "qid": "AutoScholarQuery_test_605"}
{"question": "Can you mention some studies that combined 3D Gaussian Splatting with diffusion models for efficient text-to-3D generation?", "answer": ["DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content\n  Creation", "Text-to-3D using Gaussian Splatting"], "answer_arxiv_id": ["2309.16653", "2309.16585"], "source_meta": {"published_time": "20231226"}, "qid": "AutoScholarQuery_test_606"}
{"question": "Does there exist a work which demonstrates that the PAC-Bayes framework can't be used to derive distribution-free PAC learning bounds for classes that have infinite Littlestone dimension?", "answer": ["A Limitation of the PAC-Bayes Framework"], "answer_arxiv_id": ["2006.13508"], "source_meta": {"published_time": "20230523"}, "qid": "AutoScholarQuery_test_607"}
{"question": "What works tackle the problem of designing data augmentations for graph classification?", "answer": ["Graph Data Augmentation for Graph Machine Learning: A Survey", "Data Augmentation for Deep Graph Learning: A Survey"], "answer_arxiv_id": ["2202.08871", "2202.08235"], "source_meta": {"published_time": "20220226"}, "qid": "AutoScholarQuery_test_608"}
{"question": "In what papers are the singular vectors of the first weight matrix proposed as global disentangled perturbations?", "answer": ["Closed-Form Factorization of Latent Semantics in GANs"], "answer_arxiv_id": ["2007.06600v4"], "source_meta": {"published_time": "20221011"}, "qid": "AutoScholarQuery_test_609"}
{"question": "Which works propose alternate perspectives of robustness in explanations?", "answer": ["Issues with post-hoc counterfactual explanations: a discussion", "On the Robustness of Interpretability Methods"], "answer_arxiv_id": ["1906.04774v1", "1806.08049v1"], "source_meta": {"published_time": "20230519"}, "qid": "AutoScholarQuery_test_610"}
{"question": "Which references explore reproducibility in optimization?", "answer": ["Reproducibility in Optimization: Theoretical Framework and Limits"], "answer_arxiv_id": ["2202.04598"], "source_meta": {"published_time": "20230523"}, "qid": "AutoScholarQuery_test_611"}
{"question": "What research have used normalizing flows in molecular structure design?", "answer": ["Equivariant Flows: Exact Likelihood Generative Learning for Symmetric Densities", "E(n) Equivariant Normalizing Flows"], "answer_arxiv_id": ["2006.02425", "2105.09016"], "source_meta": {"published_time": "20230613"}, "qid": "AutoScholarQuery_test_612"}
{"question": "Could you provide me some studies of zero-order methods applying strategies that the more expensive oracles are used infrequently?", "answer": ["Derivative-Free Method For Composite Optimization With Applications To Decentralized Distributed Optimization", "Oracle Complexity Separation in Convex Optimization", "One-Point Gradient-Free Methods for Composite Optimization with Applications to Distributed Optimization"], "answer_arxiv_id": ["1911.10645", "2002.02706", "2107.05951v2"], "source_meta": {"published_time": "20230415"}, "qid": "AutoScholarQuery_test_613"}
{"question": "What work adds a buffer of demonstrations to the RL framework?", "answer": ["Overcoming Exploration in Reinforcement Learning with Demonstrations"], "answer_arxiv_id": ["1709.10089"], "source_meta": {"published_time": "20221007"}, "qid": "AutoScholarQuery_test_614"}
{"question": "What works are amongst the most influential in relation to U-Net?", "answer": ["UNet++: A Nested U-Net Architecture for Medical Image Segmentation", "Attention U-Net: Learning Where to Look for the Pancreas", "3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation", "Denoising Diffusion Probabilistic Models", "nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation", "A Probabilistic U-Net for Segmentation of Ambiguous Images", "A Variational U-Net for Conditional Appearance and Shape Generation", "Road Extraction by Deep Residual U-Net"], "answer_arxiv_id": ["1807.10165", "1804.03999", "1606.06650", "2006.11239", "1809.10486", "1806.05034", "1804.04694", "1711.10684"], "source_meta": {"published_time": "20230531"}, "qid": "AutoScholarQuery_test_615"}
{"question": "What studies discuss the issue of undefined importance weights in disjoint source and target?", "answer": ["Understanding Self-Training for Gradual Domain Adaptation", "Connect, Not Collapse: Explaining Contrastive Learning for Unsupervised Domain Adaptation"], "answer_arxiv_id": ["2002.11361v1", "2204.00570v4"], "source_meta": {"published_time": "20230206"}, "qid": "AutoScholarQuery_test_616"}
{"question": "What works emphasize that an LLM's acquired knowledge should mirror established facts?", "answer": ["Alignment for Honesty"], "answer_arxiv_id": ["2312.07000"], "source_meta": {"published_time": "20240214"}, "qid": "AutoScholarQuery_test_617"}
{"question": "What work propose the task of emotional support conversation and release the corresponding dataset?", "answer": ["Towards Emotional Support Dialog Systems"], "answer_arxiv_id": ["2106.01144"], "source_meta": {"published_time": "20240220"}, "qid": "AutoScholarQuery_test_618"}
{"question": "Which papers discussed the task of temporal or timeline summarization?", "answer": ["A Temporally Sensitive Submodularity Framework for Timeline Summarization"], "answer_arxiv_id": ["1810.07949"], "source_meta": {"published_time": "20220727"}, "qid": "AutoScholarQuery_test_619"}
{"question": "Can you name the studies that provided allocation algorithms that guarantee ex-ante and EF1 ex-post for additive valuations?", "answer": ["Best of Both Worlds: Ex-Ante and Ex-Post Fairness in Resource Allocation"], "answer_arxiv_id": ["2005.14122"], "source_meta": {"published_time": "20230828"}, "qid": "AutoScholarQuery_test_620"}
{"question": "What works used text-based language models to predict text-evoked and speech-evoked brain activity?", "answer": ["Interpreting and improving natural-language processing (in machines)\n  with natural language-processing (in the brain)", "Inducing brain-relevant bias in natural language processing models", "Relating Simple Sentence Representations in Deep Neural Networks and the\n  Brain", "Low-Dimensional Structure in the Space of Language Representations is\n  Reflected in Brain Responses", "Neural Language Taskonomy: Which NLP Tasks are the most Predictive of\n  fMRI Brain Activity?", "Language models and brain alignment: beyond word-level semantics and\n  prediction", "Joint processing of linguistic properties in brains and language models"], "answer_arxiv_id": ["1905.11833", "1911.03268", "1906.11861", "2106.05426", "2205.01404", "2212.00596", "2212.08094"], "source_meta": {"published_time": "20231108"}, "qid": "AutoScholarQuery_test_621"}
{"question": "Could you list the studies that discuss the strong correlation between the supervised disentanglement score and the global-basis-compatibility?", "answer": ["Analyzing the Latent Space of GAN through Local Dimension Estimation"], "answer_arxiv_id": ["2205.13182"], "source_meta": {"published_time": "20221011"}, "qid": "AutoScholarQuery_test_622"}
{"question": "What works initially used a one-time retrieval method but faced knowledge omissions issues in handling knowledge-sensitive tasks?", "answer": ["Improving language models by retrieving from trillions of tokens", "Internet-augmented language models through few-shot prompting for\n  open-domain question answering", "Atlas: Few-shot Learning with Retrieval Augmented Language Models"], "answer_arxiv_id": ["2112.04426", "2203.05115", "2208.03299"], "source_meta": {"published_time": "20240628"}, "qid": "AutoScholarQuery_test_623"}
{"question": "Can you provide scholarly works that discuss domain-specific knowledge in Knowledge Graphs?", "answer": ["What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams"], "answer_arxiv_id": ["2009.13081"], "source_meta": {"published_time": "20231024"}, "qid": "AutoScholarQuery_test_624"}
{"question": "Are there any research papers studying VR HMD settings using public datasets such as AMASS?", "answer": ["AMASS: Archive of Motion Capture as Surface Shapes"], "answer_arxiv_id": ["1904.03278"], "source_meta": {"published_time": "20240306"}, "qid": "AutoScholarQuery_test_625"}
{"question": "What research paper is related to and compared with the one in this section, where it considers CT model with latent variables, subsections, and an additional loss term for the integration error?", "answer": ["Continuous-time system identification with neural networks: model structures and fitting criteria"], "answer_arxiv_id": ["2006.02915"], "source_meta": {"published_time": "20220420"}, "qid": "AutoScholarQuery_test_626"}
{"question": "Can you provide studies that used transformer-based architectures to tackle German sign language production?", "answer": ["Progressive Transformers for End-to-End Sign Language Production", "Sign Language Transformers: Joint End-to-end Sign Language Recognition\n  and Translation"], "answer_arxiv_id": ["2004.14874", "2003.13830"], "source_meta": {"published_time": "20231205"}, "qid": "AutoScholarQuery_test_627"}
{"question": "Are there any references showing that the chain-of-thoughts generated by LLMs are often arbitrary or contradictory?", "answer": ["Do Models Explain Themselves? Counterfactual Simulatability of Natural\n  Language Explanations", "Chain-of-Verification Reduces Hallucination in Large Language Models"], "answer_arxiv_id": ["2307.08678", "2309.11495"], "source_meta": {"published_time": "20231114"}, "qid": "AutoScholarQuery_test_628"}
{"question": "Which paper applied the Gumbel-Softmax relaxation within gradient-based BOED for contextual optimization?", "answer": ["Efficient Real-world Testing of Causal Decision Making via Bayesian Experimental Design for Contextual Optimisation"], "answer_arxiv_id": ["2207.05250"], "source_meta": {"published_time": "20230221"}, "qid": "AutoScholarQuery_test_629"}
{"question": "Which papers used the BookCorpus, also known as the Toronto Books Corpus, for pretraining?", "answer": ["BERT: Pre-training of Deep Bidirectional Transformers for Language\n  Understanding", "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "Exploring the Limits of Transfer Learning with a Unified Text-to-Text\n  Transformer"], "answer_arxiv_id": ["1810.04805", "1907.11692", "1910.10683"], "source_meta": {"published_time": "20230603"}, "qid": "AutoScholarQuery_test_630"}
{"question": "Which papers discussed strategies to adjust the spatiotemporal resolution as a way to fine-tune TAD models?", "answer": ["TALLFormer: Temporal Action Localization with a Long-memory Transformer", "Re^2TAL: Rewiring Pretrained Video Backbones for Reversible Temporal\n  Action Localization", "An Efficient Spatio-Temporal Pyramid Transformer for Action Detection"], "answer_arxiv_id": ["2204.01680", "2211.14053", "2207.10448"], "source_meta": {"published_time": "20231204"}, "qid": "AutoScholarQuery_test_631"}
{"question": "What previous works have analyzed errors in model’s reasoning in black-box LLMs?", "answer": ["Measuring Faithfulness in Chain-of-Thought Reasoning", "Self-Contradictory Reasoning Evaluation and Detection", "Towards Understanding Chain-of-Thought Prompting: An Empirical Study of\n  What Matters"], "answer_arxiv_id": ["2307.13702", "2311.09603", "2212.10001"], "source_meta": {"published_time": "20240228"}, "qid": "AutoScholarQuery_test_632"}
{"question": "Could you point me to some papers that discuss the HiPPO framework and its applications in time-series modelling?", "answer": ["HiPPO: Recurrent Memory with Optimal Polynomial Projections", "Efficiently Modeling Long Sequences with Structured State Spaces"], "answer_arxiv_id": ["2008.07669", "2111.00396"], "source_meta": {"published_time": "20230303"}, "qid": "AutoScholarQuery_test_633"}
{"question": "What research has been done in using Deep Neural Networks and graph representations with reinforcement learning in resolving propositional logic problems?", "answer": ["Automated proof synthesis for propositional logic with deep neural\n  networks", "Automated Theorem Proving in Intuitionistic Propositional Logic by Deep\n  Reinforcement Learning"], "answer_arxiv_id": ["1805.11799", "1811.00796"], "source_meta": {"published_time": "20240410"}, "qid": "AutoScholarQuery_test_634"}
{"question": "Which paper first tackled the pose estimation of novel objects without CAD models using RGB images?", "answer": ["Gen6D: Generalizable Model-Free 6-DoF Object Pose Estimation from RGB\n  Images"], "answer_arxiv_id": ["2204.10776"], "source_meta": {"published_time": "20231201"}, "qid": "AutoScholarQuery_test_635"}
{"question": "Can you cite works that have automated QA in online courses?", "answer": ["Agent Smith: Teaching Question Answering to Jill Watson"], "answer_arxiv_id": ["2112.13677v2"], "source_meta": {"published_time": "20240303"}, "qid": "AutoScholarQuery_test_636"}
{"question": "Which studies looked at robustness of MRL to distributional shifts?", "answer": ["Meta-Reinforcement Learning Robust to Distributional Shift via Model Identification and Experience Relabeling", "Distributionally Adaptive Meta Reinforcement Learning"], "answer_arxiv_id": ["2006.07178", "2210.03104"], "source_meta": {"published_time": "20230126"}, "qid": "AutoScholarQuery_test_637"}
{"question": "Which studies have transferred the knowledge from pre-trained vision-language models to object detectors?", "answer": ["Open-vocabulary Object Detection via Vision and Language Knowledge\n  Distillation"], "answer_arxiv_id": ["2104.13921"], "source_meta": {"published_time": "20231214"}, "qid": "AutoScholarQuery_test_638"}
{"question": "Could you provide me some work where the concept of LoRA is proposed?", "answer": ["LoRA: Low-Rank Adaptation of Large Language Models"], "answer_arxiv_id": ["2106.09685"], "source_meta": {"published_time": "20240224"}, "qid": "AutoScholarQuery_test_639"}
{"question": "Any links to studies considering a single update at each client and focusing on random walks?", "answer": ["Incremental Stochastic Subgradient Algorithms for Convex Optimization", "Private Weighted Random Walk Stochastic Gradient Descent", "Walkman: A Communication-Efficient Random-Walk Algorithm for Decentralized Optimization", "Privacy Amplification by Decentralization"], "answer_arxiv_id": ["0806.1092", "2009.01790", "1804.06568", "2012.05326v4"], "source_meta": {"published_time": "20231106"}, "qid": "AutoScholarQuery_test_640"}
{"question": "Which study introduced the Neural Tangent Kernel (NTK)?", "answer": ["Neural Tangent Kernel: Convergence and Generalization in Neural Networks"], "answer_arxiv_id": ["1806.07572"], "source_meta": {"published_time": "20221115"}, "qid": "AutoScholarQuery_test_641"}
{"question": "Has anyone tried to generalize the feature embedding approach to adaptive features?", "answer": ["Learning Deep Features in Instrumental Variable Regression", "Deep Proxy Causal Learning and its Application to Confounded Bandit Policy Evaluation"], "answer_arxiv_id": ["2010.07154", "2106.03907"], "source_meta": {"published_time": "20221012"}, "qid": "AutoScholarQuery_test_642"}
{"question": "Who proposed property-aware relation networks (PAR) in the context of few-shot learning?", "answer": ["Property-Aware Relation Networks for Few-Shot Molecular Property Prediction"], "answer_arxiv_id": ["2107.07994"], "source_meta": {"published_time": "20230424"}, "qid": "AutoScholarQuery_test_643"}
{"question": "What study models reasoning procedures as BFS or DFS search on reasoning trees?", "answer": ["Tree of Thoughts: Deliberate Problem Solving with Large Language Models"], "answer_arxiv_id": ["2305.10601"], "source_meta": {"published_time": "20240628"}, "qid": "AutoScholarQuery_test_644"}
{"question": "Which studies focus on evaluation of a targeted update in knowledge updating?", "answer": ["Locating and Editing Factual Associations in GPT", "Fast Model Editing at Scale"], "answer_arxiv_id": ["2202.05262", "2110.11309"], "source_meta": {"published_time": "20230615"}, "qid": "AutoScholarQuery_test_645"}
{"question": "What works describe the role of the Neural Tangent Kernel in the density of neural networks?", "answer": ["Neural Tangent Kernel: Convergence and Generalization in Neural Networks"], "answer_arxiv_id": ["1806.07572"], "source_meta": {"published_time": "20221126"}, "qid": "AutoScholarQuery_test_646"}
{"question": "What studies analyze the effect of multiple regularizations employed in deep learning, like weight decay, early stopping, or drop-outs, on the generalization abilities?", "answer": ["Norm-Based Capacity Control in Neural Networks", "Dropout Rademacher Complexity of Deep Neural Networks"], "answer_arxiv_id": ["1503.00036", "1402.3811"], "source_meta": {"published_time": "20230419"}, "qid": "AutoScholarQuery_test_647"}
{"question": "Could you provide me some studies that relate to early works on aligning text and image embeddings?", "answer": ["Deep Visual-Semantic Alignments for Generating Image Descriptions"], "answer_arxiv_id": ["1412.2306"], "source_meta": {"published_time": "20221004"}, "qid": "AutoScholarQuery_test_648"}
{"question": "Which paper introduced joint differential privacy (JDP)?", "answer": ["Robust Mediators in Large Games"], "answer_arxiv_id": ["1512.02698v2"], "source_meta": {"published_time": "20230516"}, "qid": "AutoScholarQuery_test_649"}
{"question": "Which research papers explored the implementation of state space models-based architectures?", "answer": ["Efficiently Modeling Long Sequences with Structured State Spaces", "Diagonal State Spaces are as Effective as Structured State Spaces", "On the Parameterization and Initialization of Diagonal State Space\n  Models"], "answer_arxiv_id": ["2111.00396", "2203.14343", "2206.11893"], "source_meta": {"published_time": "20231130"}, "qid": "AutoScholarQuery_test_650"}
{"question": "Which papers propose goal-driven forecasting to predict goal locations for future human walking trajectories?", "answer": ["Long-term Human Motion Prediction with Scene Context"], "answer_arxiv_id": ["2007.03672"], "source_meta": {"published_time": "20221125"}, "qid": "AutoScholarQuery_test_651"}
{"question": "Could you list the works exploring the methodologies to effectively utilize the inherent prior knowledge within CAD?", "answer": ["Counterfactually-Augmented SNLI Training Data Does Not Yield Better\n  Generalization Than Unaugmented Data", "An Investigation of the (In)effectiveness of Counterfactually Augmented\n  Data", "Unlock the Potential of Counterfactually-Augmented Data in\n  Out-Of-Distribution Generalization"], "answer_arxiv_id": ["2010.04762", "2107.00753", "2310.06666"], "source_meta": {"published_time": "20240609"}, "qid": "AutoScholarQuery_test_652"}
{"question": "Which papers discuss the concept of semantic meaning and algebraic structure of popular embeddings?", "answer": ["Towards Understanding Linear Word Analogies", "Linear Spaces of Meanings: Compositional Structures in Vision-Language Models", "Prompt Algebra for Task Composition"], "answer_arxiv_id": ["1810.04882", "2302.14383", "2306.00310"], "source_meta": {"published_time": "20231026"}, "qid": "AutoScholarQuery_test_653"}
{"question": "Which works illustrate that transformer-based LLMs have been trained on diverse, large-scale datasets to simultaneously learn several language understanding tasks?", "answer": ["Exploring the Limits of Transfer Learning with a Unified Text-to-Text\n  Transformer", "Scaling Instruction-Finetuned Language Models"], "answer_arxiv_id": ["1910.10683", "2210.11416"], "source_meta": {"published_time": "20240208"}, "qid": "AutoScholarQuery_test_654"}
{"question": "What studies discuss techniques for data efficiency in deep learning?", "answer": ["Low-Shot Learning from Imaginary Data", "Hallucination Improves the Performance of Unsupervised Visual Representation Learning", "GenCo: An Auxiliary Generator from Contrastive Learning for Enhanced Few-Shot Learning in Remote Sensing"], "answer_arxiv_id": ["1801.05401", "2307.12168", "2307.14612"], "source_meta": {"published_time": "20230228"}, "qid": "AutoScholarQuery_test_655"}
{"question": "Could you provide me some works that focus on improving the quality of input reconstruction?", "answer": ["Inverting Gradients - How easy is it to break privacy in federated learning?", "See through Gradients: Image Batch Recovery via GradInversion", "Gradient Inversion with Generative Image Prior"], "answer_arxiv_id": ["2003.14053", "2104.07586", "2110.14962"], "source_meta": {"published_time": "20230531"}, "qid": "AutoScholarQuery_test_656"}
{"question": "Could you provide me some works about model averaging methods that are further related to federated learning and ensemble methods?", "answer": ["Federated learning with matched averaging", "Model Fusion via Optimal Transport", "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"], "answer_arxiv_id": ["2002.06440", "1910.05653", "2203.05482"], "source_meta": {"published_time": "20230717"}, "qid": "AutoScholarQuery_test_657"}
{"question": "Are there studies focused on mitigation of hallucinations during the pre-training stage by the process of dataset curating and cleaning?", "answer": ["Diving Deep into Modes of Fact Hallucinations in Dialogue Systems", "HAGRID: A Human-LLM Collaborative Dataset for Generative\n  Information-Seeking with Attribution", "Med-HALT: Medical Domain Hallucination Test for Large Language Models"], "answer_arxiv_id": ["2301.04449", "2307.16883", "2307.15343"], "source_meta": {"published_time": "20240106"}, "qid": "AutoScholarQuery_test_658"}
{"question": "Could you provide examples of image-text datasets that have their own preprocessing techniques?", "answer": ["Learning Transferable Visual Models From Natural Language Supervision", "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision", "Combined Scaling for Zero-shot Transfer Learning", "Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts", "RedCaps: Web-curated image-text data created by the people, for the people", "LAION-5B: An open large-scale dataset for training next generation image-text models"], "answer_arxiv_id": ["2103.00020", "2102.05918", "2111.10050", "2102.08981", "2111.11431", "2210.08402"], "source_meta": {"published_time": "20230719"}, "qid": "AutoScholarQuery_test_659"}
{"question": "Which works consider the instruction-following abilities of LLMs?", "answer": ["LLaMA: Open and Efficient Foundation Language Models", "Llama 2: Open Foundation and Fine-Tuned Chat Models"], "answer_arxiv_id": ["2302.13971", "2307.09288"], "source_meta": {"published_time": "20240215"}, "qid": "AutoScholarQuery_test_660"}
{"question": "Could you point out the studies which discusses the high variance problem of likelihood-ratio gradient?", "answer": ["High-Dimensional Continuous Control Using Generalized Advantage Estimation"], "answer_arxiv_id": ["1506.02438"], "source_meta": {"published_time": "20231214"}, "qid": "AutoScholarQuery_test_661"}
{"question": "What papers have introduced single-camera methods for egocentric pose estimation?", "answer": ["Mo2Cap2: Real-time Mobile 3D Motion Capture with a Cap-mounted Fisheye\n  Camera", "xR-EgoPose: Egocentric 3D Human Pose from an HMD Camera"], "answer_arxiv_id": ["1803.05959", "1907.10045"], "source_meta": {"published_time": "20240228"}, "qid": "AutoScholarQuery_test_662"}
{"question": "Which works propose a novel framework or expand the idea to obtain complex and difficult instructions gradually?", "answer": ["WizardLM: Empowering Large Language Models to Follow Complex\n  Instructions", "WizardCoder: Empowering Code Large Language Models with Evol-Instruct", "Textbooks Are All You Need"], "answer_arxiv_id": ["2304.12244", "2306.08568", "2306.11644"], "source_meta": {"published_time": "20240209"}, "qid": "AutoScholarQuery_test_663"}
{"question": "Could you provide me some studies that have applied the concept of teacher-student network?", "answer": ["FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence", "Distilling the Knowledge in a Neural Network", "Knowledge Distillation: A Survey", "Semi-supervised semantic segmentation needs strong, varied perturbations", "Semi-Supervised Semantic Segmentation via Adaptive Equalization Learning", "PseudoSeg: Designing Pseudo Labels for Semantic Segmentation", "Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic\n  Segmentation", "Unbiased Teacher for Semi-Supervised Object Detection", "Humble Teachers Teach Better Students for Semi-Supervised Object\n  Detection", "Distilling Vision-Language Pre-training to Collaborate with\n  Weakly-Supervised Temporal Action Localization", "End-to-End Semi-Supervised Object Detection with Soft Teacher"], "answer_arxiv_id": ["2001.07685v2", "1503.02531", "2006.05525", "1906.01916", "2110.05474", "2010.09713", "2208.09910", "2102.09480", "2106.10456", "2212.09335", "2106.09018"], "source_meta": {"published_time": "20240317"}, "qid": "AutoScholarQuery_test_664"}
{"question": "What papers extended the methods to address pose estimation for novel objects or low-textured objects?", "answer": ["OnePose: One-Shot Object Pose Estimation without CAD Models", "OnePose++: Keypoint-Free One-Shot Object Pose Estimation without CAD\n  Models"], "answer_arxiv_id": ["2205.12257", "2301.07673"], "source_meta": {"published_time": "20231201"}, "qid": "AutoScholarQuery_test_665"}
{"question": "What studies propose generative adversarial networks for 3D shape generation?", "answer": ["Learning Representations and Generative Models for 3D Point Clouds", "3D Point Cloud Generative Adversarial Network Based on Tree Structured Graph Convolutions"], "answer_arxiv_id": ["1707.02392", "1905.06292"], "source_meta": {"published_time": "20230704"}, "qid": "AutoScholarQuery_test_666"}
{"question": "What paper demonstrated that a model trained on synthetic captions can perform better than one trained on human-provided captions?", "answer": ["Is a Caption Worth a Thousand Images? A Controlled Study for Representation Learning"], "answer_arxiv_id": ["2207.07635"], "source_meta": {"published_time": "20230719"}, "qid": "AutoScholarQuery_test_667"}
{"question": "Are there any studies that added a few trainable parameters representing new knowledge to LLMs, while keeping the original parameters frozen?", "answer": ["Calibrating Factual Knowledge in Pretrained Language Models", "Transformer-Patcher: One Mistake worth One Neuron", "Rank-One Editing of Encoder-Decoder Models", "Neural Knowledge Bank for Pretrained Transformers"], "answer_arxiv_id": ["2210.03329", "2301.09785", "2211.13317", "2208.00399"], "source_meta": {"published_time": "20231114"}, "qid": "AutoScholarQuery_test_668"}
{"question": "Can you provide references where global non-convex optimization is applied in machine learning?", "answer": ["X-Armed Bandits", "Global optimization of Lipschitz functions"], "answer_arxiv_id": ["1001.4475", "1703.02628"], "source_meta": {"published_time": "20221116"}, "qid": "AutoScholarQuery_test_669"}
{"question": "Which works are known for tackling various tasks simultaneously in the field of vision-language models?", "answer": ["Learning Transferable Visual Models From Natural Language Supervision", "Flamingo: a Visual Language Model for Few-Shot Learning", "Towards General Purpose Vision Systems", "Class-agnostic Object Detection with Multi-modal Transformer"], "answer_arxiv_id": ["2103.00020", "2204.14198", "2104.00743", "2111.11430"], "source_meta": {"published_time": "20230608"}, "qid": "AutoScholarQuery_test_670"}
{"question": "What datasets are used for knowledge editing?", "answer": ["Zero-Shot Relation Extraction via Reading Comprehension", "Locating and Editing Factual Associations in GPT", "MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop\n  Questions", "Eva-KELLM: A New Benchmark for Evaluating Knowledge Editing of LLMs"], "answer_arxiv_id": ["1706.04115", "2202.05262", "2305.14795", "2308.09954"], "source_meta": {"published_time": "20230916"}, "qid": "AutoScholarQuery_test_671"}
{"question": "What works proposed a method to learn a linear subspace for the disentanglement of written text from visual components?", "answer": ["Disentangling visual and written concepts in CLIP"], "answer_arxiv_id": ["2206.07835"], "source_meta": {"published_time": "20230523"}, "qid": "AutoScholarQuery_test_672"}
{"question": "Could you provide me some studies about the use of case-based reasoning?", "answer": ["Case-Based Reasoning for Natural Language Queries over Knowledge Bases", "CBR-iKB: A Case-Based Reasoning Approach for Question Answering over Incomplete Knowledge Bases", "Knowledge Base Question Answering by Case-based Reasoning over Subgraphs"], "answer_arxiv_id": ["2104.08762", "2204.08554", "2202.10610"], "source_meta": {"published_time": "20231024"}, "qid": "AutoScholarQuery_test_673"}
{"question": "Any existing research on generating the 3D human avatars with predefined parametric human templates?", "answer": ["AvatarGen: A 3D Generative Model for Animatable Human Avatars", "EVA3D: Compositional 3D Human Generation from 2D Image Collections", "Unsupervised Learning of Efficient Geometry-Aware Neural Articulated\n  Representations", "Generative Neural Articulated Radiance Fields", "3D-Aware Semantic-Guided Generative Model for Human Synthesis"], "answer_arxiv_id": ["2211.14589", "2210.04888", "2204.08839", "2206.14314", "2112.01422"], "source_meta": {"published_time": "20231203"}, "qid": "AutoScholarQuery_test_674"}
{"question": "What papers provided a theoratical analysis of the mechanism with respect to mean squared error of the noise?", "answer": ["Almost Tight Error Bounds on Differentially Private Continual Counting"], "answer_arxiv_id": ["2211.05006"], "source_meta": {"published_time": "20230616"}, "qid": "AutoScholarQuery_test_675"}
{"question": "Which papers propose solutions for panel detection in comic understanding?", "answer": ["Object Detection for Comics using Manga109 Annotations"], "answer_arxiv_id": ["1803.08670"], "source_meta": {"published_time": "20240118"}, "qid": "AutoScholarQuery_test_676"}
{"question": "What studies reported varying performance of LLMs when generating different types of reasoning processes?", "answer": ["Program of Thoughts Prompting: Disentangling Computation from Reasoning\n  for Numerical Reasoning Tasks", "PAL: Program-aided Language Models", "Tab-CoT: Zero-shot Tabular Chain of Thought"], "answer_arxiv_id": ["2211.12588", "2211.10435", "2305.17812"], "source_meta": {"published_time": "20240216"}, "qid": "AutoScholarQuery_test_677"}
{"question": "Which papers propose heuristic-based uncertainty metrics for generative Large Language Models (LLMs) considering machine translation?", "answer": ["Wat zei je? Detecting Out-of-Distribution Translations with Variational\n  Transformers", "Unsupervised Quality Estimation for Neural Machine Translation"], "answer_arxiv_id": ["2006.08344", "2005.10608"], "source_meta": {"published_time": "20240219"}, "qid": "AutoScholarQuery_test_678"}
{"question": "Which studies focus on 3D pose estimation task through a single RGB image input?", "answer": ["End-to-end Recovery of Human Shape and Pose", "Learning to Reconstruct 3D Human Pose and Shape via Model-fitting in the\n  Loop"], "answer_arxiv_id": ["1712.06584", "1909.12828"], "source_meta": {"published_time": "20230910"}, "qid": "AutoScholarQuery_test_679"}
{"question": "Which papers propose methods for audio-visual segmentation task?", "answer": ["Class-aware Sounding Objects Localization via Audiovisual Correspondence", "Discriminative Sounding Objects Localization via Self-supervised\n  Audiovisual Matching", "Deep Multimodal Clustering for Unsupervised Audiovisual Learning", "Unsupervised Sound Localization via Iterative Contrastive Learning", "Localizing Visual Sounds the Hard Way", "Self-Supervised Predictive Learning: A Negative-Free Method for Sound Source Localization in Visual Scenes", "Exploiting Transformation Invariance and Equivariance for\n  Self-supervised Sound Localisation", "Learning to Localize Sound Source in Visual Scenes", "Multiple Sound Sources Localization from Coarse to Fine", "Annotation-free Audio-Visual Segmentation"], "answer_arxiv_id": ["2112.11749", "2010.05466", "1807.03094", "2104.00315", "2104.02691", "2203.13412v1", "2206.12772", "1803.03849", "2007.06355", "2305.11019"], "source_meta": {"published_time": "20240317"}, "qid": "AutoScholarQuery_test_680"}
{"question": "Which works are variants of the FedAvg algorithm aimed to resolve the minimization problem in federated learning?", "answer": ["Local SGD Converges Fast and Communicates Little", "Parallel Restarted SGD with Faster Convergence and Less Communication: Demystifying Why Model Averaging Works for Deep Learning", "On the Linear Speedup Analysis of Communication Efficient Momentum SGD for Distributed Non-Convex Optimization"], "answer_arxiv_id": ["1805.09767", "1807.06629", "1905.03817"], "source_meta": {"published_time": "20230420"}, "qid": "AutoScholarQuery_test_681"}
{"question": "What was the research that put forward a data augmentation-dependent method for contrastive learning for object-centric representations?", "answer": ["Towards Self-Supervised Learning of Global and Object-Centric Representations"], "answer_arxiv_id": ["2203.05997"], "source_meta": {"published_time": "20230524"}, "qid": "AutoScholarQuery_test_682"}
{"question": "Which studies focus on table-based EHR question answering?", "answer": ["Question Answering for Complex Electronic Health Records Database using Unified Encoder-Decoder Architecture", "LeafAI: query generator for clinical cohort discovery rivaling a human programmer", "EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records", "Towards Understanding the Generalization of Medical Text-to-SQL Models and Datasets", "Text-to-SQL Generation for Question Answering on Electronic Medical Records"], "answer_arxiv_id": ["2111.14703", "2304.06203v2", "2301.07695", "2303.12898", "1908.01839"], "source_meta": {"published_time": "20231028"}, "qid": "AutoScholarQuery_test_683"}
{"question": "Can you provide examples of studies about the methodology of studying games that gradually change over time?", "answer": ["Econometrics for Learning Agents", "Learning and Efficiency in Games with Dynamic Population", "An Experimental Evaluation of Regret-Based Econometrics", "Dynamic network congestion games"], "answer_arxiv_id": ["1505.00720", "1505.00391", "1605.03838", "2009.13632v1"], "source_meta": {"published_time": "20220928"}, "qid": "AutoScholarQuery_test_684"}
{"question": "Are there any studies where the shuffling-based method was applied to FL?", "answer": ["Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond", "On the Convergence of Federated Averaging with Cyclic Client Participation"], "answer_arxiv_id": ["2110.10342", "2302.03109v1"], "source_meta": {"published_time": "20231106"}, "qid": "AutoScholarQuery_test_685"}
{"question": "Can you list the research works where the number of timesteps used to train SNNs have been reduced?", "answer": ["Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation", "ESL-SNNs: An Evolutionary Structure Learning Strategy for Spiking Neural Networks", "Going Deeper With Directly-Trained Larger Spiking Neural Networks"], "answer_arxiv_id": ["2005.01807", "2306.03693", "2011.05280"], "source_meta": {"published_time": "20230402"}, "qid": "AutoScholarQuery_test_686"}
{"question": "Could you mention the studies that focused on lifting 2D pre-trained models to create 3D models from textual prompts?", "answer": ["Magic3D: High-Resolution Text-to-3D Content Creation", "Fantasia3D: Disentangling Geometry and Appearance for High-quality\n  Text-to-3D Content Creation", "ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with\n  Variational Score Distillation", "Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation", "ATT3D: Amortized Text-to-3D Object Synthesis", "ZeroAvatar: Zero-shot 3D Avatar Generation from a Single Image", "MVDiffusion: Enabling Holistic Multi-view Image Generation with\n  Correspondence-Aware Diffusion", "Sparse3D: Distilling Multiview-Consistent Diffusion for Object\n  Reconstruction from Sparse Views", "MVDream: Multi-view Diffusion for 3D Generation"], "answer_arxiv_id": ["2211.10440", "2303.13873", "2305.16213", "2212.00774v1", "2306.07349", "2305.16411", "2307.01097", "2308.14078", "2308.16512"], "source_meta": {"published_time": "20231226"}, "qid": "AutoScholarQuery_test_687"}
{"question": "Which studies have discussed a two-step approach for Temporal Action Detection (TAD)?", "answer": ["BMN: Boundary-Matching Network for Temporal Action Proposal Generation", "BSN: Boundary Sensitive Network for Temporal Action Proposal Generation", "G-TAD: Sub-Graph Localization for Temporal Action Detection", "Video Self-Stitching Graph Network for Temporal Action Localization"], "answer_arxiv_id": ["1907.09702", "1806.02964", "1911.11462", "2011.14598"], "source_meta": {"published_time": "20231204"}, "qid": "AutoScholarQuery_test_688"}
{"question": "Could you provide me the study that constructed the emrKBQA dataset for patient-specific QA on MIMIC-III?", "answer": ["emrQA: A Large Corpus for Question Answering on Electronic Medical Records"], "answer_arxiv_id": ["1809.00732v1"], "source_meta": {"published_time": "20231028"}, "qid": "AutoScholarQuery_test_689"}
{"question": "What research papers are provided for additive valuations with binary marginals?", "answer": ["A Probabilistic Approach to Voting, Allocation, Matching, and Coalition Formation", "Fair Division with Binary Valuations: One Rule to Rule Them All"], "answer_arxiv_id": ["2002.10171", "2007.06073"], "source_meta": {"published_time": "20230828"}, "qid": "AutoScholarQuery_test_690"}
{"question": "What research studies propose changing the weight in convolutional layers according to input features using the attention mechanism?", "answer": ["CondConv: Conditionally Parameterized Convolutions for Efficient Inference", "Dynamic Neural Networks: A Survey"], "answer_arxiv_id": ["1904.04971", "2102.04906"], "source_meta": {"published_time": "20230402"}, "qid": "AutoScholarQuery_test_691"}
{"question": "Can you inform me about papers which suggested that pretraining on domain-specific text can enhance language model performance on related tasks?", "answer": ["SciBERT: A Pretrained Language Model for Scientific Text", "Domain-Specific Language Model Pretraining for Biomedical Natural\n  Language Processing", "LEGAL-BERT: The Muppets straight out of Law School", "BudgetLongformer: Can we Cheaply Pretrain a SotA Legal Language Model\n  From Scratch?"], "answer_arxiv_id": ["1903.10676", "2007.15779", "2010.02559", "2211.17135"], "source_meta": {"published_time": "20230603"}, "qid": "AutoScholarQuery_test_692"}
{"question": "Which papers have discussed Bound Propagation methods and analyzed the output bounds based on input bounds?", "answer": ["Semidefinite relaxations for certifying robustness to adversarial examples", "Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope", "Efficient Neural Network Robustness Certification with General Activation Functions", "Certifiable Robustness and Robust Training for Graph Convolutional Networks", "Collective Robustness Certificates: Exploiting Interdependence in Graph Neural Networks"], "answer_arxiv_id": ["1811.01057", "1711.00851", "1811.00866", "1906.12269", "2302.02829"], "source_meta": {"published_time": "20230925"}, "qid": "AutoScholarQuery_test_693"}
{"question": "What studies present the application of the measurement theory concepts to NLP where desirable model capabilities are unobservable constructs?", "answer": ["Evaluating Evaluation Metrics: A Framework for Analyzing NLG Evaluation\n  Metrics using Measurement Theory"], "answer_arxiv_id": ["2305.14889"], "source_meta": {"published_time": "20240613"}, "qid": "AutoScholarQuery_test_694"}
{"question": "Any works about continual pretraining for encoder-decoder LMs?", "answer": ["KILM: Knowledge Injection into Encoder-Decoder Language Models", "Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora"], "answer_arxiv_id": ["2302.09170", "2110.08534"], "source_meta": {"published_time": "20230615"}, "qid": "AutoScholarQuery_test_695"}
{"question": "Which study proposed to minimize a robust loss and find worst-case perturbation during training with projected gradient descent?", "answer": ["Towards Deep Learning Models Resistant to Adversarial Attacks"], "answer_arxiv_id": ["1706.06083"], "source_meta": {"published_time": "20230724"}, "qid": "AutoScholarQuery_test_696"}
{"question": "Which works have investigated the advantages of decoupling policy and value functions for generalization in RL?", "answer": ["Decoupling Value and Policy for Generalization in Reinforcement Learning", "Phasic Policy Gradient"], "answer_arxiv_id": ["2102.10330", "2009.04416"], "source_meta": {"published_time": "20230605"}, "qid": "AutoScholarQuery_test_697"}
{"question": "Which work considered the notion of envy-freeness up to any item (EFX) in fractionally subadditive valuations?", "answer": ["Almost Envy-Freeness with General Valuations"], "answer_arxiv_id": ["1707.04769"], "source_meta": {"published_time": "20230828"}, "qid": "AutoScholarQuery_test_698"}
{"question": "Which papers demonstrate story generation strategies for better coherency in story-telling using language models?", "answer": ["Plan-And-Write: Towards Better Automatic Storytelling"], "answer_arxiv_id": ["1811.05701"], "source_meta": {"published_time": "20240701"}, "qid": "AutoScholarQuery_test_699"}
{"question": "Which studies utilized the concept of query for different applications like detection of different objects, video instance segmentation, multiple object tracking, and video object detection?", "answer": ["End-to-End Object Detection with Transformers", "MOTR: End-to-End Multiple-Object Tracking with Transformer", "TrackFormer: Multi-Object Tracking with Transformers", "End-to-End Video Instance Segmentation with Transformers"], "answer_arxiv_id": ["2005.12872", "2105.03247", "2101.02702", "2011.14503"], "source_meta": {"published_time": "20240315"}, "qid": "AutoScholarQuery_test_700"}
{"question": "Can you list some research papers that adopted a similar formulation for advertising?", "answer": ["Send Mixed Signals – Earn More, Work Less", "Signaling Schemes for Revenue Maximization", "Screening with Persuasion"], "answer_arxiv_id": ["1202.1483", "1202.1590", "2212.03360v1"], "source_meta": {"published_time": "20230427"}, "qid": "AutoScholarQuery_test_701"}
{"question": "What papers use LLM to retrieve reasoning paths from KGs based on relation 'plans' grounded by KGs?", "answer": ["Reasoning on Graphs: Faithful and Interpretable Large Language Model\n  Reasoning"], "answer_arxiv_id": ["2310.01061"], "source_meta": {"published_time": "20240614"}, "qid": "AutoScholarQuery_test_702"}
{"question": "What study focused on examining node classification performance of one-layer GATs on a random graph model?", "answer": ["Graph Attention Retrospective"], "answer_arxiv_id": ["2202.13060"], "source_meta": {"published_time": "20230525"}, "qid": "AutoScholarQuery_test_703"}
{"question": "What works proposed variants on normalisation, which is complementary to unit scaling?", "answer": ["Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "Layer Normalization", "Proxy-Normalizing Activations to Match Batch Normalization while Removing Batch Dependence", "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks"], "answer_arxiv_id": ["1502.03167", "1607.06450", "2106.03743", "1602.07868"], "source_meta": {"published_time": "20230320"}, "qid": "AutoScholarQuery_test_704"}
{"question": "Which papers introduced algorithms regarding distributed optimization in a full participation setting using deterministic methods?", "answer": ["Communication Efficient Distributed Optimization using an Approximate Newton-type Method", "AIDE: Fast and Communication Efficient Distributed Optimization", "On Convergence of Distributed Approximate Newton Methods: Globalization, Sharper Bounds and Beyond", "Statistically Preconditioned Accelerated Gradient Method for Distributed Optimization", "An Accelerated Second-Order Method for Distributed Stochastic Optimization", "Newton Method over Networks is Fast up to the Statistical Precision"], "answer_arxiv_id": ["1312.7853", "1608.06879v1", "1908.02246", "2002.10726", "2103.14392", "2102.06780"], "source_meta": {"published_time": "20230415"}, "qid": "AutoScholarQuery_test_705"}
{"question": "Which works used alignment approaches to capture the feature of domain invariant characteristics?", "answer": ["CyCADA: Cycle-Consistent Adversarial Domain Adaptation", "Progressive Feature Alignment for Unsupervised Domain Adaptation", "FCNs in the Wild: Pixel-level Adversarial and Constraint-based\n  Adaptation", "Both Style and Distortion Matter: Dual-Path Unsupervised Domain\n  Adaptation for Panoramic Semantic Segmentation"], "answer_arxiv_id": ["1711.03213", "1811.08585", "1612.02649", "2303.14360"], "source_meta": {"published_time": "20240319"}, "qid": "AutoScholarQuery_test_706"}
{"question": "Which works extended the binary meta-learners in the CATE's estimation without a thorough theoretical analysis of their behaviour?", "answer": ["Uplift Modeling for Multiple Treatments with Cost Optimization"], "answer_arxiv_id": ["1908.05372"], "source_meta": {"published_time": "20220529"}, "qid": "AutoScholarQuery_test_707"}
{"question": "Can you specify the papers that introduced novel pre-training tasks for transformer-based pre-trained models?", "answer": ["CodeBERT: A Pre-Trained Model for Programming and Natural Languages", "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for\n  Code Understanding and Generation", "UniXcoder: Unified Cross-Modal Pre-training for Code Representation"], "answer_arxiv_id": ["2002.08155", "2109.00859", "2203.03850"], "source_meta": {"published_time": "20240109"}, "qid": "AutoScholarQuery_test_708"}
{"question": "Can you provide me some studies that use clustering or topic modeling to identify aspects in documents?", "answer": ["Modeling Online Reviews with Multi-grain Topic Models"], "answer_arxiv_id": ["0801.1063"], "source_meta": {"published_time": "20231114"}, "qid": "AutoScholarQuery_test_709"}
{"question": "What studies utilized VGG, ResNet, LSTM, and customized loss functions to enhance vision tasks in deep learning?", "answer": ["Very Deep Convolutional Networks for Large-Scale Image Recognition", "Deep Residual Learning for Image Recognition", "Image-based localization using LSTMs for structured feature correlation"], "answer_arxiv_id": ["1409.1556", "1512.03385", "1611.07890"], "source_meta": {"published_time": "20240328"}, "qid": "AutoScholarQuery_test_710"}
{"question": "What works are about data-driven LiDAR simulators?", "answer": ["LiDARsim: Realistic LiDAR Simulation by Leveraging the Real World", "Pattern-Aware Data Augmentation for LiDAR 3D Object Detection", "VISTA 2.0: An Open, Data-driven Simulator for Multimodal Sensing and\n  Policy Learning for Autonomous Vehicles", "Learning Interactive Driving Policies via Data-driven Simulation"], "answer_arxiv_id": ["2006.09348", "2112.00050", "2111.12083", "2111.12137"], "source_meta": {"published_time": "20240331"}, "qid": "AutoScholarQuery_test_711"}
{"question": "Can you specify some multimodal pretraining methods that require paired or interleaved data?", "answer": ["VLMo: Unified Vision-Language Pre-Training with\n  Mixture-of-Modality-Experts", "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision", "Image as a Foreign Language: BEiT Pretraining for All Vision and\n  Vision-Language Tasks", "Flamingo: a Visual Language Model for Few-Shot Learning"], "answer_arxiv_id": ["2111.02358", "2108.10904", "2208.10442", "2204.14198"], "source_meta": {"published_time": "20240125"}, "qid": "AutoScholarQuery_test_712"}
{"question": "What references mention the use of generative adversarial networks (GANs) in recent advancements in reconstruction from fMRI techniques?", "answer": ["Mind Reader: Reconstructing complex images from brain activities", "Reconstructing Natural Scenes from fMRI Patterns using BigBiGAN", "Reconstruction of Perceived Images from fMRI Patterns and Semantic Brain\n  Exploration using Instance-Conditioned GANs"], "answer_arxiv_id": ["2210.01769", "2001.11761", "2202.12692"], "source_meta": {"published_time": "20240329"}, "qid": "AutoScholarQuery_test_713"}
{"question": "Which papers discuss the two mainstream approaches for multi-hop KGQA: Semantic Parsing and Information Retrieval?", "answer": ["A Survey on Complex Knowledge Base Question Answering: Methods, Challenges and Solutions"], "answer_arxiv_id": ["2105.11644"], "source_meta": {"published_time": "20231024"}, "qid": "AutoScholarQuery_test_714"}
{"question": "Which publications introduced the maximum entropy exploration (maxEnt) framework?", "answer": ["Provably Efficient Maximum Entropy Exploration", "Task-Agnostic Exploration via Policy Gradient of a Non-Parametric State Entropy Estimate"], "answer_arxiv_id": ["1812.02690", "2007.04640"], "source_meta": {"published_time": "20230605"}, "qid": "AutoScholarQuery_test_715"}
{"question": "What papers recently gave attention to maximum entropy policies in the context of reinforcement learning (RL)?", "answer": ["Behavior From the Void: Unsupervised Active Pre-Training", "APS: Active Pretraining with Successor Features", "Reinforcement Learning with Prototypical Representations", "State Entropy Maximization with Random Encoders for Efficient Exploration", "Provably Efficient Maximum Entropy Exploration", "Task-Agnostic Exploration via Policy Gradient of a Non-Parametric State Entropy Estimate"], "answer_arxiv_id": ["2103.04551", "2108.13956", "2102.11271v2", "2102.09430", "1812.02690", "2007.04640"], "source_meta": {"published_time": "20230605"}, "qid": "AutoScholarQuery_test_716"}
{"question": "Which research papers have introduced more sophisticated network structures for the mapping between hazy and clear images in the context of image dehazing?", "answer": ["TransWeather: Transformer-based Restoration of Images Degraded by\n  Adverse Weather Conditions", "MB-TaylorFormer: Multi-branch Efficient Transformer Expanded by Taylor\n  Formula for Image Dehazing"], "answer_arxiv_id": ["2111.14813", "2308.14036"], "source_meta": {"published_time": "20240516"}, "qid": "AutoScholarQuery_test_717"}
{"question": "Which papers discuss that Large Language Models (LLMs) memorize data both from their original large training corpora and smaller private datasets used for downstream tasks?", "answer": ["Quantifying Memorization Across Neural Language Models", "Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy", "How BPE Affects Memorization in Transformers", "Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models", "Counterfactual Memorization in Neural Language Models", "Memorization in NLP Fine-tuning Methods"], "answer_arxiv_id": ["2202.07646", "2210.17546v3", "2110.02782", "2205.10770", "2112.12938", "2205.12506"], "source_meta": {"published_time": "20230524"}, "qid": "AutoScholarQuery_test_718"}
{"question": "Which papers exploited feature-space/n-gram discrepancy measures for selecting data in domain adaptation setting?", "answer": ["Learning to select data for transfer learning with Bayesian Optimization"], "answer_arxiv_id": ["1707.05246"], "source_meta": {"published_time": "20230206"}, "qid": "AutoScholarQuery_test_719"}
{"question": "Any works that employed a consistency-based method for confidence estimation?", "answer": ["Fine-tuning Language Models for Factuality"], "answer_arxiv_id": ["2311.08401"], "source_meta": {"published_time": "20240214"}, "qid": "AutoScholarQuery_test_720"}
{"question": "Could you mention a study that proposed to decouple the bi-level optimization of dataset condensation?", "answer": ["Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale\n  From A New Perspective"], "answer_arxiv_id": ["2306.13092"], "source_meta": {"published_time": "20231206"}, "qid": "AutoScholarQuery_test_721"}
{"question": "What research papers have been involved in the use of kernel fusion for efficient inference techniques in large language models (LLMs)?", "answer": ["FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"], "answer_arxiv_id": ["2205.14135"], "source_meta": {"published_time": "20230522"}, "qid": "AutoScholarQuery_test_722"}
{"question": "What works used Moore-Lewis selection in selection of examples?", "answer": ["Cynical Selection of Language Model Training Data", "Automatic Document Selection for Efficient Encoder Pretraining"], "answer_arxiv_id": ["1709.02279", "2210.10951"], "source_meta": {"published_time": "20230206"}, "qid": "AutoScholarQuery_test_723"}
{"question": "Which work first introduced the wait-k policy for simultaneous text translation?", "answer": ["STACL: Simultaneous Translation with Implicit Anticipation and Controllable Latency using Prefix-to-Prefix Framework"], "answer_arxiv_id": ["1810.08398v5"], "source_meta": {"published_time": "20230703"}, "qid": "AutoScholarQuery_test_724"}
{"question": "In which study did the researchers design the Poolingformer technique?", "answer": ["Poolingformer: Long Document Modeling with Pooling Attention"], "answer_arxiv_id": ["2105.04371"], "source_meta": {"published_time": "20240227"}, "qid": "AutoScholarQuery_test_725"}
{"question": "Which works studied the offline total variation denoising problem?", "answer": ["Adaptive piecewise polynomial estimation via trend filtering", "Trend Filtering on Graphs", "Adaptive Risk Bounds in Univariate Total Variation Denoising and Trend Filtering"], "answer_arxiv_id": ["1304.2986", "1410.7690", "1702.05113"], "source_meta": {"published_time": "20230531"}, "qid": "AutoScholarQuery_test_726"}
{"question": "Who initially introduced the Diffusion Generative Model?", "answer": ["Deep Unsupervised Learning using Nonequilibrium Thermodynamics"], "answer_arxiv_id": ["1503.03585"], "source_meta": {"published_time": "20230629"}, "qid": "AutoScholarQuery_test_727"}
{"question": "What studies have applied machine learning models for connectivity analysis on brain networks?", "answer": ["Machine Learning Methods for Brain Network Classification: Application to Autism Diagnosis using Cortical Morphological Networks", "Explainable Classification of Brain Networks via Contrast Subgraphs"], "answer_arxiv_id": ["2004.13321", "2006.05176"], "source_meta": {"published_time": "20221111"}, "qid": "AutoScholarQuery_test_728"}
{"question": "Could you provide me some works that involve the application of contrastive learning in diverse data modalities?", "answer": ["A Simple Framework for Contrastive Learning of Visual Representations", "Learning Transferable Visual Models From Natural Language Supervision"], "answer_arxiv_id": ["2002.05709", "2103.00020"], "source_meta": {"published_time": "20240313"}, "qid": "AutoScholarQuery_test_729"}
{"question": "What work introduced a K-Planes decomposition technique designed to reconstruct static 3D scenes and dynamic 4D videos?", "answer": ["K-Planes: Explicit Radiance Fields in Space, Time, and Appearance"], "answer_arxiv_id": ["2301.10241"], "source_meta": {"published_time": "20240212"}, "qid": "AutoScholarQuery_test_730"}
{"question": "In which papers did the researchers discuss the KRR in misspecified case?", "answer": ["Statistical Optimality of Stochastic Gradient Descent on Hard Learning Problems through Multiple Passes", "Sobolev Norm Learning Rates for Regularized Least-Squares Algorithms", "Analyzing the discrepancy principle for kernelized spectral filter learning algorithms", "Optimal Rates for Regularized Conditional Mean Embedding Learning"], "answer_arxiv_id": ["1805.10074", "1702.07254", "2004.08436v1", "2208.01711v3"], "source_meta": {"published_time": "20230512"}, "qid": "AutoScholarQuery_test_731"}
{"question": "Which papers implemented neural networks like CNNs and RNNs to enhance co-embedding methods?", "answer": ["A ConvNet for the 2020s", "Deep Residual Learning for Image Recognition", "Going Deeper with Convolutions", "Very Deep Convolutional Networks for Large-Scale Image Recognition", "Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term\n  Memory (LSTM) Network"], "answer_arxiv_id": ["2201.03545", "1512.03385", "1409.4842", "1409.1556", "1808.03314"], "source_meta": {"published_time": "20231204"}, "qid": "AutoScholarQuery_test_732"}
{"question": "Which papers approached the problem of Source-free Unsupervised Domain Adaptation (SFUDA) in the absence of source domain data?", "answer": ["Generalize then Adapt: Source-Free Domain Adaptive Semantic Segmentation", "Model Adaptation: Historical Contrastive Learning for Unsupervised\n  Domain Adaptation without Source Data"], "answer_arxiv_id": ["2108.11249", "2110.03374"], "source_meta": {"published_time": "20240319"}, "qid": "AutoScholarQuery_test_733"}
{"question": "Could you provide me some works about training convolutional networks for 6D Pose Estimation?", "answer": ["Mask R-CNN"], "answer_arxiv_id": ["1703.06870"], "source_meta": {"published_time": "20230227"}, "qid": "AutoScholarQuery_test_734"}
{"question": "Could you provide studies about ensemble methods for yielding pixel-wise uncertainty estimates?", "answer": ["Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles"], "answer_arxiv_id": ["1612.01474"], "source_meta": {"published_time": "20230328"}, "qid": "AutoScholarQuery_test_735"}
{"question": "What papers propose certified defenses which conduct a layer-by-layer analysis to derive the certified robustness guarantee of an unimodal model?", "answer": ["Provable defenses against adversarial examples via the convex outer\n  adversarial polytope", "On the Effectiveness of Interval Bound Propagation for Training\n  Verifiably Robust Models", "Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks"], "answer_arxiv_id": ["1711.00851", "1810.12715", "1702.01135"], "source_meta": {"published_time": "20240328"}, "qid": "AutoScholarQuery_test_736"}
{"question": "Could you provide me some studies that have addressed self-consistency in Large Language Models?", "answer": ["Self-Consistency Improves Chain of Thought Reasoning in Language Models"], "answer_arxiv_id": ["2203.11171"], "source_meta": {"published_time": "20230922"}, "qid": "AutoScholarQuery_test_737"}
{"question": "What datasets contain scenes where multiple people are performing various actions concurrently?", "answer": ["AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual\n  Actions", "The AVA-Kinetics Localized Human Actions Video Dataset", "MultiSports: A Multi-Person Video Dataset of Spatio-Temporally Localized\n  Sports Actions"], "answer_arxiv_id": ["1705.08421", "2005.00214", "2105.07404"], "source_meta": {"published_time": "20240406"}, "qid": "AutoScholarQuery_test_738"}
{"question": "What papers are about analyzing and understanding conventions in overcoming coordination problems?", "answer": ["Legible Normativity for AI Alignment: The Value of Silly Rules"], "answer_arxiv_id": ["1811.01267"], "source_meta": {"published_time": "20230615"}, "qid": "AutoScholarQuery_test_739"}
{"question": "Could you give an example of research papers on techniques for distinguishing uncertain areas as the dimension of the goal space increases, in the field of curriculum goal generation?", "answer": ["Uncertainty-Aware Anticipation of Activities", "BeLFusion: Latent Diffusion for Behavior-Driven Human Motion Prediction"], "answer_arxiv_id": ["1908.09540", "2211.14304"], "source_meta": {"published_time": "20221125"}, "qid": "AutoScholarQuery_test_740"}
{"question": "What are some studies about Operator Learning that leverage the Fourier transform?", "answer": ["Fourier Neural Operator for Parametric Partial Differential Equations", "Factorized Fourier Neural Operators"], "answer_arxiv_id": ["2010.08895", "2111.13802"], "source_meta": {"published_time": "20230125"}, "qid": "AutoScholarQuery_test_741"}
{"question": "What work proposed the eNCE method and identified its limitations?", "answer": ["Analyzing and Improving the Optimization Landscape of Noise-Contrastive Estimation"], "answer_arxiv_id": ["2110.11271"], "source_meta": {"published_time": "20230613"}, "qid": "AutoScholarQuery_test_742"}
{"question": "Which publications use autoregressive models for molecular structure design?", "answer": ["Symmetry-adapted generation of 3d point sets for the targeted discovery of molecules", "Generating equilibrium molecules with deep neural networks"], "answer_arxiv_id": ["1906.00957", "1810.11347"], "source_meta": {"published_time": "20230613"}, "qid": "AutoScholarQuery_test_743"}
{"question": "Could you provide me some studies exploring to make Gaussian diffusion faster and more data efficient in training?", "answer": ["Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models", "Efficient Diffusion Training via Min-SNR Weighting Strategy", "Fast Sampling of Diffusion Models via Operator Learning", "Fast Diffusion Model"], "answer_arxiv_id": ["2304.12526", "2303.09556", "2211.13449", "2306.06991"], "source_meta": {"published_time": "20230914"}, "qid": "AutoScholarQuery_test_744"}
{"question": "What research discusses the advantages of the PRM over the ORM in providing detailed feedback to enhance generators?", "answer": ["Fine-Grained Human Feedback Gives Better Rewards for Language Model\n  Training"], "answer_arxiv_id": ["2306.01693"], "source_meta": {"published_time": "20231214"}, "qid": "AutoScholarQuery_test_745"}
{"question": "What research studies constructed datasets targeting multiple image issues and proposed context schemes to better understand interleaved inputs?", "answer": ["MMICL: Empowering Vision-language Model with Multi-Modal In-Context\n  Learning", "Otter: A Multi-Modal Model with In-Context Instruction Tuning"], "answer_arxiv_id": ["2309.07915", "2305.03726"], "source_meta": {"published_time": "20240219"}, "qid": "AutoScholarQuery_test_746"}
{"question": "Can you name some studies that tackled the objective mismatch issue in RLHF learning schemes?", "answer": ["The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from\n  Human Feedback"], "answer_arxiv_id": ["2311.00168"], "source_meta": {"published_time": "20240530"}, "qid": "AutoScholarQuery_test_747"}
{"question": "Can you provide papers that discuss the important role of the noise distribution choice in success of Noise-contrastive estimation?", "answer": ["Generative Adversarial Nets", "Flow Contrastive Estimation of Energy-Based Models", "Analyzing and Improving the Optimization Landscape of Noise-Contrastive Estimation"], "answer_arxiv_id": ["1406.2661", "1912.00589", "2110.11271"], "source_meta": {"published_time": "20230613"}, "qid": "AutoScholarQuery_test_748"}
{"question": "Which paper introduced the slot attention for object-centric learning?", "answer": ["Object-Centric Learning with Slot Attention"], "answer_arxiv_id": ["2006.15055"], "source_meta": {"published_time": "20240228"}, "qid": "AutoScholarQuery_test_749"}
{"question": "Can you show me some papers that focus on improving sample efficiency with sub-optimal external policies?", "answer": ["KoGuN: Accelerating Deep Reinforcement Learning via Integrating Human Suboptimal Knowledge"], "answer_arxiv_id": ["2002.07418"], "source_meta": {"published_time": "20221007"}, "qid": "AutoScholarQuery_test_750"}
{"question": "Which works studied the concept of co-training involving separate models to generate improved pseudolabels?", "answer": ["Co-teaching: Robust Training of Deep Neural Networks with Extremely\n  Noisy Labels", "Co-training Improves Prompt-based Learning for Large Language Models"], "answer_arxiv_id": ["1804.06872", "2202.00828"], "source_meta": {"published_time": "20231115"}, "qid": "AutoScholarQuery_test_751"}
{"question": "Can you provide references regarding data-driven approaches for stereo-matching?", "answer": ["A Large Dataset to Train Convolutional Networks for Disparity, Optical\n  Flow, and Scene Flow Estimation", "Unifying Flow, Stereo and Depth Estimation", "Pyramid Stereo Matching Network", "GA-Net: Guided Aggregation Net for End-to-end Stereo Matching", "A Large Dataset to Train Convolutional Networks for Disparity, Optical\n  Flow, and Scene Flow Estimation"], "answer_arxiv_id": ["1512.02134", "2211.05783", "1803.08669", "1904.06587", "1512.02134"], "source_meta": {"published_time": "20240421"}, "qid": "AutoScholarQuery_test_752"}
{"question": "What papers focus on developing kernel methods for learning molecular potentials?", "answer": ["On representing chemical environments", "FCHL revisited: faster and more accurate quantum machine learning"], "answer_arxiv_id": ["1209.3140", "1909.01946"], "source_meta": {"published_time": "20230620"}, "qid": "AutoScholarQuery_test_753"}
{"question": "What early works utilize CNN-based image translation in portrait and face relighting?", "answer": ["Deep Shading: Convolutional Neural Networks for Screen-Space Shading", "Deferred Neural Rendering: Image Synthesis using Neural Textures"], "answer_arxiv_id": ["1603.06078", "1904.12356"], "source_meta": {"published_time": "20230411"}, "qid": "AutoScholarQuery_test_754"}
{"question": "Which papers discuss the application of specific criteria to remove weights in post-hoc pruning?", "answer": ["Dynamic Network Surgery for Efficient DNNs", "Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon", "Compressing Neural Networks using the Variational Information Bottleneck", "NISP: Pruning Networks using Neuron Importance Score Propagation", "Importance Estimation for Neural Network Pruning"], "answer_arxiv_id": ["1608.04493", "1705.07565", "1802.10399", "1711.05908", "1906.10771"], "source_meta": {"published_time": "20230228"}, "qid": "AutoScholarQuery_test_755"}
{"question": "Can you identify any works that aimed to improve computationally efficient FL with personalized local models using quantization and model parameter decoupling?", "answer": ["QuPeD: Quantized Personalization via Distillation with Applications to Federated Learning", "Efficient Split-Mix Federated Learning for On-Demand and In-Situ Customization", "HeteroFL: Computation and Communication Efficient Federated Learning for Heterogeneous Clients", "Exploiting Shared Representations for Personalized Federated Learning", "Achieving Personalized Federated Learning with Sparse Local Models"], "answer_arxiv_id": ["2107.13892", "2203.09747", "2010.01264", "2102.07078", "2201.11380"], "source_meta": {"published_time": "20230504"}, "qid": "AutoScholarQuery_test_756"}
{"question": "Which papers explore the application of parameter-efficient fine-tuning methods, such as Prefix-Tuning and LoRA?", "answer": ["Prefix-Tuning: Optimizing Continuous Prompts for Generation", "LoRA: Low-Rank Adaptation of Large Language Models"], "answer_arxiv_id": ["2101.00190", "2106.09685"], "source_meta": {"published_time": "20231114"}, "qid": "AutoScholarQuery_test_757"}
{"question": "Could you tell me about the studies that have proposed goal-driven clustering for personalising the grouping of text corpora?", "answer": ["Goal-Driven Explainable Clustering via Language Descriptions"], "answer_arxiv_id": ["2305.13749"], "source_meta": {"published_time": "20240215"}, "qid": "AutoScholarQuery_test_758"}
{"question": "Are there any studies that used a few-shot manner to train the CDR model?", "answer": ["Few-Shot Conversational Dense Retrieval", "History-Aware Conversational Dense Retrieval"], "answer_arxiv_id": ["2105.04166", "2401.16659"], "source_meta": {"published_time": "20240211"}, "qid": "AutoScholarQuery_test_759"}
{"question": "Can you list any studies that utilize differentiable logical rule learning", "answer": ["Embedding Entities and Relations for Learning and Inference in Knowledge Bases", "Differentiable Learning of Logical Rules for Knowledge Base Reasoning", "DRUM: End-To-End Differentiable Rule Mining On Knowledge Graphs", "DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning", "Variational Knowledge Graph Reasoning", "Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning", "Multi-Hop Knowledge Graph Reasoning with Reward Shaping", "M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search"], "answer_arxiv_id": ["1412.6575", "1702.08367", "1911.00055", "1707.06690", "1803.06581", "1711.05851", "1808.10568", "1802.04394"], "source_meta": {"published_time": "20230522"}, "qid": "AutoScholarQuery_test_760"}
{"question": "What studies provide insight into provably efficient exploration techniques in RL?", "answer": ["Model-based Reinforcement Learning and the Eluder Dimension", "Almost Optimal Model-Free Reinforcement Learning via Reference-Advantage Decomposition", "Policy Finetuning: Bridging Sample-Efficient Offline and Online Reinforcement Learning", "Learning Near Optimal Policies with Low Inherent Bellman Error", "Provably Efficient Reinforcement Learning with Linear Function Approximation"], "answer_arxiv_id": ["1406.1853", "2004.10019", "2106.04895", "2003.00153", "1907.05388"], "source_meta": {"published_time": "20220405"}, "qid": "AutoScholarQuery_test_761"}
{"question": "What work proposed a term-graph rewriting system for marginalizing a log joint density with conjugacy?", "answer": ["Autoconj: Recognizing and Exploiting Conjugacy Without a Domain-Specific Language"], "answer_arxiv_id": ["1811.11926"], "source_meta": {"published_time": "20230201"}, "qid": "AutoScholarQuery_test_762"}
{"question": "What studies discuss about adversarial attacks as potential safety risks in machine learning models?", "answer": ["Towards Deep Learning Models Resistant to Adversarial Attacks", "Theoretically Principled Trade-off between Robustness and Accuracy"], "answer_arxiv_id": ["1706.06083", "1901.08573"], "source_meta": {"published_time": "20230224"}, "qid": "AutoScholarQuery_test_763"}
{"question": "Which works used a simple peak detection algorithm to extract atomic coordinates from the generated voxel grids similar to the method used in the current study?", "answer": ["Learning a Continuous Representation of 3D Molecular Structures with Deep Generative Models"], "answer_arxiv_id": ["2010.08687"], "source_meta": {"published_time": "20230613"}, "qid": "AutoScholarQuery_test_764"}
{"question": "Which work focuses on 'Cause on Tape' (CoT) explanations where the explanation precedes the answer?", "answer": ["Language Models Don't Always Say What They Think: Unfaithful\n  Explanations in Chain-of-Thought Prompting"], "answer_arxiv_id": ["2305.04388"], "source_meta": {"published_time": "20231113"}, "qid": "AutoScholarQuery_test_765"}
{"question": "Which works have introduced domain adversarial methods to learn domain-invariant embeddings across the source domain and the target domain in unsupervised graph domain adaption problem?", "answer": ["DANE: Domain Adaptive Network Embedding"], "answer_arxiv_id": ["1906.00684"], "source_meta": {"published_time": "20231023"}, "qid": "AutoScholarQuery_test_766"}
{"question": "Which work first incorporated Differentiable Inductive Logic Programming to RL domain?", "answer": ["Neural Logic Reinforcement Learning"], "answer_arxiv_id": ["1904.10729"], "source_meta": {"published_time": "20230602"}, "qid": "AutoScholarQuery_test_767"}
{"question": "What papers propose 3D pretraining methods utilizing local augmentations?", "answer": ["Self-Supervised Pretraining of 3D Features on any Point-Cloud"], "answer_arxiv_id": ["2101.02691"], "source_meta": {"published_time": "20230325"}, "qid": "AutoScholarQuery_test_768"}
{"question": "Can you name the studies that looked at CVaR optimization using PG?", "answer": ["Optimizing the CVaR via Sampling", "EPOpt: Learning Robust Neural Network Policies Using Model Ensembles", "Learning Robust Options by Conditional Value at Risk Optimization"], "answer_arxiv_id": ["1404.3862", "1610.01283", "1905.09191"], "source_meta": {"published_time": "20230126"}, "qid": "AutoScholarQuery_test_769"}
{"question": "Are there any studies about the policy update of AMPO to mirror descent algorithm based on value iteration and Bellman operators?", "answer": ["A Theory of Regularized Markov Decision Processes"], "answer_arxiv_id": ["1901.11275"], "source_meta": {"published_time": "20230130"}, "qid": "AutoScholarQuery_test_770"}
{"question": "What papers have examined the effect of smaller design decisions like the loss function or policy regularization?", "answer": ["Revisiting Design Choices in Proximal Policy Optimization"], "answer_arxiv_id": ["2009.10897"], "source_meta": {"published_time": "20230602"}, "qid": "AutoScholarQuery_test_771"}
{"question": "Any works that developed representations of statistical and causal dependencies between latent factors and auxiliary variables?", "answer": ["Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA", "Weakly-Supervised Disentanglement Without Compromises", "Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style", "The Incomplete Rosetta Stone Problem: Identifiability Results for Multi-View Nonlinear ICA", "Nonlinear ICA Using Auxiliary Variables and Generalized Contrastive Learning", "Variational Autoencoders and Nonlinear ICA: A Unifying Framework", "ICE-BeeM: Identifiable Conditional Energy-Based Deep Models Based on Nonlinear ICA", "Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse Coding", "Contrastive Learning Inverts the Data Generating Process"], "answer_arxiv_id": ["1605.06336", "2002.02886", "2106.04619v4", "1905.06642", "1805.08651", "1907.04809", "2002.11537", "2007.10930", "2102.08850v4"], "source_meta": {"published_time": "20231108"}, "qid": "AutoScholarQuery_test_772"}
{"question": "What studies have demonstrated the effectiveness of contrastive methods in learning useful representations for downstream tasks?", "answer": ["Representation Learning with Contrastive Predictive Coding", "A Simple Framework for Contrastive Learning of Visual Representations", "Momentum Contrast for Unsupervised Visual Representation Learning", "Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss", "Representation Learning with Contrastive Predictive Coding", "Learning deep representations by mutual information estimation and maximization", "Learning Representations by Maximizing Mutual Information Across Views", "Contrastive Multiview Coding", "On Mutual Information Maximization for Representation Learning", "What Makes for Good Views for Contrastive Learning?", "Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere", "Representation Learning with Contrastive Predictive Coding", "Contrastive Learning Inverts the Data Generating Process", "Representation Learning with Contrastive Predictive Coding", "Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss", "Estimating divergence functionals and the likelihood ratio by convex risk minimization"], "answer_arxiv_id": ["1807.03748", "2002.05709", "1911.05722", "2106.04156", "1807.03748", "1808.06670", "1906.00910", "1906.05849", "1907.13625", "2005.10243", "2005.10242", "1807.03748", "2102.08850v4", "1807.03748", "2106.04156", "0809.0853"], "source_meta": {"published_time": "20231108"}, "qid": "AutoScholarQuery_test_773"}
{"question": "What research introduces competition-level problems integrating mathematical logic and background knowledge?", "answer": ["Measuring Mathematical Problem Solving With the MATH Dataset", "Mathematical Capabilities of ChatGPT", "Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For\n  Large Language Models"], "answer_arxiv_id": ["2103.03874", "2301.13867", "2305.15074"], "source_meta": {"published_time": "20240221"}, "qid": "AutoScholarQuery_test_774"}
{"question": "Could you provide examples of studies that use dense voxel grids in conjunction with shallow multilayer perceptrons to expedite 3D reconstruction?", "answer": ["Direct Voxel Grid Optimization: Super-fast Convergence for Radiance\n  Fields Reconstruction", "Improved Direct Voxel Grid Optimization for Radiance Fields\n  Reconstruction", "HexPlane: A Fast Representation for Dynamic Scenes"], "answer_arxiv_id": ["2111.11215", "2206.05085", "2301.09632"], "source_meta": {"published_time": "20240326"}, "qid": "AutoScholarQuery_test_775"}
{"question": "Could you provide me some studies about meta-learning in DG strategies?", "answer": ["Episodic Training for Domain Generalization", "Learning to Learn with Variational Information Bottleneck for Domain Generalization", "Learning to Generalize: Meta-Learning for Domain Generalization", "Learning to Generalize Unseen Domains via Memory-based Multi-Source Meta-Learning for Person Re-Identification"], "answer_arxiv_id": ["1902.00113", "2007.07645", "1710.03463", "2012.00417"], "source_meta": {"published_time": "20231101"}, "qid": "AutoScholarQuery_test_776"}
{"question": "What research study discusses protein docking methods such as EquiDock without prior knowledge of the epitope and the paratope?", "answer": ["Independent SE(3)-Equivariant Models for End-to-End Rigid Protein Docking"], "answer_arxiv_id": ["2111.07786"], "source_meta": {"published_time": "20230201"}, "qid": "AutoScholarQuery_test_777"}
{"question": "Could you provide me some papers that investigated adversarial perturbations in MRI and CT image reconstruction?", "answer": ["Solving Inverse Problems With Deep Neural Networks – Robustness Included?"], "answer_arxiv_id": ["2011.04268"], "source_meta": {"published_time": "20230724"}, "qid": "AutoScholarQuery_test_778"}
{"question": "Can you provide some studies dealing with the strong quadratic cost in Optimal Transport?", "answer": ["Optimal transport mapping via input convex neural networks", "2-Wasserstein Approximation via Restricted Convex Potentials with Application to Improved Training for GANs", "Wasserstein-2 Generative Networks"], "answer_arxiv_id": ["1908.10962", "1902.07197", "1909.13082v4"], "source_meta": {"published_time": "20220530"}, "qid": "AutoScholarQuery_test_779"}
{"question": "Which paper proposed a fully differentiable equivariant model that can predict coordinates of docked poses?", "answer": ["EquiBind: Geometric Deep Learning for Drug Binding Structure Prediction"], "answer_arxiv_id": ["2202.05146"], "source_meta": {"published_time": "20221012"}, "qid": "AutoScholarQuery_test_780"}
{"question": "Which studies have explored improved choices for the matrices in the continual counting context?", "answer": ["Almost Tight Error Bounds on Differentially Private Continual Counting"], "answer_arxiv_id": ["2211.05006"], "source_meta": {"published_time": "20230616"}, "qid": "AutoScholarQuery_test_781"}
{"question": "Which works are about using dynamic computation graph by skipping different blocks based on samples?", "answer": ["SkipNet: Learning Dynamic Routing in Convolutional Networks", "BlockDrop: Dynamic Inference Paths in Residual Networks"], "answer_arxiv_id": ["1711.09485", "1711.08393"], "source_meta": {"published_time": "20230402"}, "qid": "AutoScholarQuery_test_782"}
{"question": "Could you provide me some works that discuss improving model's robustness through data augmentation?", "answer": ["The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization", "AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty", "A simple way to make neural networks robust against diverse image corruptions", "AugMax: Adversarial Composition of Random Augmentations for Robust Training"], "answer_arxiv_id": ["2006.16241", "1912.02781", "2001.06057", "2110.13771"], "source_meta": {"published_time": "20230224"}, "qid": "AutoScholarQuery_test_783"}
{"question": "What research has been done on the interaction between the Bregman projected policy class and the expected Lipschitz and smooth policies?", "answer": ["A general sample complexity analysis of vanilla policy gradient", "An Improved Analysis of (Variance-Reduced) Policy Gradient and Natural Policy Gradient Methods"], "answer_arxiv_id": ["2107.11433v5", "2211.07937"], "source_meta": {"published_time": "20230130"}, "qid": "AutoScholarQuery_test_784"}
{"question": "Can you give me examples of LLM papers that use the MCP approach for evaluation?", "answer": ["Scaling Language Models: Methods, Analysis & Insights from Training Gopher", "Training Compute-Optimal Large Language Models"], "answer_arxiv_id": ["2112.11446", "2203.15556"], "source_meta": {"published_time": "20221022"}, "qid": "AutoScholarQuery_test_785"}
{"question": "In what papers were methods described that locate and edit the parameters and neurons in the LLMs in light of specific knowledge?", "answer": ["Locating and Editing Factual Associations in GPT", "Knowledge Neurons in Pretrained Transformers", "Mass-Editing Memory in a Transformer", "Editing a classifier by rewriting its prediction rules", "Transformer Feed-Forward Layers Build Predictions by Promoting Concepts\n  in the Vocabulary Space"], "answer_arxiv_id": ["2202.05262", "2104.08696", "2210.07229", "2112.01008", "2203.14680"], "source_meta": {"published_time": "20231114"}, "qid": "AutoScholarQuery_test_786"}
{"question": "What study proposed the method PatchFlow to solve the technical challenge of per-view detection of feature points in SfM?", "answer": ["Multi-View Optimization of Local Feature Geometry"], "answer_arxiv_id": ["2003.08348"], "source_meta": {"published_time": "20230627"}, "qid": "AutoScholarQuery_test_787"}
{"question": "Which papers discuss the use of Visual Instruction Fine-tuning in Large Multimodal Models?", "answer": ["Visual Instruction Tuning", "SVIT: Scaling up Visual Instruction Tuning"], "answer_arxiv_id": ["2304.08485", "2307.04087"], "source_meta": {"published_time": "20231127"}, "qid": "AutoScholarQuery_test_788"}
{"question": "What papers have employed use of persona prompts in LLMs?", "answer": ["Out of One, Many: Using Language Models to Simulate Human Samples", "Simulating Social Media Using Large Language Models to Evaluate\n  Alternative News Feed Algorithms"], "answer_arxiv_id": ["2209.06899", "2310.05984"], "source_meta": {"published_time": "20240216"}, "qid": "AutoScholarQuery_test_789"}
{"question": "Which work introduced the notion of (ρ,μ)-competitiveness?", "answer": ["Learning-Augmented Dynamic Power Management with Multiple States via New Ski Rental Bounds"], "answer_arxiv_id": ["2110.13116"], "source_meta": {"published_time": "20221006"}, "qid": "AutoScholarQuery_test_790"}
{"question": "What research investigates higher-order grammatical feature representation across languages using probing classifiers trained on mBERT embeddings?", "answer": ["Deep Subjecthood: Higher-Order Grammatical Features in Multilingual BERT"], "answer_arxiv_id": ["2101.11043"], "source_meta": {"published_time": "20240523"}, "qid": "AutoScholarQuery_test_791"}
{"question": "Which studies explored techniques in learning process design to address biases and instability in in-context learning?", "answer": ["Noisy Channel Language Model Prompting for Few-Shot Text Classification", "MetaICL: Learning to Learn In Context"], "answer_arxiv_id": ["2108.04106", "2110.15943"], "source_meta": {"published_time": "20240627"}, "qid": "AutoScholarQuery_test_792"}
{"question": "Which works have studied the underlying representations in diffusion models and proposed using them for various downstream tasks?", "answer": ["Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation", "Label-Efficient Semantic Segmentation with Diffusion Models"], "answer_arxiv_id": ["2211.12572", "2112.03126"], "source_meta": {"published_time": "20230523"}, "qid": "AutoScholarQuery_test_793"}
{"question": "What studies proposed gradient-guided autoregressive models by using the decoder’s activations as a gradient-friendly latent space?", "answer": ["Plug and Play Language Models: a Simple Approach to Controlled Text Generation", "Fudge: Controlled Text Generation With Future Discriminators"], "answer_arxiv_id": ["1912.02164", "2104.05218"], "source_meta": {"published_time": "20230531"}, "qid": "AutoScholarQuery_test_794"}
{"question": "Which research papers report on response-based distillation methods for object detection?", "answer": ["Distilling the Knowledge in a Neural Network"], "answer_arxiv_id": ["1503.02531"], "source_meta": {"published_time": "20220529"}, "qid": "AutoScholarQuery_test_795"}
{"question": "Are there any studies based on using pre-trained language model agents for role-play in text-based games?", "answer": ["Learning to Speak and Act in a Fantasy Text Adventure Game", "Pre-trained Language Models as Prior Knowledge for Playing Text-based\n  Games", "Exploring Large Language Models for Communication Games: An Empirical\n  Study on Werewolf"], "answer_arxiv_id": ["1903.03094", "2107.08408", "2309.04658"], "source_meta": {"published_time": "20240701"}, "qid": "AutoScholarQuery_test_796"}
{"question": "Which works combined models used for ARA with tradition linguistic features?", "answer": ["Linguistic Features for Readability Assessment", "BERT Embeddings for Automatic Readability Assessment"], "answer_arxiv_id": ["2006.00377", "2106.07935"], "source_meta": {"published_time": "20240603"}, "qid": "AutoScholarQuery_test_797"}
{"question": "Which paper distills pre-trained Stable Diffusion using Score Distillation Sampling (SDS) to extract a Neural Radiance Field (NeRF) from a given text prompt?", "answer": ["DreamFusion: Text-to-3D using 2D Diffusion"], "answer_arxiv_id": ["2209.14988"], "source_meta": {"published_time": "20231226"}, "qid": "AutoScholarQuery_test_798"}
{"question": "What papers focus on extending the signal-prediction framework to forecast aggregation settings?", "answer": ["The Wisdom of the Crowd and Higher-Order Beliefs"], "answer_arxiv_id": ["2102.02666"], "source_meta": {"published_time": "20231212"}, "qid": "AutoScholarQuery_test_799"}
{"question": "Which works were pertinent in the development of the Large Multimodal Models?", "answer": ["Exploring the Limits of Transfer Learning with a Unified Text-to-Text\n  Transformer", "PaLM: Scaling Language Modeling with Pathways", "UL2: Unifying Language Learning Paradigms", "Learning Transferable Visual Models From Natural Language Supervision", "BLIP: Bootstrapping Language-Image Pre-training for Unified\n  Vision-Language Understanding and Generation", "CyCLIP: Cyclic Contrastive Language-Image Pretraining"], "answer_arxiv_id": ["1910.10683", "2204.02311", "2205.05131", "2103.00020", "2201.12086", "2205.14459"], "source_meta": {"published_time": "20231127"}, "qid": "AutoScholarQuery_test_800"}
{"question": "Are there any articles proposing methods to evaluate the complexity of the hierarchical structure of a graph?", "answer": ["Structural Entropy Guided Graph Hierarchical Pooling", "A Simple yet Effective Method for Graph Classification"], "answer_arxiv_id": ["2206.13510", "2206.02404"], "source_meta": {"published_time": "20230508"}, "qid": "AutoScholarQuery_test_801"}
{"question": "Which papers discuss the role of Transformers in the field of NLP?", "answer": ["Attention Is All You Need"], "answer_arxiv_id": ["1706.03762"], "source_meta": {"published_time": "20221022"}, "qid": "AutoScholarQuery_test_802"}
{"question": "Which papers describe the basic annotation approach in creating NLI data?", "answer": ["A large annotated corpus for learning natural language inference", "A Broad-Coverage Challenge Corpus for Sentence Understanding through\n  Inference"], "answer_arxiv_id": ["1508.05326", "1704.05426"], "source_meta": {"published_time": "20240219"}, "qid": "AutoScholarQuery_test_803"}
{"question": "Could you provide references about the PixelHelp dataset which includes task goals and step-by-step instructions for Android?", "answer": ["Mapping Natural Language Instructions to Mobile UI Action Sequences"], "answer_arxiv_id": ["2005.03776"], "source_meta": {"published_time": "20230719"}, "qid": "AutoScholarQuery_test_804"}
{"question": "What studies utilize first-order logic to inspect and improve model's logical consistency in synthetic compositional reasoning tasks?", "answer": ["Logic-LM: Empowering Large Language Models with Symbolic Solvers for\n  Faithful Logical Reasoning", "LINC: A Neurosymbolic Approach for Logical Reasoning by Combining\n  Language Models with First-Order Logic Provers", "FaiRR: Faithful and Robust Deductive Reasoning over Natural Language", "Learning Symbolic Rules for Reasoning in Quasi-Natural Language"], "answer_arxiv_id": ["2305.12295", "2310.15164", "2203.10261", "2111.12038"], "source_meta": {"published_time": "20231129"}, "qid": "AutoScholarQuery_test_805"}
{"question": "Which studies propose interpolation-based mixup methods for graph augmentations?", "answer": ["G-Mixup: Graph Data Augmentation for Graph Classification", "Graph Transplant: Node Saliency-Guided Graph Mixup with Local Structure Preservation"], "answer_arxiv_id": ["2202.07179", "2111.05639"], "source_meta": {"published_time": "20220226"}, "qid": "AutoScholarQuery_test_806"}
{"question": "What research provided theoretical studies on the convergence of the LocalSGDM algorithm?", "answer": ["On the Linear Speedup Analysis of Communication Efficient Momentum SGD for Distributed Non-Convex Optimization"], "answer_arxiv_id": ["1905.03817"], "source_meta": {"published_time": "20230420"}, "qid": "AutoScholarQuery_test_807"}
{"question": "What works present GALOIS framework and its sketch setting?", "answer": ["GALOIS: Boosting Deep Reinforcement Learning via Generalizable Logic Synthesis"], "answer_arxiv_id": ["2205.13728"], "source_meta": {"published_time": "20230602"}, "qid": "AutoScholarQuery_test_808"}
{"question": "Can you provide me some researches that support sensor simulation in the context of autonomous driving perception?", "answer": ["CARLA: An Open Urban Driving Simulator", "Block-NeRF: Scalable Large Scene Neural View Synthesis", "SUMMIT: A Simulator for Urban Driving in Massive Mixed Traffic", "VISTA 2.0: An Open, Data-driven Simulator for Multimodal Sensing and Policy Learning for Autonomous Vehicles"], "answer_arxiv_id": ["1711.03938", "2202.05263", "1911.04074", "2111.12083"], "source_meta": {"published_time": "20231012"}, "qid": "AutoScholarQuery_test_809"}
{"question": "What are some of the recent research papers that achieved success in solving downstream language tasks using finetuning methods?", "answer": ["BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "XLNet: Generalized Autoregressive Pretraining for Language Understanding"], "answer_arxiv_id": ["1810.04805", "1909.11942", "1907.11692", "1906.08237"], "source_meta": {"published_time": "20230219"}, "qid": "AutoScholarQuery_test_810"}
{"question": "Who proposed a GNN framework arising from a mixture of parabolic and hyperbolic PDEs on graphs with convolutional coupling operators?", "answer": ["PDE-GCN: Novel Architectures for Graph Neural Networks Motivated by Partial Differential Equations"], "answer_arxiv_id": ["2108.01938"], "source_meta": {"published_time": "20221002"}, "qid": "AutoScholarQuery_test_811"}
{"question": "Which works have utilized gating in classical recurrent neural network (RNN) architectures such as LSTM and GRU?", "answer": ["Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"], "answer_arxiv_id": ["1406.1078"], "source_meta": {"published_time": "20221002"}, "qid": "AutoScholarQuery_test_812"}
{"question": "Can you provide papers that explored novel ways of training CNNs for FGVC?", "answer": ["Channel DropBlock: An Improved Regularization Method for Fine-Grained Visual Classification"], "answer_arxiv_id": ["2106.03432"], "source_meta": {"published_time": "20231024"}, "qid": "AutoScholarQuery_test_813"}
{"question": "What studies proposed deep learning models for protein sequence design using structure-based generative models?", "answer": ["Protein Structure and Sequence Generation with Equivariant Denoising Diffusion Probabilistic Models", "Learning from Protein Structure with Geometric Vector Perceptrons", "Multi-level Protein Representation Learning for Blind Mutational Effect Prediction"], "answer_arxiv_id": ["2205.15019", "2009.01411", "2306.04899"], "source_meta": {"published_time": "20230629"}, "qid": "AutoScholarQuery_test_814"}
{"question": "Which work proposed the model-free algorithm MOFFLE for low-nonnegative-rank MDPs in the field of reward-free reinforcement learning?", "answer": ["Model-free Representation Learning and Exploration in Low-rank MDPs"], "answer_arxiv_id": ["2102.07035"], "source_meta": {"published_time": "20220628"}, "qid": "AutoScholarQuery_test_815"}
{"question": "What papers utilize the Monte Carlo Dropout method in variational Bayesian methods for approximating the intractable integrals arising in Bayesian inference?", "answer": ["Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning", "Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference"], "answer_arxiv_id": ["1506.02142", "1506.02158"], "source_meta": {"published_time": "20230328"}, "qid": "AutoScholarQuery_test_816"}
{"question": "Which studies extended the work of cage-based deformation in NeRF editing?", "answer": ["Interactive Geometry Editing of Neural Radiance Fields"], "answer_arxiv_id": ["2303.11537"], "source_meta": {"published_time": "20240103"}, "qid": "AutoScholarQuery_test_817"}
{"question": "What studies discuss the training of Pre-trained Language Models(PLMs) for predicting masked words?", "answer": ["Mask More and Mask Later: Efficient Pre-training of Masked Language Models by Disentangling the [MASK] Token", "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "MASS: Masked Sequence to Sequence Pre-training for Language Generation", "Unsupervised Cross-lingual Representation Learning at Scale", "Cross-lingual Language Model Pretraining"], "answer_arxiv_id": ["2211.04898", "1810.04805", "1907.11692", "1910.13461", "1905.02450", "1911.02116", "1901.07291"], "source_meta": {"published_time": "20230601"}, "qid": "AutoScholarQuery_test_818"}
{"question": "What research focused on constructing a purpose-driven affordance dataset?", "answer": ["One-Shot Affordance Detection"], "answer_arxiv_id": ["2106.14747"], "source_meta": {"published_time": "20231026"}, "qid": "AutoScholarQuery_test_819"}
{"question": "What works have been done in the field of image editing in computer vision?", "answer": ["Plug-and-Play Diffusion Features for Text-Driven Image-to-Image\n  Translation", "Expressive Text-to-Image Generation with Rich Text"], "answer_arxiv_id": ["2211.12572", "2304.06720"], "source_meta": {"published_time": "20231211"}, "qid": "AutoScholarQuery_test_820"}
{"question": "Could you provide me some works that focus on source domain data estimation in the context of SFUDA?", "answer": ["Source-Free Domain Adaptation for Semantic Segmentation"], "answer_arxiv_id": ["2103.16372"], "source_meta": {"published_time": "20240319"}, "qid": "AutoScholarQuery_test_821"}
{"question": "Which work combines the semantic and instance segmentation tasks effectively?", "answer": ["Per-Pixel Classification is Not All You Need for Semantic Segmentation", "Masked-attention Mask Transformer for Universal Image Segmentation"], "answer_arxiv_id": ["2107.06278", "2112.01527"], "source_meta": {"published_time": "20230227"}, "qid": "AutoScholarQuery_test_822"}
{"question": "Which studies presented automatic machine learning (AutoML) approaches?", "answer": ["Auto-Pytorch: Multi-Fidelity MetaLearning for Efficient and Robust AutoDL"], "answer_arxiv_id": ["2006.13799"], "source_meta": {"published_time": "20230208"}, "qid": "AutoScholarQuery_test_823"}
{"question": "Which papers discussed federated learning in the context of autonomous driving?", "answer": ["FedDrive: Generalizing Federated Learning to Semantic Segmentation in\n  Autonomous Driving", "Deep Federated Learning for Autonomous Driving"], "answer_arxiv_id": ["2202.13670", "2110.05754"], "source_meta": {"published_time": "20240401"}, "qid": "AutoScholarQuery_test_824"}
{"question": "Could you provide me studies that propose approaches to unite segmentation datasets from multiple domains?", "answer": ["MSeg: A Composite Dataset for Multi-domain Semantic Segmentation", "Multi-dataset Pretraining: A Unified Model for Semantic Segmentation"], "answer_arxiv_id": ["2112.13762", "2106.04121"], "source_meta": {"published_time": "20230227"}, "qid": "AutoScholarQuery_test_825"}
{"question": "Which papers have investigated the simplicity bias in Deep Neural Networks (DNNs)?", "answer": ["SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data", "The Implicit Bias of Gradient Descent on Separable Data", "Implicit Bias of Gradient Descent on Linear Convolutional Networks", "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness", "The Origins and Prevalence of Texture Bias in Convolutional Neural Networks"], "answer_arxiv_id": ["1710.10174", "1710.10345", "1806.00468", "1811.12231", "1911.09071"], "source_meta": {"published_time": "20230323"}, "qid": "AutoScholarQuery_test_826"}
{"question": "What works propose to utilize the MVS approach using cost volume for better occlusion handling in generalizable view synthesis?", "answer": ["MVSNeRF: Fast Generalizable Radiance Field Reconstruction from\n  Multi-View Stereo", "GeoNeRF: Generalizing NeRF with Geometry Priors", "Neural Rays for Occlusion-aware Image-based Rendering"], "answer_arxiv_id": ["2103.15595", "2111.13539", "2107.13421"], "source_meta": {"published_time": "20240421"}, "qid": "AutoScholarQuery_test_827"}
{"question": "Which papers discuss earlier datasets focused on sign language recognition using isolated signs?", "answer": ["MS-ASL: A Large-Scale Data Set and Benchmark for Understanding American\n  Sign Language", "Word-level Deep Sign Language Recognition from Video: A New Large-scale\n  Dataset and Methods Comparison"], "answer_arxiv_id": ["1812.01053", "1910.11006"], "source_meta": {"published_time": "20231205"}, "qid": "AutoScholarQuery_test_828"}
{"question": "Any works that propose a view synthesis framework using two images with small overlapped regions?", "answer": ["Learning to Render Novel Views from Wide-Baseline Stereo Pairs"], "answer_arxiv_id": ["2304.08463"], "source_meta": {"published_time": "20240421"}, "qid": "AutoScholarQuery_test_829"}
{"question": "Which papers proposed a multi-stage language model for TTS with phonemes as input and acoustic tokens as output?", "answer": ["Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers"], "answer_arxiv_id": ["2301.02111"], "source_meta": {"published_time": "20240603"}, "qid": "AutoScholarQuery_test_830"}
{"question": "Any studies about enhancing PLMs through pretraining on domain-relevant documents?", "answer": ["BARTScore: Evaluating Generated Text as Text Generation"], "answer_arxiv_id": ["2106.11520"], "source_meta": {"published_time": "20240224"}, "qid": "AutoScholarQuery_test_831"}
{"question": "What research works give a gradient-based approach to interpretability methods?", "answer": ["Layer-wise Relevance Propagation for Neural Networks with Local\n  Renormalization Layers", "Axiomatic Attribution for Deep Networks"], "answer_arxiv_id": ["1604.00825", "1703.01365"], "source_meta": {"published_time": "20231113"}, "qid": "AutoScholarQuery_test_832"}
{"question": "Which works have utilized the relationship between multiple images and between network layers in FGVC?", "answer": ["Cross-X Learning for Fine-Grained Visual Categorization"], "answer_arxiv_id": ["1909.04412"], "source_meta": {"published_time": "20231024"}, "qid": "AutoScholarQuery_test_833"}
{"question": "Which research proposed the Conditional Mutual Information (CMI) approach for learning disentangled representations?", "answer": ["Disentanglement and Generalization Under Correlation Shifts"], "answer_arxiv_id": ["2112.14754"], "source_meta": {"published_time": "20230523"}, "qid": "AutoScholarQuery_test_834"}
{"question": "Could you provide me some studies about delayed sampling which uses automatic marginalization to improve inference?", "answer": ["Delayed Sampling and Automatic Rao-Blackwellization of Probabilistic Programs", "Automated learning with a probabilistic programming language: Birch", "Pyro: Deep Universal Probabilistic Programming", "Functional Tensors for Probabilistic Programming", "Tensor Variable Elimination for Plated Factor Graphs", "Reactive Probabilistic Programming", "Semi-Symbolic Inference for Efficient Streaming Probabilistic Programming"], "answer_arxiv_id": ["1708.07787v2", "1810.01539", "1810.09538", "1910.10775v2", "1902.03210", "1908.07563v2", "2209.07490v2"], "source_meta": {"published_time": "20230201"}, "qid": "AutoScholarQuery_test_835"}
{"question": "Can you name some studies that use Recurrent Neural Network or Graph Neural Network for improving the geometry shapes of building extraction results?", "answer": ["Topological Map Extraction from Overhead Images", "PolyWorld: Polygonal Building Extraction with Graph Neural Networks in\n  Satellite Images"], "answer_arxiv_id": ["1812.01497", "2111.15491"], "source_meta": {"published_time": "20240407"}, "qid": "AutoScholarQuery_test_836"}
{"question": "What studies examined the performance issues in machine translation models?", "answer": ["Analyzing Uncertainty in Neural Machine Translation"], "answer_arxiv_id": ["1803.00047"], "source_meta": {"published_time": "20230219"}, "qid": "AutoScholarQuery_test_837"}
{"question": "Which studies are about leveraging demonstrations into the policy-update steps of Reinforcement Learning?", "answer": ["Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations", "Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards", "Overcoming Exploration in Reinforcement Learning with Demonstrations", "Reinforced Imitation: Sample Efficient Deep Reinforcement Learning for Map-less Navigation by Leveraging Prior Demonstrations", "Integrating Behavior Cloning and Reinforcement Learning for Improved Performance in Dense and Sparse Reward Environments"], "answer_arxiv_id": ["1709.10087", "1707.08817", "1709.10089", "1805.07095", "1910.04281"], "source_meta": {"published_time": "20221007"}, "qid": "AutoScholarQuery_test_838"}
{"question": "Can you cite the research papers that investigated the use of six IMUs for full-body motion estimation?", "answer": ["Sparse Inertial Poser: Automatic 3D Human Pose Estimation from Sparse\n  IMUs", "Deep Inertial Poser: Learning to Reconstruct Human Pose from Sparse\n  Inertial Measurements in Real Time", "TransPose: Real-time 3D Human Translation and Pose Estimation with Six\n  Inertial Sensors", "Physical Inertial Poser (PIP): Physics-aware Real-time Human Motion\n  Tracking from Sparse Inertial Sensors"], "answer_arxiv_id": ["1703.08014", "1810.04703", "2105.04605", "2203.08528"], "source_meta": {"published_time": "20240306"}, "qid": "AutoScholarQuery_test_839"}
{"question": "Which work marked a significant development in Vision-Language Pre-training (VLP) and has trained encoders on a large amount of data?", "answer": ["Learning Transferable Visual Models From Natural Language Supervision"], "answer_arxiv_id": ["2103.00020"], "source_meta": {"published_time": "20230227"}, "qid": "AutoScholarQuery_test_840"}
{"question": "What papers are about contextualized models for encoding word meaning in the LSC task?", "answer": ["Explaining and Improving BERT Performance on Lexical Semantic Change\n  Detection"], "answer_arxiv_id": ["2103.07259"], "source_meta": {"published_time": "20240605"}, "qid": "AutoScholarQuery_test_841"}
{"question": "Could you mention any works that proposed the concept of learning EBM by using a ConvNet as the energy function?", "answer": ["A Theory of Generative ConvNet"], "answer_arxiv_id": ["1602.03264v3"], "source_meta": {"published_time": "20230613"}, "qid": "AutoScholarQuery_test_842"}
{"question": "Which investigation proposes a method that mitigates the issue of model bias and generalization in zero-/few-shot anomaly detection?", "answer": ["Catching Both Gray and Black Swans: Open-set Supervised Anomaly\n  Detection", "Explicit Boundary Guided Semi-Push-Pull Contrastive Learning for\n  Supervised Anomaly Detection"], "answer_arxiv_id": ["2203.14506", "2207.01463"], "source_meta": {"published_time": "20240319"}, "qid": "AutoScholarQuery_test_843"}
{"question": "Who introduced the approach of dataset condensation that leverages gradient-based hyper-parameter optimization?", "answer": ["Dataset Distillation"], "answer_arxiv_id": ["1811.10959"], "source_meta": {"published_time": "20231021"}, "qid": "AutoScholarQuery_test_844"}
{"question": "What works have used frame averaging to produce equivariant output from non-equivariant architecture backbones?", "answer": ["Frame Averaging for Invariant and Equivariant Network Design", "Frame Averaging for Equivariant Shape Space Learning", "FAENet: Frame Averaging Equivariant GNN for Materials Modeling"], "answer_arxiv_id": ["2110.03336", "2112.01741", "2305.05577"], "source_meta": {"published_time": "20230517"}, "qid": "AutoScholarQuery_test_845"}
{"question": "What papers focus on HMT techniques using depth sensors?", "answer": ["Real-time RGBD-based Extended Body Pose Estimation", "RobustFusion: Robust Volumetric Performance Reconstruction under\n  Human-object Interactions from Monocular RGBD Stream", "DoubleFusion: Real-time Capture of Human Performances with Inner Body\n  Shapes from a Single Depth Sensor"], "answer_arxiv_id": ["2103.03663", "2104.14837", "1804.06023"], "source_meta": {"published_time": "20240306"}, "qid": "AutoScholarQuery_test_846"}
{"question": "What study modifies questions and answers by counterfactual presupposition in VQAv2 for a new challenging scenario for complementary MLLMs?", "answer": ["Making the V in VQA Matter: Elevating the Role of Image Understanding in\n  Visual Question Answering"], "answer_arxiv_id": ["1612.00837"], "source_meta": {"published_time": "20231010"}, "qid": "AutoScholarQuery_test_847"}
{"question": "Can you mention any work about Out-of-distribution (OOD) generalization on graphs?", "answer": ["Out-Of-Distribution Generalization on Graphs: A Survey", "Shift-Robust GNNs: Overcoming the Limitations of Localized Graph Training Data"], "answer_arxiv_id": ["2202.07987", "2108.01099"], "source_meta": {"published_time": "20231023"}, "qid": "AutoScholarQuery_test_848"}
{"question": "Can you tell me about the studies that propose tuning methods for CLIP?", "answer": ["Learning Transferable Visual Models From Natural Language Supervision"], "answer_arxiv_id": ["2103.00020"], "source_meta": {"published_time": "20230604"}, "qid": "AutoScholarQuery_test_849"}
{"question": "Which research papers focus on addressing label-shift scenarios with open-set domain adaptation (OSDA), partial-set domain adaptation (PDA), and open-partial-set domain adaptation (OPDA)?", "answer": ["Open Set Domain Adaptation by Backpropagation", "Partial Transfer Learning with Selective Adversarial Networks", "Learning to Transfer Examples for Partial Domain Adaptation", "OVANet: One-vs-All Network for Universal Domain Adaptation"], "answer_arxiv_id": ["1804.10427", "1707.07901", "1903.12230", "2104.03344"], "source_meta": {"published_time": "20240306"}, "qid": "AutoScholarQuery_test_850"}
{"question": "What research studies use hard pseudolabels from teachers to train student models in the outcontext of low-resource semi-supervised sequence generation?", "answer": ["Sequence-Level Knowledge Distillation", "Is GPT-3 a Good Data Annotator?", "GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation", "Want To Reduce Labeling Cost? GPT-3 Can Help", "ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks", "Large Language Models Are Reasoning Teachers"], "answer_arxiv_id": ["1606.07947", "2212.10450", "2104.08826", "2108.13487", "2303.15056", "2212.10071"], "source_meta": {"published_time": "20231115"}, "qid": "AutoScholarQuery_test_851"}
{"question": "What papers dealt with the classical transduction setting, a special case of semi-supervised learning?", "answer": ["Learning by Transduction"], "answer_arxiv_id": ["1301.7375v1"], "source_meta": {"published_time": "20230427"}, "qid": "AutoScholarQuery_test_852"}
{"question": "Which papers introduce an asymmetric mechanism within the Multi-Agent Debate framework?", "answer": ["Encouraging Divergent Thinking in Large Language Models through\n  Multi-Agent Debate", "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate"], "answer_arxiv_id": ["2305.19118", "2308.07201"], "source_meta": {"published_time": "20240228"}, "qid": "AutoScholarQuery_test_853"}
{"question": "Which works applied data augmentation strategy or generative adversarial networks (GANs) in Domain Generalization (DG) to handle the domain shift?", "answer": ["Towards Recognizing Unseen Categories in Unseen Domains", "Learning to Generate Novel Domains for Domain Generalization"], "answer_arxiv_id": ["2007.12256", "2007.03304"], "source_meta": {"published_time": "20231219"}, "qid": "AutoScholarQuery_test_854"}
{"question": "Are there any references that proposed the first general gradient inversion method?", "answer": ["Deep Leakage from Gradients"], "answer_arxiv_id": ["1906.08935"], "source_meta": {"published_time": "20230531"}, "qid": "AutoScholarQuery_test_855"}
{"question": "Can you give examples of studies that have provided physics questions in multiple-choice format and require multistep reasoning?", "answer": ["Measuring Massive Multitask Language Understanding", "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for\n  Foundation Models", "Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For\n  Large Language Models"], "answer_arxiv_id": ["2009.03300", "2305.08322", "2305.15074"], "source_meta": {"published_time": "20240221"}, "qid": "AutoScholarQuery_test_856"}
{"question": "Are there any studies on vision-language models that train to generate text autoregressively?", "answer": ["Long-term Recurrent Convolutional Networks for Visual Recognition and Description", "Show and Tell: A Neural Image Caption Generator", "Unifying Vision-and-Language Tasks via Text Generation", "Answer-Me: Multi-Task Learning for Generalization to Many Question-Answering Tasks"], "answer_arxiv_id": ["1411.4389", "1411.4555", "2102.02779", "2205.00949"], "source_meta": {"published_time": "20220914"}, "qid": "AutoScholarQuery_test_857"}
{"question": "Which work leveraged orthogonal transformations to avoid the direct computation of Jacobian determinants?", "answer": ["Improving Variational Auto-Encoders using Householder Flow"], "answer_arxiv_id": ["1611.09630"], "source_meta": {"published_time": "20230524"}, "qid": "AutoScholarQuery_test_858"}
{"question": "What studies contribute to formality in cross-style learning?", "answer": ["Dear Sir or Madam, May I introduce the GYAFC Dataset: Corpus, Benchmarks\n  and Metrics for Formality Style Transfer"], "answer_arxiv_id": ["1803.06535"], "source_meta": {"published_time": "20230524"}, "qid": "AutoScholarQuery_test_859"}
{"question": "Can you provide some references that introduce code filtering strategies where functional correctness prediction is achieved without code execution?", "answer": ["Fault-Aware Neural Code Rankers", "LEVER: Learning to Verify Language-to-Code Generation with Execution", "Coder Reviewer Reranking for Code Generation"], "answer_arxiv_id": ["2206.03865", "2302.08468", "2211.16490"], "source_meta": {"published_time": "20240802"}, "qid": "AutoScholarQuery_test_860"}
{"question": "Could you provide examples of research employ additional convolutional modules to learn a hierarchical feature space?", "answer": ["B-CNN: Branch Convolutional Neural Network for Hierarchical\n  Classification", "Visual Tree Convolutional Neural Network in Image Classification"], "answer_arxiv_id": ["1709.09890", "1906.01536"], "source_meta": {"published_time": "20230604"}, "qid": "AutoScholarQuery_test_861"}
{"question": "Could you provide me some studies that focus on modelling the user's state alongside the strategies in ESC systems?", "answer": ["Improving Multi-turn Emotional Support Dialogue Generation with\n  Lookahead Strategy Planning", "Knowledge-enhanced Memory Model for Emotional Support Conversation"], "answer_arxiv_id": ["2210.04242", "2310.07700"], "source_meta": {"published_time": "20240220"}, "qid": "AutoScholarQuery_test_862"}
{"question": "Are there any studies on decoupling the modeling of the environment using a semantic map from the end-to-end network?", "answer": ["Learning To Explore Using Active Neural SLAM"], "answer_arxiv_id": ["2004.05155"], "source_meta": {"published_time": "20230203"}, "qid": "AutoScholarQuery_test_863"}
{"question": "Could you provide the study that reevaluated the methods of [bib.bibx36] and reported small robustness gains compared to adversarial training?", "answer": ["MagNet and “Efficient Defenses Against Adversarial Attacks” are Not Robust to Adversarial Examples"], "answer_arxiv_id": ["1711.08478"], "source_meta": {"published_time": "20230724"}, "qid": "AutoScholarQuery_test_864"}
{"question": "Which papers discuss multi-agent autonomous driving simulators which use real driving data to initialize scenarios and logged behavior?", "answer": ["Nocturne: a scalable driving benchmark for bringing multi-agent learning one step closer to the real world", "MetaDrive: Composing Diverse Driving Scenarios for Generalizable Reinforcement Learning", "nuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles"], "answer_arxiv_id": ["2206.09889", "2109.12674", "2106.11810"], "source_meta": {"published_time": "20231012"}, "qid": "AutoScholarQuery_test_865"}
{"question": "What studies used cross-entropy method for both optimization and sampling in standard RL?", "answer": ["Efficient Risk-Averse Reinforcement Learning"], "answer_arxiv_id": ["2205.05138"], "source_meta": {"published_time": "20230126"}, "qid": "AutoScholarQuery_test_866"}
{"question": "What work can recover Lipschitz contextual bandits, despite having suboptimal dynamic regret bounds?", "answer": ["A Kernel-Based Approach to Non-Stationary Reinforcement Learning in Metric Spaces"], "answer_arxiv_id": ["2007.05078v2"], "source_meta": {"published_time": "20230711"}, "qid": "AutoScholarQuery_test_867"}
{"question": "What works are about text/speech balloon detection in manga?", "answer": ["Object Detection for Comics using Manga109 Annotations", "COO: Comic Onomatopoeia Dataset for Recognizing Arbitrary or Truncated\n  Texts"], "answer_arxiv_id": ["1803.08670", "2207.04675"], "source_meta": {"published_time": "20240118"}, "qid": "AutoScholarQuery_test_868"}
{"question": "What works have explored prompting and style conversion for generative data augmentation in low-resource NLP?", "answer": ["ZeroGen: Efficient Zero-shot Learning via Dataset Generation", "PromptMix: A Class Boundary Augmentation Method for Large Language Model\n  Distillation", "Style Transfer as Data Augmentation: A Case Study on Named Entity\n  Recognition"], "answer_arxiv_id": ["2202.07922", "2310.14192", "2210.07916"], "source_meta": {"published_time": "20240606"}, "qid": "AutoScholarQuery_test_869"}
{"question": "Can you cite studies showcasing the effectiveness of LLM-powered data augmentation in cross-lingual commonsense reasoning?", "answer": ["LLM-powered Data Augmentation for Enhanced Cross-lingual Performance"], "answer_arxiv_id": ["2305.14288"], "source_meta": {"published_time": "20240209"}, "qid": "AutoScholarQuery_test_870"}
{"question": "Could you provide me the work that conducted analysis on the embedding layer of mT5 and XLM-R?", "answer": ["Hyperpolyglot LLMs: Cross-Lingual Interpretability in Token Embeddings"], "answer_arxiv_id": ["2311.18034"], "source_meta": {"published_time": "20240523"}, "qid": "AutoScholarQuery_test_871"}
{"question": "Which works use Graph Neural Networks and Recurrent Neural Networks to update encodings in temporal graph learning?", "answer": ["Structured Sequence Modeling with Graph Convolutional Recurrent Networks", "T-GCN: A Temporal Graph Convolutional Network for Traffic Prediction", "ROLAND: Graph Learning Framework for Dynamic Graphs", "CS-TGN: Community Search via Temporal Graph Neural Networks", "Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks", "Anomaly Detection in Multiplex Dynamic Networks: from Blockchain Security to Brain Disease Prediction"], "answer_arxiv_id": ["1612.07659", "1811.05320", "2208.07239", "2303.08964", "1908.01207", "2211.08378"], "source_meta": {"published_time": "20230619"}, "qid": "AutoScholarQuery_test_872"}
{"question": "Which papers solved classification and detection problems in LiDAR perception using deep learning?", "answer": ["Revisiting Point Cloud Shape Classification with a Simple and Effective Baseline", "Benchmarking and Analyzing Point Cloud Classification under Corruptions", "PointCLIP: Point Cloud Understanding by CLIP", "PointPillars: Fast Encoders for Object Detection from Point Clouds", "PV-RCNN++: Point-Voxel Feature Set Abstraction With Local Vector Representation for 3D Object Detection", "Pseudo-LiDAR++: Accurate Depth for 3D Object Detection in Autonomous Driving"], "answer_arxiv_id": ["2106.05304", "2202.03377", "2112.02413", "1812.05784", "2102.00463", "1906.06310"], "source_meta": {"published_time": "20231031"}, "qid": "AutoScholarQuery_test_873"}
{"question": "Could you provide the work that developed an off-policy algorithm capable of computing the inner integral analytically?", "answer": ["Expected Policy Gradients for Reinforcement Learning"], "answer_arxiv_id": ["1801.03326v2"], "source_meta": {"published_time": "20221024"}, "qid": "AutoScholarQuery_test_874"}
{"question": "What works propose to make use of loss functions to handle known symmetries in object pose prediction?", "answer": ["PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes", "Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation"], "answer_arxiv_id": ["1711.00199", "1901.02970"], "source_meta": {"published_time": "20230227"}, "qid": "AutoScholarQuery_test_875"}
{"question": "Which studies evaluated the generalization of an RL agent by training and testing them in totally different environments?", "answer": ["Quantifying Generalization in Reinforcement Learning", "Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design"], "answer_arxiv_id": ["1812.02341", "2012.02096"], "source_meta": {"published_time": "20230426"}, "qid": "AutoScholarQuery_test_876"}
{"question": "Which work showed that the ranking of attention scores computed by a GAT layer is unconditioned on the query node?", "answer": ["How Attentive are Graph Attention Networks?"], "answer_arxiv_id": ["2105.14491"], "source_meta": {"published_time": "20230525"}, "qid": "AutoScholarQuery_test_877"}
{"question": "What studies introduced a strategy for normalizing multi-modal attributes to ensure consistency across modalities?", "answer": ["Attribute-Consistent Knowledge Graph Representation Learning for\n  Multi-Modal Entity Alignment"], "answer_arxiv_id": ["2304.01563"], "source_meta": {"published_time": "20240723"}, "qid": "AutoScholarQuery_test_878"}
{"question": "Which papers have created datasets to support research in human behavior understanding?", "answer": ["Towards Automatic Learning of Procedures from Web Instructional Videos", "UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild"], "answer_arxiv_id": ["1703.09788", "1212.0402"], "source_meta": {"published_time": "20231020"}, "qid": "AutoScholarQuery_test_879"}
{"question": "What work demonstrated the faster learning and better generalization when NTK-target alignment is high?", "answer": ["What can linearized neural networks actually say about generalization?"], "answer_arxiv_id": ["2106.06770"], "source_meta": {"published_time": "20230128"}, "qid": "AutoScholarQuery_test_880"}
{"question": "What research has been conducted on Memory-efficient training methods for reducing the memory footprint during the training process?", "answer": ["Reversible Vision Transformers", "Reformer: The Efficient Transformer", "Training Deep Nets with Sublinear Memory Cost"], "answer_arxiv_id": ["2302.04869", "2001.04451", "1604.06174"], "source_meta": {"published_time": "20230601"}, "qid": "AutoScholarQuery_test_881"}
{"question": "What research focuses on distilling reasoning processes?", "answer": ["Learning by Distilling Context"], "answer_arxiv_id": ["2209.15189"], "source_meta": {"published_time": "20230615"}, "qid": "AutoScholarQuery_test_882"}
{"question": "What research focuses on manipulating visual features to target the issue of image-text isolation?", "answer": ["InstructBLIP: Towards General-purpose Vision-Language Models with\n  Instruction Tuning", "Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large\n  Language Models"], "answer_arxiv_id": ["2305.06500", "2305.15023"], "source_meta": {"published_time": "20240219"}, "qid": "AutoScholarQuery_test_883"}
{"question": "Which studies focused on bottom-up methods in instance segmentation in 3D perception?", "answer": ["OccuSeg: Occupancy-aware 3D Instance Segmentation", "Hierarchical Aggregation for 3D Instance Segmentation", "3D-SIS: 3D Semantic Instance Segmentation of RGB-D Scans", "Language-Grounded Indoor 3D Semantic Segmentation in the Wild", "Instance Segmentation in 3D Scenes using Semantic Superpoint Tree\n  Networks"], "answer_arxiv_id": ["2003.06537v3", "2108.02350", "1812.07003", "2204.07761", "2108.07478"], "source_meta": {"published_time": "20230325"}, "qid": "AutoScholarQuery_test_884"}
{"question": "Any work discusses unigram similarity metrics related to the downstream performance?", "answer": ["Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks"], "answer_arxiv_id": ["2004.10964"], "source_meta": {"published_time": "20230206"}, "qid": "AutoScholarQuery_test_885"}
{"question": "What papers incorporate rationalization methodologies into supervised NLI models to enhance their resilience against adversarial datasets?", "answer": ["Can Rationalization Improve Robustness?", "Supervising Model Attention with Human Explanations for Robust Natural\n  Language Inference"], "answer_arxiv_id": ["2204.11790", "2104.08142"], "source_meta": {"published_time": "20231113"}, "qid": "AutoScholarQuery_test_886"}
{"question": "What studies provide solutions for feature matching in low-textured regions using dense or semi-dense matching methods?", "answer": ["Learning Accurate Dense Correspondences and When to Trust Them", "Neighbourhood Consensus Networks", "Dual-Resolution Correspondence Networks", "LoFTR: Detector-Free Local Feature Matching with Transformers", "Quadtree Attention for Vision Transformers", "ASpanFormer: Detector-Free Image Matching with Adaptive Span Transformer", "MatchFormer: Interleaving Attention in Transformers for Feature Matching"], "answer_arxiv_id": ["2101.01710", "1810.10510", "2006.08844", "2104.00680", "2201.02767", "2208.14201", "2203.09645"], "source_meta": {"published_time": "20230627"}, "qid": "AutoScholarQuery_test_887"}
{"question": "Who has defined simplicity bias based on the number of linear components to define a decision boundary and studied its effects?", "answer": ["The Pitfalls of Simplicity Bias in Neural Networks"], "answer_arxiv_id": ["2006.07710"], "source_meta": {"published_time": "20230323"}, "qid": "AutoScholarQuery_test_888"}
{"question": "Which works proposed the extension of 3D Gaussian Splatting technique to dynamic scenes?", "answer": ["4D Gaussian Splatting for Real-Time Dynamic Scene Rendering", "Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis", "Real-time Photorealistic Dynamic Scene Representation and Rendering with\n  4D Gaussian Splatting"], "answer_arxiv_id": ["2310.08528", "2308.09713", "2310.10642"], "source_meta": {"published_time": "20231226"}, "qid": "AutoScholarQuery_test_889"}
{"question": "Any works discussed about the introduction of additional layers into the model architecture instead of updating a large number of model parameters?", "answer": ["Differentially Private Fine-tuning of Language Models"], "answer_arxiv_id": ["2110.06500"], "source_meta": {"published_time": "20230524"}, "qid": "AutoScholarQuery_test_890"}
{"question": "Which publications discuss HMT methods utilizing ego-centric views?", "answer": ["UnrealEgo: A New Dataset for Robust Egocentric 3D Human Motion Capture", "Scene-aware Egocentric 3D Human Pose Estimation", "Estimating Egocentric 3D Human Pose in Global Space", "Ego-Body Pose Estimation via Ego-Head Pose Estimation"], "answer_arxiv_id": ["2208.01633", "2212.11684", "2104.13454", "2212.04636"], "source_meta": {"published_time": "20240306"}, "qid": "AutoScholarQuery_test_891"}
{"question": "Which studies focus on the integration of contrastive signals in dataset condensation?", "answer": ["Dataset Condensation with Contrastive Signals"], "answer_arxiv_id": ["2202.02916"], "source_meta": {"published_time": "20231021"}, "qid": "AutoScholarQuery_test_892"}
{"question": "Which studies focused on using synthetic data to create new datasets or augment existing ones?", "answer": ["FlowNet: Learning Optical Flow with Convolutional Networks", "Playing for Data: Ground Truth from Computer Games", "VisDA: The Visual Domain Adaptation Challenge", "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning", "Structured3D: A Large Photo-realistic Dataset for Structured 3D Modeling", "ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation"], "answer_arxiv_id": ["1504.06852", "1608.02192v1", "1710.06924", "1612.06890", "1908.00222", "2007.04954"], "source_meta": {"published_time": "20230719"}, "qid": "AutoScholarQuery_test_893"}
{"question": "Which works employed contrastive learning for graph representation learning?", "answer": ["Deep Graph Infomax", "InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization", "Graph Contrastive Learning with Augmentations", "Deep Graph Contrastive Representation Learning", "GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training"], "answer_arxiv_id": ["1809.10341", "1908.01000", "2010.13902", "2006.04131", "2006.09963"], "source_meta": {"published_time": "20230622"}, "qid": "AutoScholarQuery_test_894"}
{"question": "What papers leverage pre-trained object detection models to extract image regional features offline for training multi-modal transformers?", "answer": ["Large-Scale Adversarial Training for Vision-and-Language Representation\n  Learning"], "answer_arxiv_id": ["2006.06195"], "source_meta": {"published_time": "20231219"}, "qid": "AutoScholarQuery_test_895"}
{"question": "Which study formally introduced reward-free exploration for tabular MDP?", "answer": ["Reward-Free Exploration for Reinforcement Learning"], "answer_arxiv_id": ["2002.02794"], "source_meta": {"published_time": "20220628"}, "qid": "AutoScholarQuery_test_896"}
{"question": "What research proposed the gradient-sliding method for addressing separated structure in optimization problems?", "answer": ["Gradient Sliding for Composite Optimization"], "answer_arxiv_id": ["1406.0919v2"], "source_meta": {"published_time": "20230415"}, "qid": "AutoScholarQuery_test_897"}
{"question": "Any studies that work on alignment of LLMs evaluators to human evaluation standards?", "answer": ["Calibrating LLM-Based Evaluator"], "answer_arxiv_id": ["2309.13308"], "source_meta": {"published_time": "20240224"}, "qid": "AutoScholarQuery_test_898"}
{"question": "Which works offer end-to-end methods for multimodal Language Models?", "answer": ["Flamingo: a Visual Language Model for Few-Shot Learning", "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image\n  Encoders and Large Language Models", "InstructBLIP: Towards General-purpose Vision-Language Models with\n  Instruction Tuning", "BLIP: Bootstrapping Language-Image Pre-training for Unified\n  Vision-Language Understanding and Generation", "BEiT: BERT Pre-Training of Image Transformers", "Image as a Foreign Language: BEiT Pretraining for All Vision and\n  Vision-Language Tasks", "Visual Instruction Tuning", "mPLUG-Owl: Modularization Empowers Large Language Models with\n  Multimodality", "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large\n  Language Models", "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init\n  Attention", "Otter: A Multi-Modal Model with In-Context Instruction Tuning", "OFA: Unifying Architectures, Tasks, and Modalities Through a Simple\n  Sequence-to-Sequence Learning Framework", "PaLI: A Jointly-Scaled Multilingual Language-Image Model"], "answer_arxiv_id": ["2204.14198", "2301.12597", "2305.06500", "2201.12086", "2106.08254", "2208.10442", "2304.08485", "2304.14178", "2304.10592", "2303.16199", "2305.03726", "2202.03052", "2209.06794"], "source_meta": {"published_time": "20231013"}, "qid": "AutoScholarQuery_test_899"}
{"question": "Could you tell me the works related to representer point selection for estimating training data influence?", "answer": ["Representer Point Selection for Explaining Deep Neural Networks", "Adapting and Evaluating Influence-Estimation Methods for Gradient-Boosted Decision Trees"], "answer_arxiv_id": ["1811.09720", "2205.00359"], "source_meta": {"published_time": "20230531"}, "qid": "AutoScholarQuery_test_900"}
{"question": "Which works are responsible for the introduction of unsupervised models for perceptual grouping?", "answer": ["On the Binding Problem in Artificial Neural Networks"], "answer_arxiv_id": ["2012.05208"], "source_meta": {"published_time": "20230524"}, "qid": "AutoScholarQuery_test_901"}
{"question": "Can you name some examples of projects that integrated machine learning, particularly LLMs, into automated theorem proving?", "answer": ["Learning to Reason in Large Theories without Imitation", "Constructions in combinatorics via neural networks", "LeanDojo: Theorem Proving with Retrieval-Augmented Language Models", "Generative Language Modeling for Automated Theorem Proving", "Proof Artifact Co-training for Theorem Proving with Language Models", "NaturalProofs: Mathematical Theorem Proving in Natural Language", "Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal\n  Proofs"], "answer_arxiv_id": ["1905.10501", "2104.14516", "2306.15626", "2009.03393", "2102.06203", "2104.01112", "2210.12283"], "source_meta": {"published_time": "20240410"}, "qid": "AutoScholarQuery_test_902"}
{"question": "Could you provide me some works centered on multilingual machine translation?", "answer": ["Beyond English-Centric Multilingual Machine Translation"], "answer_arxiv_id": ["2010.11125"], "source_meta": {"published_time": "20240209"}, "qid": "AutoScholarQuery_test_903"}
{"question": "What studies have used techniques like residual structure, skip connection, and dropout in basic CNN frameworks for image restoration?", "answer": ["Accurate Image Super-Resolution Using Very Deep Convolutional Networks", "Plug-and-Play Image Restoration with Deep Denoiser Prior", "Image Super-Resolution Using Very Deep Residual Channel Attention Networks", "Residual Dense Network for Image Restoration", "Reflash Dropout in Image Super-Resolution"], "answer_arxiv_id": ["1511.04587", "2008.13751", "1807.02758", "1812.10477", "2112.12089"], "source_meta": {"published_time": "20221004"}, "qid": "AutoScholarQuery_test_904"}
{"question": "Are there studies which focus on refining generated code through iterative reviews and improvements based on execution results?", "answer": ["Reflexion: Language Agents with Verbal Reinforcement Learning"], "answer_arxiv_id": ["2303.11366"], "source_meta": {"published_time": "20240802"}, "qid": "AutoScholarQuery_test_905"}
{"question": "Which works have been done on representation learning on hypergraphs?", "answer": ["Hypergraph Neural Networks", "A Survey on Hyperlink Prediction"], "answer_arxiv_id": ["1809.09401", "2207.02911v1"], "source_meta": {"published_time": "20230619"}, "qid": "AutoScholarQuery_test_906"}
{"question": "Which studies showed that the fine-tuning of large vision-language foundational models with a few examples from the target dataset can enhance performance?", "answer": ["Conditional Prompt Learning for Vision-Language Models", "Learning to Prompt for Vision-Language Models", "Visual Prompt Tuning", "MaPLe: Multi-modal Prompt Learning"], "answer_arxiv_id": ["2203.05557", "2109.01134", "2203.12119", "2210.03117"], "source_meta": {"published_time": "20230604"}, "qid": "AutoScholarQuery_test_907"}
{"question": "Could you provide me some examples of works that introduced a parallel iterative routing?", "answer": ["Capsules with Inverted Dot-Product Attention Routing"], "answer_arxiv_id": ["2002.04764"], "source_meta": {"published_time": "20240320"}, "qid": "AutoScholarQuery_test_908"}
{"question": "Which papers extended the ideas from non-stationary multi-armed bandits to various contextual bandit settings?", "answer": ["Learning Contextual Bandits in a Non-stationary Environment", "Efficient Contextual Bandits in Non-stationary Worlds", "A New Algorithm for Non-stationary Contextual Bandits: Efficient, Optimal, and Parameter-free", "Non-stationary Reinforcement Learning without Prior Knowledge: An Optimal Black-box Approach"], "answer_arxiv_id": ["1805.09365", "1708.01799v4", "1902.00980v3", "2102.05406"], "source_meta": {"published_time": "20230711"}, "qid": "AutoScholarQuery_test_909"}
{"question": "What papers discussed techniques on disentanglement based on Variation Autoencoder (VAE)?", "answer": ["Auto-Encoding Variational Bayes"], "answer_arxiv_id": ["1312.6114"], "source_meta": {"published_time": "20230523"}, "qid": "AutoScholarQuery_test_910"}
{"question": "Which papers cover the convergence of PFL on homogeneous data?", "answer": ["Tighter Theory for Local SGD on Identical and Heterogeneous Data"], "answer_arxiv_id": ["1909.04746v4"], "source_meta": {"published_time": "20231106"}, "qid": "AutoScholarQuery_test_911"}
{"question": "What papers made advancements with hybrid pose-based methods in deep learning?", "answer": ["Camera Relocalization by Computing Pairwise Relative Poses Using\n  Convolutional Neural Network"], "answer_arxiv_id": ["1707.09733"], "source_meta": {"published_time": "20240328"}, "qid": "AutoScholarQuery_test_912"}
{"question": "What prior works discussed random walk-based unsupervised representation learning methods?", "answer": ["node2vec: Scalable Feature Learning for Networks"], "answer_arxiv_id": ["1607.00653"], "source_meta": {"published_time": "20231003"}, "qid": "AutoScholarQuery_test_913"}
{"question": "Which research provides learning methods for object-level anomaly detection during meta-training?", "answer": ["Few-Shot One-Class Classification via Meta-Learning"], "answer_arxiv_id": ["2007.04146"], "source_meta": {"published_time": "20230215"}, "qid": "AutoScholarQuery_test_914"}
{"question": "Which papers proposed the Preference Ranking Optimization as an alternative to PPO?", "answer": ["Preference Ranking Optimization for Human Alignment"], "answer_arxiv_id": ["2306.17492"], "source_meta": {"published_time": "20240530"}, "qid": "AutoScholarQuery_test_915"}
{"question": "Any works about combining NeRFs by inserting objects into pre-existing NeRF scenes?", "answer": ["Control-NeRF: Editable Feature Volumes for Scene Rendering and\n  Manipulation"], "answer_arxiv_id": ["2204.10850"], "source_meta": {"published_time": "20240103"}, "qid": "AutoScholarQuery_test_916"}
{"question": "What studies require an additional boundedness assumption when fρ* falls into a less-smooth interpolation space?", "answer": ["Optimal Convergence for Distributed Learning with Stochastic Gradient Methods and Spectral Algorithms", "Sobolev Norm Learning Rates for Regularized Least-Squares Algorithms", "Optimal Learning Rates for Regularized Least-Squares with a Fourier Capacity Condition", "Optimal Rates for Regularized Conditional Mean Embedding Learning"], "answer_arxiv_id": ["1801.07226", "1702.07254", "2204.07856v4", "2208.01711v3"], "source_meta": {"published_time": "20230512"}, "qid": "AutoScholarQuery_test_917"}
{"question": "Which research papers adopted a 3D-Unet architecture to produce video volumes directly from an input image?", "answer": ["Stochastic Adversarial Video Prediction", "Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation", "AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models\n  without Specific Tuning", "Stochastic Image-to-Video Synthesis using cINNs", "MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and\n  Interpolation", "Diffusion Models for Video Prediction and Infilling"], "answer_arxiv_id": ["1804.01523", "2307.06940", "2307.04725", "2105.04551", "2205.09853", "2206.07696"], "source_meta": {"published_time": "20230914"}, "qid": "AutoScholarQuery_test_918"}
{"question": "Who first explored the food bank problem in the context of additive valuations?", "answer": ["Online Fair Division: analysing a Food Bank problem"], "answer_arxiv_id": ["1502.07571"], "source_meta": {"published_time": "20230828"}, "qid": "AutoScholarQuery_test_919"}
{"question": "Which paper proposed an enhanced Key-Value Memory neural network for Information Retrieval-based methods?", "answer": ["Key-Value Memory Networks for Directly Reading Documents"], "answer_arxiv_id": ["1606.03126"], "source_meta": {"published_time": "20231024"}, "qid": "AutoScholarQuery_test_920"}
{"question": "What works studied enabling more robust operation for certain tasks through recovering explicit localization information in model representations?", "answer": ["Perceptual Grouping in Contrastive Vision-Language Models"], "answer_arxiv_id": ["2210.09996"], "source_meta": {"published_time": "20240411"}, "qid": "AutoScholarQuery_test_921"}
{"question": "Are there any existing datasets that concentrate on the novel use of word meanings and omit conventional examples?", "answer": ["Metaphorical Polysemy Detection: Conventional Metaphor meets Word Sense\n  Disambiguation"], "answer_arxiv_id": ["2212.08395"], "source_meta": {"published_time": "20240605"}, "qid": "AutoScholarQuery_test_922"}
{"question": "Which papers present the use of ordinary and partial differential equations in designing, interpreting, and analyzing graph machine learning architectures?", "answer": ["Discrete and Continuous Deep Residual Learning Over Graphs", "Graph Neural Ordinary Differential Equations", "Continuous Graph Neural Networks"], "answer_arxiv_id": ["1911.09554", "1911.07532", "1912.00967"], "source_meta": {"published_time": "20221002"}, "qid": "AutoScholarQuery_test_923"}
{"question": "Which papers focus on distributed optimization using stochastic methods through client sampling?", "answer": ["Faster federated optimization under second-order similarity"], "answer_arxiv_id": ["2209.02257"], "source_meta": {"published_time": "20230415"}, "qid": "AutoScholarQuery_test_924"}
{"question": "What work explores learning an anomaly detector in the feature space with a category-agnostic model?", "answer": ["Registration based Few-Shot Anomaly Detection"], "answer_arxiv_id": ["2207.07361"], "source_meta": {"published_time": "20230215"}, "qid": "AutoScholarQuery_test_925"}
{"question": "Which papers propose first-order methods for efficiently solving min-max optimization problems in Weak Minty Variational Inequalities?", "answer": ["The Complexity of Constrained Min-Max Optimization", "Efficient Methods for Structured Nonconvex-Nonconcave Min-Max Optimization", "Escaping limit cycles: Global convergence for constrained nonconvex-nonconcave minimax problems", "Fast Extra Gradient Methods for Smooth Structured Nonconvex-Nonconcave Minimax Problems", "Solving Nonconvex-Nonconcave Min-Max Problems exhibiting Weak Minty Solutions", "Solving stochastic weak Minty variational inequalities without increasing batch size"], "answer_arxiv_id": ["2009.09623", "2011.00364", "2302.09831", "2106.02326", "2201.12247", "2302.09029"], "source_meta": {"published_time": "20230227"}, "qid": "AutoScholarQuery_test_926"}
{"question": "Could you tell me which papers introduced RL from AI Feedback training approach in the context of RLHF?", "answer": ["Constitutional AI: Harmlessness from AI Feedback"], "answer_arxiv_id": ["2212.08073"], "source_meta": {"published_time": "20240530"}, "qid": "AutoScholarQuery_test_927"}
{"question": "Which works propose methods for character re-identification in manga?", "answer": ["Unsupervised Manga Character Re-identification via Face-body and\n  Spatial-temporal Associated Clustering", "Identity-Aware Semi-Supervised Learning for Comic Character\n  Re-Identification"], "answer_arxiv_id": ["2204.04621", "2308.09096"], "source_meta": {"published_time": "20240118"}, "qid": "AutoScholarQuery_test_928"}
{"question": "Which papers discuss the RP gradient based on the reparameterization trick?", "answer": ["Auto-Encoding Variational Bayes"], "answer_arxiv_id": ["1312.6114"], "source_meta": {"published_time": "20231214"}, "qid": "AutoScholarQuery_test_929"}
{"question": "Which work showed that simple classifiers can detect images created by a single category of networks?", "answer": ["FaceForensics++: Learning to Detect Manipulated Facial Images"], "answer_arxiv_id": ["1901.08971"], "source_meta": {"published_time": "20230613"}, "qid": "AutoScholarQuery_test_930"}
{"question": "Which research introduced acoustic tokens into semantic token modeling and proposed a multi-stage generative framework in speech language models?", "answer": ["AudioLM: a Language Modeling Approach to Audio Generation"], "answer_arxiv_id": ["2209.03143"], "source_meta": {"published_time": "20240603"}, "qid": "AutoScholarQuery_test_931"}
{"question": "What studies proposed Stochastic Gradient Descent Ascent (SGDA) algorithms to address stochastic minimax optimization problems?", "answer": ["A Single-Loop Smoothed Gradient Descent-Ascent Algorithm for Nonconvex-Concave Min-Max Problems", "On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems", "Single-Timescale Stochastic Nonconvex-Concave Optimization for Smooth Nonlinear TD Learning", "Optimal Epoch Stochastic Gradient Descent Ascent Methods for Min-Max Optimization"], "answer_arxiv_id": ["2010.15768", "1906.00331", "2008.10103", "2002.05309"], "source_meta": {"published_time": "20230420"}, "qid": "AutoScholarQuery_test_932"}
{"question": "Could you provide me the studies of part correspondence?", "answer": ["Learning Local Shape Descriptors from Part Correspondences With\n  Multi-view Convolutional Networks"], "answer_arxiv_id": ["1706.04496"], "source_meta": {"published_time": "20231128"}, "qid": "AutoScholarQuery_test_933"}
{"question": "Which studies fall under the category of sparse-point annotation methods in weakly supervised 3D instance segmentation?", "answer": ["Exploring Data-Efficient 3D Scene Understanding with Contrastive Scene\n  Contexts", "PointContrast: Unsupervised Pre-training for 3D Point Cloud\n  Understanding"], "answer_arxiv_id": ["2012.09165", "2007.10985"], "source_meta": {"published_time": "20240322"}, "qid": "AutoScholarQuery_test_934"}
{"question": "Any studies about anomaly score improvement in reconstruction-based techniques by combining forecasting error and reconstruction probability?", "answer": ["Multivariate Time-series Anomaly Detection via Graph Attention Network"], "answer_arxiv_id": ["2009.02040"], "source_meta": {"published_time": "20231024"}, "qid": "AutoScholarQuery_test_935"}
{"question": "Can you provide works that used feature-based distillation methods in object detection?", "answer": ["FitNets: Hints for Thin Deep Nets", "Distilling Object Detectors with Fine-grained Feature Imitation", "Distilling Object Detectors via Decoupled Features"], "answer_arxiv_id": ["1412.6550", "1906.03609", "2103.14475"], "source_meta": {"published_time": "20220529"}, "qid": "AutoScholarQuery_test_936"}
{"question": "Which papers provide a comprehensive overview of transfer learning?", "answer": ["A Comprehensive Survey on Transfer Learning"], "answer_arxiv_id": ["1911.02685"], "source_meta": {"published_time": "20230323"}, "qid": "AutoScholarQuery_test_937"}
{"question": "Could you tell me some studies that implemented honesty-based fine-tuning and trained LLMs to admit limitations?", "answer": ["Alignment for Honesty"], "answer_arxiv_id": ["2312.07000"], "source_meta": {"published_time": "20240214"}, "qid": "AutoScholarQuery_test_938"}
{"question": "Could you tell me which paper proposed the fixed pre-decision module to bridge the gap between SimulMT and SimulST?", "answer": ["SimulMT to SimulST: Adapting Simultaneous Text Translation to End-to-End Simultaneous Speech Translation"], "answer_arxiv_id": ["2011.02048"], "source_meta": {"published_time": "20230703"}, "qid": "AutoScholarQuery_test_939"}
{"question": "Which papers discuss the use of barriers or immediate switching between the objective and constraint in CMDP framework?", "answer": ["IPO: Interior-point Policy Optimization under Constraints", "CRPO: A New Approach for Safe Reinforcement Learning with Convergence Guarantee"], "answer_arxiv_id": ["1910.09615", "2011.05869"], "source_meta": {"published_time": "20230130"}, "qid": "AutoScholarQuery_test_940"}
{"question": "Which work modified the algorithm to get the Probabilistic Serial fractional outcome while showing a weak notion of efficiency with a simpler proof?", "answer": ["A Probabilistic Approach to Voting, Allocation, Matching, and Coalition Formation"], "answer_arxiv_id": ["2002.10171"], "source_meta": {"published_time": "20230828"}, "qid": "AutoScholarQuery_test_941"}
{"question": "Which work introduced the use of orthogonality in feature space to encourage inter-class separation and intra-class clustering?", "answer": ["Orthogonal Projection Loss", "On orthogonality and learning recurrent networks with long term dependencies"], "answer_arxiv_id": ["2103.14021", "1702.00071"], "source_meta": {"published_time": "20231118"}, "qid": "AutoScholarQuery_test_942"}
{"question": "What paper employs self and cross-reconstruction modules to learn discriminative and smooth representations and uses DGCNN for learning the per-point feature embeddings?", "answer": ["DPC: Unsupervised Deep Point Correspondence via Cross and Self\n  Construction"], "answer_arxiv_id": ["2110.08636"], "source_meta": {"published_time": "20231128"}, "qid": "AutoScholarQuery_test_943"}
{"question": "Could you mention some studies that contributed to remarkable text-to-3D generation results?", "answer": ["DreamFusion: Text-to-3D using 2D Diffusion", "Magic3D: High-Resolution Text-to-3D Content Creation", "Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation", "DreamTime: An Improved Optimization Strategy for Text-to-3D Content Creation"], "answer_arxiv_id": ["2209.14988", "2211.10440", "2303.13873", "2306.12422"], "source_meta": {"published_time": "20230521"}, "qid": "AutoScholarQuery_test_944"}
{"question": "What studies advanced diffusion probabilistic models to generate high-resolution and diverse images?", "answer": ["Deep Unsupervised Learning using Nonequilibrium Thermodynamics", "Denoising Diffusion Probabilistic Models", "GLIDE: Towards Photorealistic Image Generation and Editing with\n  Text-Guided Diffusion Models", "High-Resolution Image Synthesis with Latent Diffusion Models", "Photorealistic Text-to-Image Diffusion Models with Deep Language\n  Understanding"], "answer_arxiv_id": ["1503.03585", "2006.11239", "2112.10741", "2112.10752", "2205.11487"], "source_meta": {"published_time": "20240103"}, "qid": "AutoScholarQuery_test_945"}
{"question": "Which works have sought to detect important skill neurons in performing a specific task?", "answer": ["Task-Specific Skill Localization in Fine-tuned Language Models", "Finding Skill Neurons in Pre-trained Transformer-based Language Models", "Task-specific Compression for Multi-task Language Models using\n  Attribution-based Pruning"], "answer_arxiv_id": ["2302.06600", "2211.07349", "2205.04157"], "source_meta": {"published_time": "20231116"}, "qid": "AutoScholarQuery_test_946"}
{"question": "Which studies have proposed methods to detect skill neurons within already trained models using an inference-based method?", "answer": ["Finding Skill Neurons in Pre-trained Transformer-based Language Models", "Task-specific Compression for Multi-task Language Models using\n  Attribution-based Pruning"], "answer_arxiv_id": ["2211.07349", "2205.04157"], "source_meta": {"published_time": "20231116"}, "qid": "AutoScholarQuery_test_947"}
{"question": "Which papers investigated CLIP-based classifiers and their performance in open-granularity classification?", "answer": ["Learning Transferable Visual Models From Natural Language Supervision", "Conditional Prompt Learning for Vision-Language Models", "Learning to Prompt for Vision-Language Models"], "answer_arxiv_id": ["2103.00020", "2203.05557", "2109.01134"], "source_meta": {"published_time": "20230604"}, "qid": "AutoScholarQuery_test_948"}
{"question": "What studies have produced abundant equivariant neural networks?", "answer": ["Tensor field networks: Rotation- and translation-equivariant neural networks for 3D point clouds", "Directional Message Passing for Molecular Graphs", "SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks", "E(n) Equivariant Graph Neural Networks"], "answer_arxiv_id": ["1802.08219", "2003.03123", "2006.10503", "2102.09844"], "source_meta": {"published_time": "20230201"}, "qid": "AutoScholarQuery_test_949"}
{"question": "Which studies used transformers and diffusion models for creating high-fidelity images from text?", "answer": ["VQGAN-CLIP: Open Domain Image Generation and Editing with Natural\n  Language Guidance", "CogView2: Faster and Better Text-to-Image Generation via Hierarchical\n  Transformers", "Denoising Diffusion Probabilistic Models", "Blended Diffusion for Text-driven Editing of Natural Images", "Text2LIVE: Text-Driven Layered Image and Video Editing", "Prompt-to-Prompt Image Editing with Cross Attention Control", "DiffusionCLIP: Text-Guided Diffusion Models for Robust Image\n  Manipulation", "More Control for Free! Image Synthesis with Semantic Diffusion Guidance", "GLIDE: Towards Photorealistic Image Generation and Editing with\n  Text-Guided Diffusion Models"], "answer_arxiv_id": ["2204.08583", "2204.14217", "2006.11239", "2111.14818", "2204.02491", "2208.01626", "2110.02711", "2112.05744", "2112.10741"], "source_meta": {"published_time": "20230406"}, "qid": "AutoScholarQuery_test_950"}
{"question": "What research works have explored how to address the susceptibility of current large language models to certain errors?", "answer": ["Internet-Augmented Dialogue Generation", "LaMDA: Language Models for Dialog Applications", "PAL: Program-aided Language Models", "Toolformer: Language Models Can Teach Themselves to Use Tools"], "answer_arxiv_id": ["2107.07566", "2201.08239", "2211.10435", "2302.04761"], "source_meta": {"published_time": "20240223"}, "qid": "AutoScholarQuery_test_951"}
{"question": "Are there any works that have further strengthened text embedders?", "answer": ["Text Embeddings by Weakly-Supervised Contrastive Pre-training", "C-Pack: Packaged Resources To Advance General Chinese Embedding", "Large Dual Encoders Are Generalizable Retrievers"], "answer_arxiv_id": ["2212.03533", "2309.07597", "2112.07899"], "source_meta": {"published_time": "20240215"}, "qid": "AutoScholarQuery_test_952"}
{"question": "Could you provide some works that discussed the problem of selling information in economics and computer science?", "answer": ["Optimal Mechanisms for Selling Information", "Selling Information Through Consulting", "How to Sell Information Optimally: an Algorithmic Study", "Optimal Pricing of Information", "Is Selling Complete Information (Approximately) Optimal?", "Optimal Advertising for Information Products", "Selling Data to an Agent with Endogenous Information"], "answer_arxiv_id": ["1204.5519", "1907.04397v3", "2011.14570", "2102.13289", "2202.09013", "2002.10045v5", "2103.05788v4"], "source_meta": {"published_time": "20230427"}, "qid": "AutoScholarQuery_test_953"}
{"question": "Which studies approached the combination of Imitation learning and Reinforcement learning by treating RL as a sequence modelling problem and train an autoregressive model using offline data?", "answer": ["Online Decision Transformer", "Offline Reinforcement Learning as One Big Sequence Modeling Problem", "Decision Transformer: Reinforcement Learning via Sequence Modeling"], "answer_arxiv_id": ["2202.05607", "2106.02039", "2106.01345"], "source_meta": {"published_time": "20220405"}, "qid": "AutoScholarQuery_test_954"}
{"question": "Could you provide me studies that improved the performance of small-scale models by fine-tuning them using reasoning processes generated by LLMs?", "answer": ["Training Verifiers to Solve Math Word Problems", "Large Language Models Are Reasoning Teachers", "SCOTT: Self-Consistent Chain-of-Thought Distillation", "Teaching Small Language Models to Reason"], "answer_arxiv_id": ["2110.14168", "2212.10071", "2305.01879", "2212.08410"], "source_meta": {"published_time": "20240216"}, "qid": "AutoScholarQuery_test_955"}
{"question": "Could you provide me some works that show asymmetric quantization methods outperform their symmetric counterparts?", "answer": ["AFPQ: Asymmetric Floating Point Quantization for LLMs"], "answer_arxiv_id": ["2311.01792"], "source_meta": {"published_time": "20240216"}, "qid": "AutoScholarQuery_test_956"}
{"question": "Could you provide me some works focusing on video visual relation detection using datasets like AG, VidVRD and VidOR?", "answer": ["Hollywood in Homes: Crowdsourcing Data Collection for Activity\n  Understanding"], "answer_arxiv_id": ["1604.01753"], "source_meta": {"published_time": "20240406"}, "qid": "AutoScholarQuery_test_957"}
{"question": "Could you mention a study that proposed plain diffusion Transformer architecture to learn the denoising diffusion process on latent patches?", "answer": ["Scalable Diffusion Models with Transformers"], "answer_arxiv_id": ["2212.09748"], "source_meta": {"published_time": "20230704"}, "qid": "AutoScholarQuery_test_958"}
{"question": "What works discusses the combination of the KD and model quantization method to achieve high compression ratios?", "answer": ["TernaryBERT: Distillation-aware Ultra-low Bit BERT", "Understanding and Improving Knowledge Distillation for\n  Quantization-Aware Training of Large Transformer Encoders"], "answer_arxiv_id": ["2009.12812", "2211.11014"], "source_meta": {"published_time": "20240216"}, "qid": "AutoScholarQuery_test_959"}
{"question": "Which research first studied a local minimax risk for instance optimality in differential privacy?", "answer": ["Near Instance-Optimality in Differential Privacy"], "answer_arxiv_id": ["2005.10630"], "source_meta": {"published_time": "20230301"}, "qid": "AutoScholarQuery_test_960"}
{"question": "Are there any works related to the framework of detector-free Structure-from-Motion?", "answer": ["Structure-from-Motion using Dense CNN Features with Keypoint Relocalization", "OnePose++: Keypoint-Free One-Shot Object Pose Estimation without CAD Models"], "answer_arxiv_id": ["1805.03879", "2301.07673"], "source_meta": {"published_time": "20230627"}, "qid": "AutoScholarQuery_test_961"}
{"question": "Which work discusses the generation of CDRs on a single chain in regards to pipeline-based antibody design?", "answer": ["Iterative Refinement Graph Neural Network for Antibody Sequence-Structure Co-design"], "answer_arxiv_id": ["2110.04624"], "source_meta": {"published_time": "20230201"}, "qid": "AutoScholarQuery_test_962"}
{"question": "Which research works presented improvements on the top of EDM?", "answer": ["MDM: Molecular Diffusion Model for 3D Molecule Generation", "Diffusion-based Molecule Generationwith Informative Prior Bridges", "MiDi: Mixed Graph and 3D Denoising Diffusion for Molecule Generation", "Geometric Latent Diffusion Models for 3D Molecule Generation"], "answer_arxiv_id": ["2209.05710", "2209.00865", "2302.09048", "2305.01140"], "source_meta": {"published_time": "20230613"}, "qid": "AutoScholarQuery_test_963"}
{"question": "Could you provide me some works that have developed RL, behavioral cloning, and LLM-based models on the web front?", "answer": ["Learning to Navigate the Web", "Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration", "A Data-Driven Approach for Learning to Control Computers"], "answer_arxiv_id": ["1812.09195", "1802.08802", "2202.08137"], "source_meta": {"published_time": "20230719"}, "qid": "AutoScholarQuery_test_964"}
{"question": "Which papers introduced factorization along spatial and temporal dimensions on the granularity of the encoder?", "answer": ["ViViT: A Video Vision Transformer", "Is Space-Time Attention All You Need for Video Understanding?"], "answer_arxiv_id": ["2103.15691", "2102.05095"], "source_meta": {"published_time": "20231204"}, "qid": "AutoScholarQuery_test_965"}
{"question": "Can you mention some studies that address the issue of diversity in single-imgae 3D generation, especially in face generation or starting from text for 3D generation?", "answer": ["Generating Diverse 3D Reconstructions from a Single Occluded Face Image", "ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with\n  Variational Score Distillation"], "answer_arxiv_id": ["2112.00879", "2305.16213"], "source_meta": {"published_time": "20231226"}, "qid": "AutoScholarQuery_test_966"}
{"question": "Can you provide a study that effectively tackles domain adaptation and active domain adaptation tasks for 3D semantic segmentation?", "answer": ["UniDA3D: Unified Domain Adaptive 3D Semantic Segmentation Pipeline"], "answer_arxiv_id": ["2212.10390"], "source_meta": {"published_time": "20231031"}, "qid": "AutoScholarQuery_test_967"}
{"question": "Which study proposed to use rooted homomorphism counts as node features in a graph neural network (GNN)?", "answer": ["Graph Neural Networks with Local Graph Parameters"], "answer_arxiv_id": ["2106.06707"], "source_meta": {"published_time": "20230609"}, "qid": "AutoScholarQuery_test_968"}
{"question": "Which studies used feature decomposition to enhance feature alignment in domain adaptation and domain generalization?", "answer": ["Decompose to Adapt: Cross-domain Object Detection via Feature\n  Disentanglement", "Learning to Balance Specificity and Invariance for In and Out of Domain\n  Generalization", "Efficient Domain Generalization via Common-Specific Low-Rank\n  Decomposition", "Modality-Agnostic Debiasing for Single Domain Generalization"], "answer_arxiv_id": ["2201.01929", "2008.12839", "2003.12815", "2303.07123"], "source_meta": {"published_time": "20240306"}, "qid": "AutoScholarQuery_test_969"}
{"question": "Could you provide me some studies that focus on the impact of noisy information on retrieval-augmented generation?", "answer": ["Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language\n  Models", "Making Retrieval-Augmented Language Models Robust to Irrelevant Context", "Benchmarking Large Language Models in Retrieval-Augmented Generation"], "answer_arxiv_id": ["2311.09210", "2310.01558", "2309.01431"], "source_meta": {"published_time": "20240531"}, "qid": "AutoScholarQuery_test_970"}
{"question": "What studies tackled the challenge of missing data within MMKGs with adversarial methods?", "answer": ["Embedding Multimodal Relational Data for Knowledge Base Completion"], "answer_arxiv_id": ["1809.01341"], "source_meta": {"published_time": "20240723"}, "qid": "AutoScholarQuery_test_971"}
{"question": "What papers utilized the strategy of transforming categorical data into a continuous space and then applying Gaussian diffusion?", "answer": ["Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning", "Equivariant Diffusion for Molecule Generation in 3D"], "answer_arxiv_id": ["2208.04202", "2203.17003"], "source_meta": {"published_time": "20230629"}, "qid": "AutoScholarQuery_test_972"}
{"question": "Name a research work that involves the use of neural networks as alternative function classes in BO methods?", "answer": ["Scalable Bayesian Optimization Using Deep Neural Networks"], "answer_arxiv_id": ["1502.05700"], "source_meta": {"published_time": "20221116"}, "qid": "AutoScholarQuery_test_973"}
{"question": "Which studies explored the effect of the choice of the in-context samples on the performance of in-context learning?", "answer": ["What Makes Good In-Context Examples for GPT-$3$?"], "answer_arxiv_id": ["2101.06804"], "source_meta": {"published_time": "20231116"}, "qid": "AutoScholarQuery_test_974"}
{"question": "Could you mention any research that was focused on applying CLIP to downstream tasks?", "answer": ["Learning to Prompt for Vision-Language Models", "Conditional Prompt Learning for Vision-Language Models", "CLIP-Adapter: Better Vision-Language Models with Feature Adapters", "Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling"], "answer_arxiv_id": ["2109.01134", "2203.05557", "2110.04544", "2111.03930"], "source_meta": {"published_time": "20230227"}, "qid": "AutoScholarQuery_test_975"}
{"question": "What are the studies where the RL agents are evaluated by seeing if they can quickly adapt from one task to another?", "answer": ["A Simple Neural Attentive Meta-Learner", "CARLA: An Open Urban Driving Simulator", "BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning", "Language Conditioned Imitation Learning over Unstructured Data"], "answer_arxiv_id": ["1707.03141v3", "1711.03938", "1810.08272", "2005.07648"], "source_meta": {"published_time": "20230426"}, "qid": "AutoScholarQuery_test_976"}
{"question": "Which works focused on the evaluation of RL agents by changing the surfaces of the objects in the environment?", "answer": ["Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World", "Natural Environment Benchmarks for Reinforcement Learning", "Investigating Generalisation in Continuous Deep Reinforcement Learning", "Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks"], "answer_arxiv_id": ["1703.06907", "1811.06032", "1902.07015", "1812.07252"], "source_meta": {"published_time": "20230426"}, "qid": "AutoScholarQuery_test_977"}
{"question": "Could you cite the works where multilingual LLMs were evaluated on individual tasks such as Translation, Question-Answering, Summarization, and Reasoning?", "answer": ["On the Cross-lingual Transferability of Monolingual Representations", "TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages", "XOR QA: Cross-lingual Open-Retrieval Question Answering", "XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44\n  Languages", "CrossSum: Beyond English-Centric Cross-Lingual Summarization for 1,500+\n  Language Pairs", "Language Models are Multilingual Chain-of-Thought Reasoners", "XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning"], "answer_arxiv_id": ["1910.11856", "2003.05002v1", "2010.11856", "2106.13822", "2112.08804", "2210.03057", "2005.00333"], "source_meta": {"published_time": "20240425"}, "qid": "AutoScholarQuery_test_978"}
{"question": "Can you provide the references that propose techniques for enhancing hierarchical classification consistency?", "answer": ["Unified Vision and Language Prompt Learning", "MaPLe: Multi-modal Prompt Learning", "LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of\n  Vision & Language Models"], "answer_arxiv_id": ["2210.07225", "2210.03117", "2210.01115"], "source_meta": {"published_time": "20230604"}, "qid": "AutoScholarQuery_test_979"}
{"question": "Could you provide some references focusing on achieving ϵ-stationary points under non-convex settings?", "answer": ["Derivative-free optimization methods"], "answer_arxiv_id": ["1904.11585"], "source_meta": {"published_time": "20230621"}, "qid": "AutoScholarQuery_test_980"}
{"question": "What studies have been conducted to predict future actions from a sequence of observed actions?", "answer": ["Uncertainty-Aware Anticipation of Activities", "When will you do what? - Anticipating Temporal Occurrences of Activities", "Anticipating human actions by correlating past with the future with\n  Jaccard similarity measures"], "answer_arxiv_id": ["1908.09540", "1804.00892", "2105.12414"], "source_meta": {"published_time": "20221125"}, "qid": "AutoScholarQuery_test_981"}
{"question": "Are there works that demonstrate attempts to make images unlearnable or uneditable to handle unauthorized data usage issues in diffusion models?", "answer": ["Glaze: Protecting Artists from Style Mimicry by Text-to-Image Models", "Raising the Cost of Malicious AI-Powered Image Editing"], "answer_arxiv_id": ["2302.04222", "2302.06588"], "source_meta": {"published_time": "20231030"}, "qid": "AutoScholarQuery_test_982"}
{"question": "Can you list the research that introduced the Mirror Descent Policy Optimization?", "answer": ["Mirror Descent Policy Optimization"], "answer_arxiv_id": ["2005.09814"], "source_meta": {"published_time": "20230130"}, "qid": "AutoScholarQuery_test_983"}
{"question": "Which research projects looked into misclassification issues of black-box classifiers where human text was mistaken as LLM-generated?", "answer": ["Can AI-Generated Text be Reliably Detected?"], "answer_arxiv_id": ["2303.11156"], "source_meta": {"published_time": "20231113"}, "qid": "AutoScholarQuery_test_984"}
{"question": "Could you provide examples of the works on building structured and interpretable models such as Hamiltonian Neural Networks (HNN), Lagrangian Neural Networks (LNN), and Sparse Identification of Nonlinear Dynamics (SINDy)?", "answer": ["Hamiltonian Neural Networks", "Lagrangian Neural Networks"], "answer_arxiv_id": ["1906.01563", "2003.04630"], "source_meta": {"published_time": "20220420"}, "qid": "AutoScholarQuery_test_985"}
{"question": "Did any researchers create a synchrony-based model with no reliance on explicit supervision for grouping?", "answer": ["Neuronal Synchrony in Complex-Valued Deep Networks", "Complex-Valued Autoencoders for Object Discovery"], "answer_arxiv_id": ["1312.6115", "2204.02075"], "source_meta": {"published_time": "20230524"}, "qid": "AutoScholarQuery_test_986"}
{"question": "Who proposed analytic methods for gradient inversion?", "answer": ["R-GAP: Recursive Gradient Attack on Privacy", "Rethinking Privacy Preserving Deep Learning: How to Evaluate and Thwart Privacy Attacks"], "answer_arxiv_id": ["2010.07733", "2006.11601"], "source_meta": {"published_time": "20230531"}, "qid": "AutoScholarQuery_test_987"}
{"question": "Are there any research related to meta-learning?", "answer": ["Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"], "answer_arxiv_id": ["1703.03400"], "source_meta": {"published_time": "20230215"}, "qid": "AutoScholarQuery_test_988"}
{"question": "Could you provide me some studies that detected hallucinations by analyzing the relationship between input prompts and the LLM's output responses?", "answer": ["Exploring the Relationship between LLM Hallucinations and Prompt\n  Linguistic Nuances: Readability, Formality, and Concreteness", "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for\n  Generative Large Language Models", "LLM Lies: Hallucinations are not Bugs, but Features as Adversarial\n  Examples"], "answer_arxiv_id": ["2309.11064", "2303.08896", "2310.01469"], "source_meta": {"published_time": "20240106"}, "qid": "AutoScholarQuery_test_989"}
{"question": "Which papers mentioned the application of transformer-based pre-trained models in code search?", "answer": ["GraphCodeBERT: Pre-training Code Representations with Data Flow", "Soft-Labeled Contrastive Pre-training for Function-level Code\n  Representation"], "answer_arxiv_id": ["2009.08366", "2210.09597"], "source_meta": {"published_time": "20240109"}, "qid": "AutoScholarQuery_test_990"}
{"question": "Could you provide references that discuss alternative approaches to the matrix mechanism that reduce the variance by adding bias?", "answer": ["A Simple and Practical Algorithm for Differentially Private Data Release", "Leveraging Public Data for Practical Private Query Release", "Differentially Private Query Release Through Adaptive Projection", "Dual Query: Practical Private Query Release for High Dimensional Data", "AIM: An Adaptive and Iterative Mechanism for Differentially Private Synthetic Data", "Iterative Methods for Private Synthetic Data: Unifying Framework and New Methods", "New Oracle-Efficient Algorithms for Private Synthetic Data Release", "PrivSyn: Differentially Private Data Synthesis"], "answer_arxiv_id": ["1012.4763", "2102.08598v2", "2103.06641", "1402.1526", "2201.12677", "2106.07153", "2007.05453", "2012.15128v1"], "source_meta": {"published_time": "20230514"}, "qid": "AutoScholarQuery_test_991"}
{"question": "What papers are about using synthetic data to improve the domain-generalization of NLI models?", "answer": ["Training Question Answering Models From Synthetic Data", "Generate, Annotate, and Learn: NLP with Synthetic Text", "QAmeleon: Multilingual QA with Only 5 Examples", "Synthetic Data Generation with Large Language Models for Text\n  Classification: Potential and Limitations"], "answer_arxiv_id": ["2002.09599", "2106.06168", "2211.08264", "2310.07849"], "source_meta": {"published_time": "20240219"}, "qid": "AutoScholarQuery_test_992"}
{"question": "Which works describe the advancements in 3D reconstruction and novel view synthesis with NeRF?", "answer": ["NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis", "NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo\n  Collections", "Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance\n  Fields", "MVSNeRF: Fast Generalizable Radiance Field Reconstruction from\n  Multi-View Stereo", "Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural\n  Radiance Fields"], "answer_arxiv_id": ["2003.08934", "2008.02268", "2103.13415", "2103.15595", "2307.11335"], "source_meta": {"published_time": "20240326"}, "qid": "AutoScholarQuery_test_993"}
{"question": "What works evaluated the applicability of MAML and its variants to graph neural networks (GNNs)?", "answer": ["Meta-Learning GNN Initializations for Low-Resource Molecular Property Prediction"], "answer_arxiv_id": ["2003.05996"], "source_meta": {"published_time": "20230424"}, "qid": "AutoScholarQuery_test_994"}
{"question": "What papers discuss client-side local distillation to transfer global knowledge to local models in generic FL?", "answer": ["Data-Free Knowledge Distillation for Heterogeneous Federated Learning"], "answer_arxiv_id": ["2105.10056"], "source_meta": {"published_time": "20230213"}, "qid": "AutoScholarQuery_test_995"}
{"question": "Which papers describe the analysis of the semantic property of intermediate latent space by its local geometry?", "answer": ["Do Not Escape From the Manifold: Discovering the Local Coordinates on the Latent Space of GANs"], "answer_arxiv_id": ["2106.06959"], "source_meta": {"published_time": "20221011"}, "qid": "AutoScholarQuery_test_996"}
{"question": "What are the examples of research done on SLAM methods in the context of object navigation tasks?", "answer": ["Object Goal Navigation using Goal-Oriented Semantic Exploration", "PONI: Potential Functions for ObjectGoal Navigation with Interaction-free Learning"], "answer_arxiv_id": ["2007.00643", "2201.10029"], "source_meta": {"published_time": "20230203"}, "qid": "AutoScholarQuery_test_997"}
{"question": "What works discuss the performance degradation in NLI models when presented with adversarial datasets?", "answer": ["Stress Test Evaluation for Natural Language Inference", "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural\n  Language Inference", "Adversarial NLI: A New Benchmark for Natural Language Understanding", "An Empirical Study on Model-agnostic Debiasing Strategies for Robust\n  Natural Language Inference"], "answer_arxiv_id": ["1806.00692", "1902.01007", "1910.14599", "2010.03777"], "source_meta": {"published_time": "20231113"}, "qid": "AutoScholarQuery_test_998"}
{"question": "What studies have focused on training language like models with privacy guarantees?", "answer": ["Large-Scale Differentially Private BERT"], "answer_arxiv_id": ["2108.01624"], "source_meta": {"published_time": "20230524"}, "qid": "AutoScholarQuery_test_999"}
